{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xiaoyicong\\AppData\\Local\\Temp\\ipykernel_27288\\3309417524.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  umam_embeddings = torch.load(\"umam_embeddings.pt\")\n",
      "C:\\Users\\xiaoyicong\\AppData\\Local\\Temp\\ipykernel_27288\\3309417524.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  umdm_embeddings = torch.load(\"umdm_embeddings.pt\")\n",
      "C:\\Users\\xiaoyicong\\AppData\\Local\\Temp\\ipykernel_27288\\3309417524.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  umum_embeddings = torch.load(\"umum_embeddings.pt\")\n",
      "C:\\Users\\xiaoyicong\\AppData\\Local\\Temp\\ipykernel_27288\\3309417524.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  user_content_embedding = torch.load(\"user_content_based_embeddings.pt\")\n"
     ]
    }
   ],
   "source": [
    "umam_embeddings = torch.load(\"umam_embeddings.pt\")\n",
    "umdm_embeddings = torch.load(\"umdm_embeddings.pt\")\n",
    "umum_embeddings = torch.load(\"umum_embeddings.pt\")\n",
    "user_content_embedding = torch.load(\"user_content_based_embeddings.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(943, 943, 943, 943)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(umam_embeddings), len(umdm_embeddings), len(umum_embeddings), len(user_content_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_content_embedding[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 5089]),\n",
       " torch.Size([1, 5089]),\n",
       " torch.Size([1, 5089]),\n",
       " torch.Size([1, 825]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "umam_embeddings[1].size(), umdm_embeddings[1].size(), umum_embeddings[1].size(), user_content_embedding[1].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 15267])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((umam_embeddings[1], umdm_embeddings[1], umum_embeddings[1]), dim = -1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare User and Movie Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataloader import DataLoader\n",
    "data_loader = DataLoader(size=\"100k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_id</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>genre_unknown</th>\n",
       "      <th>genre_Action</th>\n",
       "      <th>genre_Adventure</th>\n",
       "      <th>genre_Animation</th>\n",
       "      <th>genre_Children's</th>\n",
       "      <th>genre_Comedy</th>\n",
       "      <th>genre_Crime</th>\n",
       "      <th>...</th>\n",
       "      <th>genre_Fantasy</th>\n",
       "      <th>genre_Film-Noir</th>\n",
       "      <th>genre_Horror</th>\n",
       "      <th>genre_Musical</th>\n",
       "      <th>genre_Mystery</th>\n",
       "      <th>genre_Romance</th>\n",
       "      <th>genre_Sci-Fi</th>\n",
       "      <th>genre_Thriller</th>\n",
       "      <th>genre_War</th>\n",
       "      <th>genre_Western</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story</td>\n",
       "      <td>1995</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>GoldenEye</td>\n",
       "      <td>1995</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Four Rooms</td>\n",
       "      <td>1995</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Get Shorty</td>\n",
       "      <td>1995</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Copycat</td>\n",
       "      <td>1995</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   movie_id       title  year  genre_unknown  genre_Action  genre_Adventure  \\\n",
       "0         1   Toy Story  1995              0             0                0   \n",
       "1         2   GoldenEye  1995              0             1                1   \n",
       "2         3  Four Rooms  1995              0             0                0   \n",
       "3         4  Get Shorty  1995              0             1                0   \n",
       "4         5     Copycat  1995              0             0                0   \n",
       "\n",
       "   genre_Animation  genre_Children's  genre_Comedy  genre_Crime  ...  \\\n",
       "0                1                 1             1            0  ...   \n",
       "1                0                 0             0            0  ...   \n",
       "2                0                 0             0            0  ...   \n",
       "3                0                 0             1            0  ...   \n",
       "4                0                 0             0            1  ...   \n",
       "\n",
       "   genre_Fantasy  genre_Film-Noir  genre_Horror  genre_Musical  genre_Mystery  \\\n",
       "0              0                0             0              0              0   \n",
       "1              0                0             0              0              0   \n",
       "2              0                0             0              0              0   \n",
       "3              0                0             0              0              0   \n",
       "4              0                0             0              0              0   \n",
       "\n",
       "   genre_Romance  genre_Sci-Fi  genre_Thriller  genre_War  genre_Western  \n",
       "0              0             0               0          0              0  \n",
       "1              0             0               1          0              0  \n",
       "2              0             0               1          0              0  \n",
       "3              0             0               0          0              0  \n",
       "4              0             0               1          0              0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader.load_items().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_split import train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_data = DataLoader(size=\"100k\")\n",
    "\n",
    "# Load rating data\n",
    "data = movie_data.load_ratings()\n",
    "data = data[['user', 'item', 'rating']]\n",
    "train_list, test_list = train_test_split(data)\n",
    "ratings = pd.concat([train_list, train_list], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{97: 0,\n",
       " 266: 1,\n",
       " 811: 2,\n",
       " 24: 3,\n",
       " 31: 4,\n",
       " 281: 5,\n",
       " 569: 6,\n",
       " 260: 7,\n",
       " 332: 8,\n",
       " 324: 9,\n",
       " 423: 10,\n",
       " 468: 11,\n",
       " 287: 12,\n",
       " 894: 13,\n",
       " 869: 14,\n",
       " 639: 15,\n",
       " 539: 16,\n",
       " 500: 17,\n",
       " 482: 18,\n",
       " 335: 19,\n",
       " 849: 20,\n",
       " 771: 21,\n",
       " 926: 22,\n",
       " 40: 23,\n",
       " 364: 24,\n",
       " 765: 25,\n",
       " 308: 26,\n",
       " 445: 27,\n",
       " 714: 28,\n",
       " 705: 29,\n",
       " 77: 30,\n",
       " 818: 31,\n",
       " 596: 32,\n",
       " 372: 33,\n",
       " 166: 34,\n",
       " 756: 35,\n",
       " 251: 36,\n",
       " 883: 37,\n",
       " 437: 38,\n",
       " 240: 39,\n",
       " 108: 40,\n",
       " 68: 41,\n",
       " 175: 42,\n",
       " 159: 43,\n",
       " 140: 44,\n",
       " 755: 45,\n",
       " 732: 46,\n",
       " 307: 47,\n",
       " 533: 48,\n",
       " 695: 49,\n",
       " 803: 50,\n",
       " 64: 51,\n",
       " 236: 52,\n",
       " 766: 53,\n",
       " 900: 54,\n",
       " 87: 55,\n",
       " 693: 56,\n",
       " 724: 57,\n",
       " 779: 58,\n",
       " 517: 59,\n",
       " 661: 60,\n",
       " 588: 61,\n",
       " 327: 62,\n",
       " 219: 63,\n",
       " 210: 64,\n",
       " 657: 65,\n",
       " 71: 66,\n",
       " 929: 67,\n",
       " 650: 68,\n",
       " 469: 69,\n",
       " 847: 70,\n",
       " 200: 71,\n",
       " 480: 72,\n",
       " 448: 73,\n",
       " 522: 74,\n",
       " 676: 75,\n",
       " 941: 76,\n",
       " 137: 77,\n",
       " 209: 78,\n",
       " 626: 79,\n",
       " 214: 80,\n",
       " 573: 81,\n",
       " 356: 82,\n",
       " 622: 83,\n",
       " 73: 84,\n",
       " 478: 85,\n",
       " 45: 86,\n",
       " 892: 87,\n",
       " 591: 88,\n",
       " 614: 89,\n",
       " 507: 90,\n",
       " 267: 91,\n",
       " 169: 92,\n",
       " 333: 93,\n",
       " 452: 94,\n",
       " 310: 95,\n",
       " 261: 96,\n",
       " 607: 97,\n",
       " 939: 98,\n",
       " 823: 99,\n",
       " 434: 100,\n",
       " 328: 101,\n",
       " 731: 102,\n",
       " 886: 103,\n",
       " 303: 104,\n",
       " 618: 105,\n",
       " 61: 106,\n",
       " 248: 107,\n",
       " 708: 108,\n",
       " 410: 109,\n",
       " 211: 110,\n",
       " 670: 111,\n",
       " 389: 112,\n",
       " 67: 113,\n",
       " 352: 114,\n",
       " 276: 115,\n",
       " 524: 116,\n",
       " 560: 117,\n",
       " 381: 118,\n",
       " 262: 119,\n",
       " 449: 120,\n",
       " 897: 121,\n",
       " 586: 122,\n",
       " 255: 123,\n",
       " 282: 124,\n",
       " 853: 125,\n",
       " 498: 126,\n",
       " 121: 127,\n",
       " 111: 128,\n",
       " 616: 129,\n",
       " 425: 130,\n",
       " 885: 131,\n",
       " 193: 132,\n",
       " 893: 133,\n",
       " 930: 134,\n",
       " 542: 135,\n",
       " 371: 136,\n",
       " 589: 137,\n",
       " 426: 138,\n",
       " 138: 139,\n",
       " 312: 140,\n",
       " 729: 141,\n",
       " 306: 142,\n",
       " 706: 143,\n",
       " 508: 144,\n",
       " 523: 145,\n",
       " 863: 146,\n",
       " 674: 147,\n",
       " 395: 148,\n",
       " 535: 149,\n",
       " 79: 150,\n",
       " 599: 151,\n",
       " 794: 152,\n",
       " 781: 153,\n",
       " 430: 154,\n",
       " 606: 155,\n",
       " 529: 156,\n",
       " 854: 157,\n",
       " 66: 158,\n",
       " 142: 159,\n",
       " 785: 160,\n",
       " 527: 161,\n",
       " 775: 162,\n",
       " 627: 163,\n",
       " 311: 164,\n",
       " 745: 165,\n",
       " 681: 166,\n",
       " 383: 167,\n",
       " 612: 168,\n",
       " 50: 169,\n",
       " 686: 170,\n",
       " 405: 171,\n",
       " 413: 172,\n",
       " 851: 173,\n",
       " 830: 174,\n",
       " 826: 175,\n",
       " 938: 176,\n",
       " 199: 177,\n",
       " 543: 178,\n",
       " 34: 179,\n",
       " 32: 180,\n",
       " 704: 181,\n",
       " 450: 182,\n",
       " 360: 183,\n",
       " 833: 184,\n",
       " 552: 185,\n",
       " 110: 186,\n",
       " 78: 187,\n",
       " 216: 188,\n",
       " 245: 189,\n",
       " 83: 190,\n",
       " 317: 191,\n",
       " 351: 192,\n",
       " 466: 193,\n",
       " 345: 194,\n",
       " 3: 195,\n",
       " 102: 196,\n",
       " 454: 197,\n",
       " 632: 198,\n",
       " 232: 199,\n",
       " 6: 200,\n",
       " 55: 201,\n",
       " 322: 202,\n",
       " 907: 203,\n",
       " 353: 204,\n",
       " 788: 205,\n",
       " 98: 206,\n",
       " 759: 207,\n",
       " 774: 208,\n",
       " 901: 209,\n",
       " 600: 210,\n",
       " 904: 211,\n",
       " 665: 212,\n",
       " 888: 213,\n",
       " 26: 214,\n",
       " 85: 215,\n",
       " 11: 216,\n",
       " 604: 217,\n",
       " 334: 218,\n",
       " 429: 219,\n",
       " 668: 220,\n",
       " 942: 221,\n",
       " 119: 222,\n",
       " 319: 223,\n",
       " 520: 224,\n",
       " 30: 225,\n",
       " 56: 226,\n",
       " 673: 227,\n",
       " 320: 228,\n",
       " 623: 229,\n",
       " 723: 230,\n",
       " 197: 231,\n",
       " 553: 232,\n",
       " 82: 233,\n",
       " 678: 234,\n",
       " 658: 235,\n",
       " 584: 236,\n",
       " 299: 237,\n",
       " 828: 238,\n",
       " 329: 239,\n",
       " 205: 240,\n",
       " 212: 241,\n",
       " 570: 242,\n",
       " 797: 243,\n",
       " 315: 244,\n",
       " 291: 245,\n",
       " 463: 246,\n",
       " 603: 247,\n",
       " 440: 248,\n",
       " 228: 249,\n",
       " 807: 250,\n",
       " 905: 251,\n",
       " 297: 252,\n",
       " 752: 253,\n",
       " 689: 254,\n",
       " 60: 255,\n",
       " 343: 256,\n",
       " 880: 257,\n",
       " 409: 258,\n",
       " 399: 259,\n",
       " 548: 260,\n",
       " 8: 261,\n",
       " 156: 262,\n",
       " 891: 263,\n",
       " 421: 264,\n",
       " 577: 265,\n",
       " 735: 266,\n",
       " 534: 267,\n",
       " 293: 268,\n",
       " 544: 269,\n",
       " 879: 270,\n",
       " 363: 271,\n",
       " 483: 272,\n",
       " 549: 273,\n",
       " 871: 274,\n",
       " 594: 275,\n",
       " 631: 276,\n",
       " 229: 277,\n",
       " 922: 278,\n",
       " 688: 279,\n",
       " 213: 280,\n",
       " 80: 281,\n",
       " 149: 282,\n",
       " 721: 283,\n",
       " 443: 284,\n",
       " 300: 285,\n",
       " 424: 286,\n",
       " 295: 287,\n",
       " 134: 288,\n",
       " 582: 289,\n",
       " 651: 290,\n",
       " 418: 291,\n",
       " 1: 292,\n",
       " 666: 293,\n",
       " 515: 294,\n",
       " 313: 295,\n",
       " 864: 296,\n",
       " 877: 297,\n",
       " 495: 298,\n",
       " 173: 299,\n",
       " 848: 300,\n",
       " 347: 301,\n",
       " 366: 302,\n",
       " 850: 303,\n",
       " 91: 304,\n",
       " 884: 305,\n",
       " 182: 306,\n",
       " 275: 307,\n",
       " 843: 308,\n",
       " 70: 309,\n",
       " 292: 310,\n",
       " 132: 311,\n",
       " 301: 312,\n",
       " 663: 313,\n",
       " 145: 314,\n",
       " 136: 315,\n",
       " 165: 316,\n",
       " 29: 317,\n",
       " 530: 318,\n",
       " 194: 319,\n",
       " 737: 320,\n",
       " 881: 321,\n",
       " 170: 322,\n",
       " 433: 323,\n",
       " 141: 324,\n",
       " 174: 325,\n",
       " 7: 326,\n",
       " 605: 327,\n",
       " 870: 328,\n",
       " 74: 329,\n",
       " 644: 330,\n",
       " 362: 331,\n",
       " 239: 332,\n",
       " 146: 333,\n",
       " 908: 334,\n",
       " 235: 335,\n",
       " 221: 336,\n",
       " 918: 337,\n",
       " 602: 338,\n",
       " 133: 339,\n",
       " 934: 340,\n",
       " 358: 341,\n",
       " 186: 342,\n",
       " 42: 343,\n",
       " 868: 344,\n",
       " 109: 345,\n",
       " 839: 346,\n",
       " 57: 347,\n",
       " 835: 348,\n",
       " 733: 349,\n",
       " 531: 350,\n",
       " 25: 351,\n",
       " 406: 352,\n",
       " 484: 353,\n",
       " 919: 354,\n",
       " 339: 355,\n",
       " 52: 356,\n",
       " 458: 357,\n",
       " 866: 358,\n",
       " 597: 359,\n",
       " 265: 360,\n",
       " 494: 361,\n",
       " 789: 362,\n",
       " 778: 363,\n",
       " 935: 364,\n",
       " 19: 365,\n",
       " 814: 366,\n",
       " 536: 367,\n",
       " 378: 368,\n",
       " 368: 369,\n",
       " 84: 370,\n",
       " 62: 371,\n",
       " 382: 372,\n",
       " 273: 373,\n",
       " 286: 374,\n",
       " 361: 375,\n",
       " 355: 376,\n",
       " 555: 377,\n",
       " 279: 378,\n",
       " 13: 379,\n",
       " 183: 380,\n",
       " 369: 381,\n",
       " 431: 382,\n",
       " 619: 383,\n",
       " 224: 384,\n",
       " 540: 385,\n",
       " 659: 386,\n",
       " 441: 387,\n",
       " 479: 388,\n",
       " 550: 389,\n",
       " 754: 390,\n",
       " 177: 391,\n",
       " 635: 392,\n",
       " 501: 393,\n",
       " 933: 394,\n",
       " 457: 395,\n",
       " 164: 396,\n",
       " 249: 397,\n",
       " 598: 398,\n",
       " 834: 399,\n",
       " 376: 400,\n",
       " 75: 401,\n",
       " 114: 402,\n",
       " 537: 403,\n",
       " 528: 404,\n",
       " 391: 405,\n",
       " 105: 406,\n",
       " 115: 407,\n",
       " 412: 408,\n",
       " 93: 409,\n",
       " 532: 410,\n",
       " 90: 411,\n",
       " 337: 412,\n",
       " 932: 413,\n",
       " 867: 414,\n",
       " 878: 415,\n",
       " 571: 416,\n",
       " 906: 417,\n",
       " 576: 418,\n",
       " 95: 419,\n",
       " 12: 420,\n",
       " 397: 421,\n",
       " 502: 422,\n",
       " 44: 423,\n",
       " 43: 424,\n",
       " 330: 425,\n",
       " 168: 426,\n",
       " 491: 427,\n",
       " 825: 428,\n",
       " 875: 429,\n",
       " 713: 430,\n",
       " 101: 431,\n",
       " 427: 432,\n",
       " 179: 433,\n",
       " 417: 434,\n",
       " 656: 435,\n",
       " 831: 436,\n",
       " 621: 437,\n",
       " 178: 438,\n",
       " 396: 439,\n",
       " 696: 440,\n",
       " 784: 441,\n",
       " 645: 442,\n",
       " 384: 443,\n",
       " 258: 444,\n",
       " 496: 445,\n",
       " 336: 446,\n",
       " 579: 447,\n",
       " 16: 448,\n",
       " 4: 449,\n",
       " 486: 450,\n",
       " 257: 451,\n",
       " 514: 452,\n",
       " 715: 453,\n",
       " 667: 454,\n",
       " 465: 455,\n",
       " 394: 456,\n",
       " 223: 457,\n",
       " 180: 458,\n",
       " 290: 459,\n",
       " 545: 460,\n",
       " 325: 461,\n",
       " 558: 462,\n",
       " 10: 463,\n",
       " 250: 464,\n",
       " 23: 465,\n",
       " 357: 466,\n",
       " 222: 467,\n",
       " 629: 468,\n",
       " 812: 469,\n",
       " 927: 470,\n",
       " 750: 471,\n",
       " 341: 472,\n",
       " 432: 473,\n",
       " 519: 474,\n",
       " 786: 475,\n",
       " 546: 476,\n",
       " 204: 477,\n",
       " 595: 478,\n",
       " 446: 479,\n",
       " 238: 480,\n",
       " 94: 481,\n",
       " 526: 482,\n",
       " 817: 483,\n",
       " 634: 484,\n",
       " 790: 485,\n",
       " 285: 486,\n",
       " 185: 487,\n",
       " 758: 488,\n",
       " 921: 489,\n",
       " 435: 490,\n",
       " 654: 491,\n",
       " 154: 492,\n",
       " 76: 493,\n",
       " 451: 494,\n",
       " 690: 495,\n",
       " 757: 496,\n",
       " 278: 497,\n",
       " 69: 498,\n",
       " 447: 499,\n",
       " 592: 500,\n",
       " 487: 501,\n",
       " 189: 502,\n",
       " 272: 503,\n",
       " 698: 504,\n",
       " 583: 505,\n",
       " 237: 506,\n",
       " 488: 507,\n",
       " 89: 508,\n",
       " 801: 509,\n",
       " 118: 510,\n",
       " 126: 511,\n",
       " 858: 512,\n",
       " 513: 513,\n",
       " 824: 514,\n",
       " 559: 515,\n",
       " 127: 516,\n",
       " 117: 517,\n",
       " 474: 518,\n",
       " 653: 519,\n",
       " 58: 520,\n",
       " 636: 521,\n",
       " 859: 522,\n",
       " 370: 523,\n",
       " 269: 524,\n",
       " 47: 525,\n",
       " 350: 526,\n",
       " 196: 527,\n",
       " 943: 528,\n",
       " 787: 529,\n",
       " 694: 530,\n",
       " 264: 531,\n",
       " 444: 532,\n",
       " 637: 533,\n",
       " 305: 534,\n",
       " 342: 535,\n",
       " 911: 536,\n",
       " 150: 537,\n",
       " 125: 538,\n",
       " 742: 539,\n",
       " 51: 540,\n",
       " 354: 541,\n",
       " 873: 542,\n",
       " 143: 543,\n",
       " 471: 544,\n",
       " 400: 545,\n",
       " 590: 546,\n",
       " 321: 547,\n",
       " 20: 548,\n",
       " 763: 549,\n",
       " 746: 550,\n",
       " 762: 551,\n",
       " 408: 552,\n",
       " 538: 553,\n",
       " 585: 554,\n",
       " 39: 555,\n",
       " 176: 556,\n",
       " 246: 557,\n",
       " 782: 558,\n",
       " 630: 559,\n",
       " 711: 560,\n",
       " 809: 561,\n",
       " 155: 562,\n",
       " 288: 563,\n",
       " 568: 564,\n",
       " 18: 565,\n",
       " 128: 566,\n",
       " 323: 567,\n",
       " 256: 568,\n",
       " 751: 569,\n",
       " 909: 570,\n",
       " 191: 571,\n",
       " 116: 572,\n",
       " 581: 573,\n",
       " 181: 574,\n",
       " 302: 575,\n",
       " 716: 576,\n",
       " 718: 577,\n",
       " 683: 578,\n",
       " 753: 579,\n",
       " 652: 580,\n",
       " 518: 581,\n",
       " 928: 582,\n",
       " 46: 583,\n",
       " 856: 584,\n",
       " 158: 585,\n",
       " 937: 586,\n",
       " 172: 587,\n",
       " 17: 588,\n",
       " 512: 589,\n",
       " 49: 590,\n",
       " 915: 591,\n",
       " 793: 592,\n",
       " 516: 593,\n",
       " 887: 594,\n",
       " 481: 595,\n",
       " 284: 596,\n",
       " 617: 597,\n",
       " 226: 598,\n",
       " 27: 599,\n",
       " 738: 600,\n",
       " 438: 601,\n",
       " 896: 602,\n",
       " 365: 603,\n",
       " 230: 604,\n",
       " 38: 605,\n",
       " 910: 606,\n",
       " 375: 607,\n",
       " 470: 608,\n",
       " 912: 609,\n",
       " 802: 610,\n",
       " 685: 611,\n",
       " 195: 612,\n",
       " 805: 613,\n",
       " 815: 614,\n",
       " 504: 615,\n",
       " 914: 616,\n",
       " 899: 617,\n",
       " 580: 618,\n",
       " 913: 619,\n",
       " 163: 620,\n",
       " 923: 621,\n",
       " 153: 622,\n",
       " 842: 623,\n",
       " 671: 624,\n",
       " 712: 625,\n",
       " 112: 626,\n",
       " 227: 627,\n",
       " 821: 628,\n",
       " 104: 629,\n",
       " 422: 630,\n",
       " 420: 631,\n",
       " 707: 632,\n",
       " 587: 633,\n",
       " 736: 634,\n",
       " 120: 635,\n",
       " 54: 636,\n",
       " 152: 637,\n",
       " 404: 638,\n",
       " 890: 639,\n",
       " 208: 640,\n",
       " 796: 641,\n",
       " 719: 642,\n",
       " 9: 643,\n",
       " 761: 644,\n",
       " 37: 645,\n",
       " 453: 646,\n",
       " 254: 647,\n",
       " 304: 648,\n",
       " 855: 649,\n",
       " 572: 650,\n",
       " 624: 651,\n",
       " 669: 652,\n",
       " 679: 653,\n",
       " 263: 654,\n",
       " 611: 655,\n",
       " 298: 656,\n",
       " 415: 657,\n",
       " 151: 658,\n",
       " 744: 659,\n",
       " 641: 660,\n",
       " 837: 661,\n",
       " 551: 662,\n",
       " 747: 663,\n",
       " 489: 664,\n",
       " 148: 665,\n",
       " 147: 666,\n",
       " 680: 667,\n",
       " 895: 668,\n",
       " 697: 669,\n",
       " 660: 670,\n",
       " 349: 671,\n",
       " 464: 672,\n",
       " 326: 673,\n",
       " 187: 674,\n",
       " 124: 675,\n",
       " 804: 676,\n",
       " 609: 677,\n",
       " 144: 678,\n",
       " 903: 679,\n",
       " 198: 680,\n",
       " 610: 681,\n",
       " 280: 682,\n",
       " 294: 683,\n",
       " 401: 684,\n",
       " 123: 685,\n",
       " 184: 686,\n",
       " 203: 687,\n",
       " 439: 688,\n",
       " 247: 689,\n",
       " 416: 690,\n",
       " 722: 691,\n",
       " 840: 692,\n",
       " 836: 693,\n",
       " 130: 694,\n",
       " 638: 695,\n",
       " 403: 696,\n",
       " 740: 697,\n",
       " 844: 698,\n",
       " 860: 699,\n",
       " 220: 700,\n",
       " 642: 701,\n",
       " 862: 702,\n",
       " 709: 703,\n",
       " 760: 704,\n",
       " 865: 705,\n",
       " 625: 706,\n",
       " 791: 707,\n",
       " 717: 708,\n",
       " 387: 709,\n",
       " 916: 710,\n",
       " 510: 711,\n",
       " 268: 712,\n",
       " 810: 713,\n",
       " 442: 714,\n",
       " 497: 715,\n",
       " 113: 716,\n",
       " 692: 717,\n",
       " 233: 718,\n",
       " 820: 719,\n",
       " 608: 720,\n",
       " 672: 721,\n",
       " 374: 722,\n",
       " 925: 723,\n",
       " 795: 724,\n",
       " 234: 725,\n",
       " 741: 726,\n",
       " 677: 727,\n",
       " 318: 728,\n",
       " 649: 729,\n",
       " 411: 730,\n",
       " 846: 731,\n",
       " 710: 732,\n",
       " 359: 733,\n",
       " 259: 734,\n",
       " 628: 735,\n",
       " 633: 736,\n",
       " 283: 737,\n",
       " 377: 738,\n",
       " 385: 739,\n",
       " 225: 740,\n",
       " 898: 741,\n",
       " 768: 742,\n",
       " 473: 743,\n",
       " 348: 744,\n",
       " 506: 745,\n",
       " 640: 746,\n",
       " 931: 747,\n",
       " 874: 748,\n",
       " 852: 749,\n",
       " 620: 750,\n",
       " 917: 751,\n",
       " 646: 752,\n",
       " 798: 753,\n",
       " 557: 754,\n",
       " 902: 755,\n",
       " 578: 756,\n",
       " 86: 757,\n",
       " 243: 758,\n",
       " 699: 759,\n",
       " 160: 760,\n",
       " 525: 761,\n",
       " 36: 762,\n",
       " 541: 763,\n",
       " 171: 764,\n",
       " 655: 765,\n",
       " 838: 766,\n",
       " 808: 767,\n",
       " 799: 768,\n",
       " 889: 769,\n",
       " 734: 770,\n",
       " 96: 771,\n",
       " 564: 772,\n",
       " 241: 773,\n",
       " 743: 774,\n",
       " 575: 775,\n",
       " 691: 776,\n",
       " 461: 777,\n",
       " 554: 778,\n",
       " 827: 779,\n",
       " 207: 780,\n",
       " 393: 781,\n",
       " 398: 782,\n",
       " 767: 783,\n",
       " 800: 784,\n",
       " 218: 785,\n",
       " 5: 786,\n",
       " 769: 787,\n",
       " 643: 788,\n",
       " 845: 789,\n",
       " 613: 790,\n",
       " 739: 791,\n",
       " 547: 792,\n",
       " 726: 793,\n",
       " 684: 794,\n",
       " 99: 795,\n",
       " 728: 796,\n",
       " 574: 797,\n",
       " 407: 798,\n",
       " 503: 799,\n",
       " 48: 800,\n",
       " 33: 801,\n",
       " 780: 802,\n",
       " 201: 803,\n",
       " 135: 804,\n",
       " 28: 805,\n",
       " 829: 806,\n",
       " 231: 807,\n",
       " 490: 808,\n",
       " 773: 809,\n",
       " 379: 810,\n",
       " 289: 811,\n",
       " 419: 812,\n",
       " 675: 813,\n",
       " 392: 814,\n",
       " 593: 815,\n",
       " 499: 816,\n",
       " 139: 817,\n",
       " 63: 818,\n",
       " 472: 819,\n",
       " 648: 820,\n",
       " 129: 821,\n",
       " 920: 822,\n",
       " 521: 823,\n",
       " 65: 824,\n",
       " 813: 825,\n",
       " 15: 826,\n",
       " 157: 827,\n",
       " 41: 828,\n",
       " 493: 829,\n",
       " 380: 830,\n",
       " 188: 831,\n",
       " 764: 832,\n",
       " 217: 833,\n",
       " 792: 834,\n",
       " 53: 835,\n",
       " 338: 836,\n",
       " 749: 837,\n",
       " 720: 838,\n",
       " 725: 839,\n",
       " 296: 840,\n",
       " 702: 841,\n",
       " 252: 842,\n",
       " 727: 843,\n",
       " 462: 844,\n",
       " 456: 845,\n",
       " 940: 846,\n",
       " 816: 847,\n",
       " 270: 848,\n",
       " 202: 849,\n",
       " 162: 850,\n",
       " 556: 851,\n",
       " 730: 852,\n",
       " 402: 853,\n",
       " 703: 854,\n",
       " 477: 855,\n",
       " 822: 856,\n",
       " 772: 857,\n",
       " 106: 858,\n",
       " 566: 859,\n",
       " 390: 860,\n",
       " 2: 861,\n",
       " 882: 862,\n",
       " 562: 863,\n",
       " 81: 864,\n",
       " 206: 865,\n",
       " 35: 866,\n",
       " 776: 867,\n",
       " 509: 868,\n",
       " 428: 869,\n",
       " 455: 870,\n",
       " 367: 871,\n",
       " 92: 872,\n",
       " 340: 873,\n",
       " 565: 874,\n",
       " 346: 875,\n",
       " 777: 876,\n",
       " 242: 877,\n",
       " 14: 878,\n",
       " 316: 879,\n",
       " 601: 880,\n",
       " 388: 881,\n",
       " 274: 882,\n",
       " 167: 883,\n",
       " 841: 884,\n",
       " 936: 885,\n",
       " 647: 886,\n",
       " 819: 887,\n",
       " 485: 888,\n",
       " 924: 889,\n",
       " 505: 890,\n",
       " 832: 891,\n",
       " 244: 892,\n",
       " 567: 893,\n",
       " 876: 894,\n",
       " 563: 895,\n",
       " 687: 896,\n",
       " 190: 897,\n",
       " 783: 898,\n",
       " 700: 899,\n",
       " 476: 900,\n",
       " 682: 901,\n",
       " 511: 902,\n",
       " 59: 903,\n",
       " 475: 904,\n",
       " 561: 905,\n",
       " 857: 906,\n",
       " 748: 907,\n",
       " 253: 908,\n",
       " 22: 909,\n",
       " 314: 910,\n",
       " 460: 911,\n",
       " 161: 912,\n",
       " 277: 913,\n",
       " 192: 914,\n",
       " 386: 915,\n",
       " 806: 916,\n",
       " 414: 917,\n",
       " 492: 918,\n",
       " 344: 919,\n",
       " 770: 920,\n",
       " 309: 921,\n",
       " 662: 922,\n",
       " 131: 923,\n",
       " 664: 924,\n",
       " 872: 925,\n",
       " 100: 926,\n",
       " 373: 927,\n",
       " 88: 928,\n",
       " 459: 929,\n",
       " 331: 930,\n",
       " 215: 931,\n",
       " 467: 932,\n",
       " 122: 933,\n",
       " 615: 934,\n",
       " 21: 935,\n",
       " 701: 936,\n",
       " 72: 937,\n",
       " 107: 938,\n",
       " 271: 939,\n",
       " 861: 940,\n",
       " 436: 941,\n",
       " 103: 942}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_list = ratings['user'].unique().tolist()\n",
    "item_list = ratings['item'].unique().tolist()\n",
    "\n",
    "user2idx = {user: idx for idx, user in enumerate(user_list)}\n",
    "idx2user = {idx: user for user, idx in user2idx.items()}\n",
    "\n",
    "\n",
    "item2idx = {item: idx for idx, item in enumerate(item_list)}\n",
    "idx2item = {idx: item for item, idx in item2idx.items()}\n",
    "\n",
    "user2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{228: 0,\n",
       " 222: 1,\n",
       " 670: 2,\n",
       " 89: 3,\n",
       " 484: 4,\n",
       " 98: 5,\n",
       " 50: 6,\n",
       " 7: 7,\n",
       " 435: 8,\n",
       " 655: 9,\n",
       " 168: 10,\n",
       " 466: 11,\n",
       " 432: 12,\n",
       " 429: 13,\n",
       " 79: 14,\n",
       " 430: 15,\n",
       " 174: 16,\n",
       " 192: 17,\n",
       " 32: 18,\n",
       " 357: 19,\n",
       " 202: 20,\n",
       " 189: 21,\n",
       " 1: 22,\n",
       " 919: 23,\n",
       " 135: 24,\n",
       " 204: 25,\n",
       " 431: 26,\n",
       " 526: 27,\n",
       " 197: 28,\n",
       " 96: 29,\n",
       " 205: 30,\n",
       " 191: 31,\n",
       " 408: 32,\n",
       " 663: 33,\n",
       " 82: 34,\n",
       " 434: 35,\n",
       " 603: 36,\n",
       " 195: 37,\n",
       " 23: 38,\n",
       " 100: 39,\n",
       " 133: 40,\n",
       " 83: 41,\n",
       " 169: 42,\n",
       " 186: 43,\n",
       " 69: 44,\n",
       " 173: 45,\n",
       " 1126: 46,\n",
       " 423: 47,\n",
       " 874: 48,\n",
       " 508: 49,\n",
       " 272: 50,\n",
       " 124: 51,\n",
       " 313: 52,\n",
       " 283: 53,\n",
       " 268: 54,\n",
       " 286: 55,\n",
       " 275: 56,\n",
       " 14: 57,\n",
       " 321: 58,\n",
       " 319: 59,\n",
       " 237: 60,\n",
       " 325: 61,\n",
       " 276: 62,\n",
       " 924: 63,\n",
       " 9: 64,\n",
       " 304: 65,\n",
       " 294: 66,\n",
       " 748: 67,\n",
       " 690: 68,\n",
       " 258: 69,\n",
       " 323: 70,\n",
       " 289: 71,\n",
       " 307: 72,\n",
       " 315: 73,\n",
       " 300: 74,\n",
       " 308: 75,\n",
       " 988: 76,\n",
       " 678: 77,\n",
       " 243: 78,\n",
       " 901: 79,\n",
       " 763: 80,\n",
       " 367: 81,\n",
       " 742: 82,\n",
       " 372: 83,\n",
       " 662: 84,\n",
       " 25: 85,\n",
       " 178: 86,\n",
       " 318: 87,\n",
       " 56: 88,\n",
       " 216: 89,\n",
       " 475: 90,\n",
       " 109: 91,\n",
       " 117: 92,\n",
       " 127: 93,\n",
       " 55: 94,\n",
       " 238: 95,\n",
       " 180: 96,\n",
       " 486: 97,\n",
       " 176: 98,\n",
       " 729: 99,\n",
       " 421: 100,\n",
       " 8: 101,\n",
       " 582: 102,\n",
       " 11: 103,\n",
       " 41: 104,\n",
       " 200: 105,\n",
       " 12: 106,\n",
       " 92: 107,\n",
       " 288: 108,\n",
       " 132: 109,\n",
       " 1007: 110,\n",
       " 97: 111,\n",
       " 518: 112,\n",
       " 58: 113,\n",
       " 151: 114,\n",
       " 324: 115,\n",
       " 886: 116,\n",
       " 682: 117,\n",
       " 302: 118,\n",
       " 705: 119,\n",
       " 504: 120,\n",
       " 498: 121,\n",
       " 514: 122,\n",
       " 490: 123,\n",
       " 875: 124,\n",
       " 328: 125,\n",
       " 1021: 126,\n",
       " 136: 127,\n",
       " 262: 128,\n",
       " 271: 129,\n",
       " 1020: 130,\n",
       " 153: 131,\n",
       " 303: 132,\n",
       " 1022: 133,\n",
       " 1019: 134,\n",
       " 611: 135,\n",
       " 332: 136,\n",
       " 989: 137,\n",
       " 538: 138,\n",
       " 877: 139,\n",
       " 938: 140,\n",
       " 322: 141,\n",
       " 333: 142,\n",
       " 326: 143,\n",
       " 331: 144,\n",
       " 301: 145,\n",
       " 342: 146,\n",
       " 3: 147,\n",
       " 126: 148,\n",
       " 287: 149,\n",
       " 676: 150,\n",
       " 252: 151,\n",
       " 340: 152,\n",
       " 458: 153,\n",
       " 121: 154,\n",
       " 125: 155,\n",
       " 473: 156,\n",
       " 826: 157,\n",
       " 979: 158,\n",
       " 16: 159,\n",
       " 15: 160,\n",
       " 236: 161,\n",
       " 281: 162,\n",
       " 19: 163,\n",
       " 405: 164,\n",
       " 1284: 165,\n",
       " 756: 166,\n",
       " 685: 167,\n",
       " 13: 168,\n",
       " 284: 169,\n",
       " 257: 170,\n",
       " 298: 171,\n",
       " 762: 172,\n",
       " 471: 173,\n",
       " 455: 174,\n",
       " 274: 175,\n",
       " 118: 176,\n",
       " 546: 177,\n",
       " 291: 178,\n",
       " 990: 179,\n",
       " 1105: 180,\n",
       " 1025: 181,\n",
       " 362: 182,\n",
       " 882: 183,\n",
       " 891: 184,\n",
       " 350: 185,\n",
       " 334: 186,\n",
       " 270: 187,\n",
       " 881: 188,\n",
       " 770: 189,\n",
       " 566: 190,\n",
       " 975: 191,\n",
       " 693: 192,\n",
       " 229: 193,\n",
       " 232: 194,\n",
       " 451: 195,\n",
       " 673: 196,\n",
       " 1188: 197,\n",
       " 595: 198,\n",
       " 44: 199,\n",
       " 1150: 200,\n",
       " 1013: 201,\n",
       " 354: 202,\n",
       " 235: 203,\n",
       " 978: 204,\n",
       " 449: 205,\n",
       " 218: 206,\n",
       " 728: 207,\n",
       " 249: 208,\n",
       " 871: 209,\n",
       " 717: 210,\n",
       " 934: 211,\n",
       " 1244: 212,\n",
       " 409: 213,\n",
       " 22: 214,\n",
       " 410: 215,\n",
       " 831: 216,\n",
       " 866: 217,\n",
       " 295: 218,\n",
       " 95: 219,\n",
       " 120: 220,\n",
       " 53: 221,\n",
       " 684: 222,\n",
       " 1011: 223,\n",
       " 293: 224,\n",
       " 815: 225,\n",
       " 982: 226,\n",
       " 159: 227,\n",
       " 273: 228,\n",
       " 264: 229,\n",
       " 456: 230,\n",
       " 1090: 231,\n",
       " 597: 232,\n",
       " 370: 233,\n",
       " 824: 234,\n",
       " 746: 235,\n",
       " 70: 236,\n",
       " 369: 237,\n",
       " 282: 238,\n",
       " 470: 239,\n",
       " 227: 240,\n",
       " 841: 241,\n",
       " 234: 242,\n",
       " 895: 243,\n",
       " 105: 244,\n",
       " 210: 245,\n",
       " 651: 246,\n",
       " 845: 247,\n",
       " 450: 248,\n",
       " 172: 249,\n",
       " 406: 250,\n",
       " 628: 251,\n",
       " 230: 252,\n",
       " 38: 253,\n",
       " 833: 254,\n",
       " 452: 255,\n",
       " 983: 256,\n",
       " 73: 257,\n",
       " 31: 258,\n",
       " 327: 259,\n",
       " 356: 260,\n",
       " 240: 261,\n",
       " 1042: 262,\n",
       " 660: 263,\n",
       " 827: 264,\n",
       " 255: 265,\n",
       " 1157: 266,\n",
       " 769: 267,\n",
       " 568: 268,\n",
       " 411: 269,\n",
       " 385: 270,\n",
       " 552: 271,\n",
       " 1210: 272,\n",
       " 64: 273,\n",
       " 974: 274,\n",
       " 562: 275,\n",
       " 245: 276,\n",
       " 984: 277,\n",
       " 554: 278,\n",
       " 550: 279,\n",
       " 619: 280,\n",
       " 1033: 281,\n",
       " 754: 282,\n",
       " 123: 283,\n",
       " 879: 284,\n",
       " 248: 285,\n",
       " 749: 286,\n",
       " 150: 287,\n",
       " 259: 288,\n",
       " 292: 289,\n",
       " 250: 290,\n",
       " 285: 291,\n",
       " 310: 292,\n",
       " 339: 293,\n",
       " 873: 294,\n",
       " 1134: 295,\n",
       " 269: 296,\n",
       " 348: 297,\n",
       " 898: 298,\n",
       " 1265: 299,\n",
       " 347: 300,\n",
       " 751: 301,\n",
       " 696: 302,\n",
       " 148: 303,\n",
       " 689: 304,\n",
       " 1238: 305,\n",
       " 299: 306,\n",
       " 823: 307,\n",
       " 344: 308,\n",
       " 591: 309,\n",
       " 750: 310,\n",
       " 887: 311,\n",
       " 926: 312,\n",
       " 461: 313,\n",
       " 642: 314,\n",
       " 39: 315,\n",
       " 209: 316,\n",
       " 544: 317,\n",
       " 51: 318,\n",
       " 71: 319,\n",
       " 4: 320,\n",
       " 137: 321,\n",
       " 1168: 322,\n",
       " 612: 323,\n",
       " 181: 324,\n",
       " 692: 325,\n",
       " 428: 326,\n",
       " 214: 327,\n",
       " 856: 328,\n",
       " 1014: 329,\n",
       " 1051: 330,\n",
       " 144: 331,\n",
       " 1012: 332,\n",
       " 462: 333,\n",
       " 377: 334,\n",
       " 246: 335,\n",
       " 42: 336,\n",
       " 724: 337,\n",
       " 1016: 338,\n",
       " 170: 339,\n",
       " 699: 340,\n",
       " 297: 341,\n",
       " 427: 342,\n",
       " 529: 343,\n",
       " 952: 344,\n",
       " 963: 345,\n",
       " 1070: 346,\n",
       " 531: 347,\n",
       " 647: 348,\n",
       " 955: 349,\n",
       " 24: 350,\n",
       " 91: 351,\n",
       " 5: 352,\n",
       " 161: 353,\n",
       " 772: 354,\n",
       " 226: 355,\n",
       " 943: 356,\n",
       " 156: 357,\n",
       " 346: 358,\n",
       " 111: 359,\n",
       " 28: 360,\n",
       " 201: 361,\n",
       " 426: 362,\n",
       " 108: 363,\n",
       " 1067: 364,\n",
       " 652: 365,\n",
       " 888: 366,\n",
       " 638: 367,\n",
       " 244: 368,\n",
       " 740: 369,\n",
       " 863: 370,\n",
       " 1131: 371,\n",
       " 1560: 372,\n",
       " 355: 373,\n",
       " 129: 374,\n",
       " 147: 375,\n",
       " 336: 376,\n",
       " 558: 377,\n",
       " 1313: 378,\n",
       " 1226: 379,\n",
       " 691: 380,\n",
       " 937: 381,\n",
       " 515: 382,\n",
       " 381: 383,\n",
       " 223: 384,\n",
       " 744: 385,\n",
       " 107: 386,\n",
       " 45: 387,\n",
       " 1379: 388,\n",
       " 1381: 389,\n",
       " 900: 390,\n",
       " 923: 391,\n",
       " 86: 392,\n",
       " 59: 393,\n",
       " 904: 394,\n",
       " 885: 395,\n",
       " 343: 396,\n",
       " 1658: 397,\n",
       " 1048: 398,\n",
       " 256: 399,\n",
       " 1462: 400,\n",
       " 1251: 401,\n",
       " 306: 402,\n",
       " 855: 403,\n",
       " 1073: 404,\n",
       " 1080: 405,\n",
       " 1115: 406,\n",
       " 316: 407,\n",
       " 903: 408,\n",
       " 279: 409,\n",
       " 26: 410,\n",
       " 171: 411,\n",
       " 165: 412,\n",
       " 20: 413,\n",
       " 512: 414,\n",
       " 260: 415,\n",
       " 971: 416,\n",
       " 116: 417,\n",
       " 472: 418,\n",
       " 935: 419,\n",
       " 113: 420,\n",
       " 60: 421,\n",
       " 290: 422,\n",
       " 960: 423,\n",
       " 536: 424,\n",
       " 736: 425,\n",
       " 1005: 426,\n",
       " 277: 427,\n",
       " 190: 428,\n",
       " 30: 429,\n",
       " 718: 430,\n",
       " 1403: 431,\n",
       " 1255: 432,\n",
       " 818: 433,\n",
       " 698: 434,\n",
       " 936: 435,\n",
       " 278: 436,\n",
       " 1153: 437,\n",
       " 639: 438,\n",
       " 933: 439,\n",
       " 1089: 440,\n",
       " 345: 441,\n",
       " 1295: 442,\n",
       " 52: 443,\n",
       " 534: 444,\n",
       " 509: 445,\n",
       " 1038: 446,\n",
       " 305: 447,\n",
       " 280: 448,\n",
       " 330: 449,\n",
       " 905: 450,\n",
       " 412: 451,\n",
       " 846: 452,\n",
       " 312: 453,\n",
       " 242: 454,\n",
       " 596: 455,\n",
       " 253: 456,\n",
       " 1132: 457,\n",
       " 1163: 458,\n",
       " 1079: 459,\n",
       " 122: 460,\n",
       " 1047: 461,\n",
       " 88: 462,\n",
       " 488: 463,\n",
       " 487: 464,\n",
       " 659: 465,\n",
       " 553: 466,\n",
       " 1194: 467,\n",
       " 527: 468,\n",
       " 517: 469,\n",
       " 198: 470,\n",
       " 664: 471,\n",
       " 66: 472,\n",
       " 199: 473,\n",
       " 796: 474,\n",
       " 155: 475,\n",
       " 513: 476,\n",
       " 414: 477,\n",
       " 1195: 478,\n",
       " 584: 479,\n",
       " 648: 480,\n",
       " 371: 481,\n",
       " 510: 482,\n",
       " 727: 483,\n",
       " 716: 484,\n",
       " 87: 485,\n",
       " 57: 486,\n",
       " 958: 487,\n",
       " 615: 488,\n",
       " 747: 489,\n",
       " 702: 490,\n",
       " 1465: 491,\n",
       " 549: 492,\n",
       " 528: 493,\n",
       " 792: 494,\n",
       " 213: 495,\n",
       " 311: 496,\n",
       " 519: 497,\n",
       " 162: 498,\n",
       " 786: 499,\n",
       " 196: 500,\n",
       " 739: 501,\n",
       " 483: 502,\n",
       " 661: 503,\n",
       " 604: 504,\n",
       " 511: 505,\n",
       " 1193: 506,\n",
       " 580: 507,\n",
       " 194: 508,\n",
       " 731: 509,\n",
       " 193: 510,\n",
       " 215: 511,\n",
       " 949: 512,\n",
       " 835: 513,\n",
       " 61: 514,\n",
       " 1101: 515,\n",
       " 185: 516,\n",
       " 956: 517,\n",
       " 239: 518,\n",
       " 640: 519,\n",
       " 962: 520,\n",
       " 496: 521,\n",
       " 781: 522,\n",
       " 665: 523,\n",
       " 836: 524,\n",
       " 469: 525,\n",
       " 164: 526,\n",
       " 386: 527,\n",
       " 143: 528,\n",
       " 522: 529,\n",
       " 175: 530,\n",
       " 1111: 531,\n",
       " 721: 532,\n",
       " 1324: 533,\n",
       " 535: 534,\n",
       " 179: 535,\n",
       " 1326: 536,\n",
       " 1008: 537,\n",
       " 714: 538,\n",
       " 43: 539,\n",
       " 1018: 540,\n",
       " 708: 541,\n",
       " 62: 542,\n",
       " 1009: 543,\n",
       " 476: 544,\n",
       " 393: 545,\n",
       " 396: 546,\n",
       " 775: 547,\n",
       " 72: 548,\n",
       " 211: 549,\n",
       " 448: 550,\n",
       " 1385: 551,\n",
       " 217: 552,\n",
       " 1160: 553,\n",
       " 780: 554,\n",
       " 387: 555,\n",
       " 94: 556,\n",
       " 77: 557,\n",
       " 821: 558,\n",
       " 407: 559,\n",
       " 735: 560,\n",
       " 1166: 561,\n",
       " 443: 562,\n",
       " 1135: 563,\n",
       " 134: 564,\n",
       " 1469: 565,\n",
       " 10: 566,\n",
       " 557: 567,\n",
       " 358: 568,\n",
       " 425: 569,\n",
       " 1315: 570,\n",
       " 182: 571,\n",
       " 49: 572,\n",
       " 93: 573,\n",
       " 402: 574,\n",
       " 1010: 575,\n",
       " 588: 576,\n",
       " 27: 577,\n",
       " 633: 578,\n",
       " 207: 579,\n",
       " 625: 580,\n",
       " 203: 581,\n",
       " 154: 582,\n",
       " 241: 583,\n",
       " 707: 584,\n",
       " 542: 585,\n",
       " 709: 586,\n",
       " 1129: 587,\n",
       " 403: 588,\n",
       " 768: 589,\n",
       " 892: 590,\n",
       " 694: 591,\n",
       " 251: 592,\n",
       " 880: 593,\n",
       " 337: 594,\n",
       " 948: 595,\n",
       " 261: 596,\n",
       " 687: 597,\n",
       " 847: 598,\n",
       " 1197: 599,\n",
       " 649: 600,\n",
       " 81: 601,\n",
       " 1286: 602,\n",
       " 516: 603,\n",
       " 436: 604,\n",
       " 848: 605,\n",
       " 493: 606,\n",
       " 163: 607,\n",
       " 378: 608,\n",
       " 634: 609,\n",
       " 166: 610,\n",
       " 802: 611,\n",
       " 481: 612,\n",
       " 579: 613,\n",
       " 467: 614,\n",
       " 1028: 615,\n",
       " 609: 616,\n",
       " 607: 617,\n",
       " 502: 618,\n",
       " 569: 619,\n",
       " 447: 620,\n",
       " 965: 621,\n",
       " 632: 622,\n",
       " 233: 623,\n",
       " 1046: 624,\n",
       " 265: 625,\n",
       " 610: 626,\n",
       " 843: 627,\n",
       " 1006: 628,\n",
       " 183: 629,\n",
       " 219: 630,\n",
       " 822: 631,\n",
       " 755: 632,\n",
       " 433: 633,\n",
       " 160: 634,\n",
       " 1211: 635,\n",
       " 589: 636,\n",
       " 806: 637,\n",
       " 463: 638,\n",
       " 616: 639,\n",
       " 578: 640,\n",
       " 921: 641,\n",
       " 505: 642,\n",
       " 1421: 643,\n",
       " 520: 644,\n",
       " 618: 645,\n",
       " 74: 646,\n",
       " 686: 647,\n",
       " 482: 648,\n",
       " 495: 649,\n",
       " 712: 650,\n",
       " 48: 651,\n",
       " 208: 652,\n",
       " 254: 653,\n",
       " 499: 654,\n",
       " 157: 655,\n",
       " 641: 656,\n",
       " 1411: 657,\n",
       " 231: 658,\n",
       " 68: 659,\n",
       " 715: 660,\n",
       " 523: 661,\n",
       " 1456: 662,\n",
       " 1154: 663,\n",
       " 605: 664,\n",
       " 646: 665,\n",
       " 945: 666,\n",
       " 732: 667,\n",
       " 419: 668,\n",
       " 1252: 669,\n",
       " 825: 670,\n",
       " 21: 671,\n",
       " 65: 672,\n",
       " 614: 673,\n",
       " 613: 674,\n",
       " 1045: 675,\n",
       " 942: 676,\n",
       " 506: 677,\n",
       " 17: 678,\n",
       " 485: 679,\n",
       " 382: 680,\n",
       " 99: 681,\n",
       " 1065: 682,\n",
       " 507: 683,\n",
       " 141: 684,\n",
       " 420: 685,\n",
       " 567: 686,\n",
       " 653: 687,\n",
       " 479: 688,\n",
       " 654: 689,\n",
       " 1140: 690,\n",
       " 501: 691,\n",
       " 47: 692,\n",
       " 1147: 693,\n",
       " 1074: 694,\n",
       " 637: 695,\n",
       " 392: 696,\n",
       " 1121: 697,\n",
       " 525: 698,\n",
       " 480: 699,\n",
       " 1118: 700,\n",
       " 537: 701,\n",
       " 530: 702,\n",
       " 1591: 703,\n",
       " 829: 704,\n",
       " 1277: 705,\n",
       " 959: 706,\n",
       " 908: 707,\n",
       " 221: 708,\n",
       " 1601: 709,\n",
       " 1598: 710,\n",
       " 1097: 711,\n",
       " 840: 712,\n",
       " 1199: 713,\n",
       " 644: 714,\n",
       " 1081: 715,\n",
       " 460: 716,\n",
       " 902: 717,\n",
       " 994: 718,\n",
       " 1378: 719,\n",
       " 1152: 720,\n",
       " 622: 721,\n",
       " 560: 722,\n",
       " 627: 723,\n",
       " 2: 724,\n",
       " 1228: 725,\n",
       " 849: 726,\n",
       " 720: 727,\n",
       " 623: 728,\n",
       " 225: 729,\n",
       " 399: 730,\n",
       " 29: 731,\n",
       " 1035: 732,\n",
       " 142: 733,\n",
       " 862: 734,\n",
       " 363: 735,\n",
       " 576: 736,\n",
       " 797: 737,\n",
       " 474: 738,\n",
       " 636: 739,\n",
       " 912: 740,\n",
       " 149: 741,\n",
       " 635: 742,\n",
       " 1083: 743,\n",
       " 441: 744,\n",
       " 547: 745,\n",
       " 574: 746,\n",
       " 581: 747,\n",
       " 1273: 748,\n",
       " 1109: 749,\n",
       " 559: 750,\n",
       " 446: 751,\n",
       " 674: 752,\n",
       " 872: 753,\n",
       " 844: 754,\n",
       " 894: 755,\n",
       " 404: 756,\n",
       " 398: 757,\n",
       " 753: 758,\n",
       " 418: 759,\n",
       " 860: 760,\n",
       " 1060: 761,\n",
       " 1149: 762,\n",
       " 1240: 763,\n",
       " 1031: 764,\n",
       " 1652: 765,\n",
       " 33: 766,\n",
       " 1098: 767,\n",
       " 813: 768,\n",
       " 468: 769,\n",
       " 867: 770,\n",
       " 561: 771,\n",
       " 752: 772,\n",
       " 212: 773,\n",
       " 656: 774,\n",
       " 1222: 775,\n",
       " 464: 776,\n",
       " 703: 777,\n",
       " 1592: 778,\n",
       " 1227: 779,\n",
       " 477: 780,\n",
       " 1656: 781,\n",
       " 896: 782,\n",
       " 1404: 783,\n",
       " 1171: 784,\n",
       " 785: 785,\n",
       " 778: 786,\n",
       " 338: 787,\n",
       " 794: 788,\n",
       " 1288: 789,\n",
       " 713: 790,\n",
       " 349: 791,\n",
       " 745: 792,\n",
       " 1262: 793,\n",
       " 658: 794,\n",
       " 710: 795,\n",
       " 683: 796,\n",
       " 969: 797,\n",
       " 737: 798,\n",
       " 657: 799,\n",
       " 1075: 800,\n",
       " 497: 801,\n",
       " 1039: 802,\n",
       " 946: 803,\n",
       " 1036: 804,\n",
       " 1599: 805,\n",
       " 842: 806,\n",
       " 697: 807,\n",
       " 521: 808,\n",
       " 1091: 809,\n",
       " 478: 810,\n",
       " 629: 811,\n",
       " 1206: 812,\n",
       " 415: 813,\n",
       " 145: 814,\n",
       " 583: 815,\n",
       " 101: 816,\n",
       " 1063: 817,\n",
       " 606: 818,\n",
       " 417: 819,\n",
       " 139: 820,\n",
       " 1267: 821,\n",
       " 961: 822,\n",
       " 1142: 823,\n",
       " 602: 824,\n",
       " 152: 825,\n",
       " 401: 826,\n",
       " 353: 827,\n",
       " 931: 828,\n",
       " 669: 829,\n",
       " 869: 830,\n",
       " 187: 831,\n",
       " 1037: 832,\n",
       " 1002: 833,\n",
       " 103: 834,\n",
       " 220: 835,\n",
       " 918: 836,\n",
       " 1023: 837,\n",
       " 364: 838,\n",
       " 876: 839,\n",
       " 1258: 840,\n",
       " 67: 841,\n",
       " 1221: 842,\n",
       " 130: 843,\n",
       " 1254: 844,\n",
       " 688: 845,\n",
       " 1110: 846,\n",
       " 114: 847,\n",
       " 380: 848,\n",
       " 395: 849,\n",
       " 631: 850,\n",
       " 1177: 851,\n",
       " 1282: 852,\n",
       " 1041: 853,\n",
       " 1173: 854,\n",
       " 489: 855,\n",
       " 1174: 856,\n",
       " 820: 857,\n",
       " 1161: 858,\n",
       " 1024: 859,\n",
       " 995: 860,\n",
       " 991: 861,\n",
       " 1139: 862,\n",
       " 1141: 863,\n",
       " 188: 864,\n",
       " 389: 865,\n",
       " 503: 866,\n",
       " 184: 867,\n",
       " 1102: 868,\n",
       " 1444: 869,\n",
       " 366: 870,\n",
       " 494: 871,\n",
       " 131: 872,\n",
       " 951: 873,\n",
       " 1203: 874,\n",
       " 177: 875,\n",
       " 672: 876,\n",
       " 630: 877,\n",
       " 1298: 878,\n",
       " 40: 879,\n",
       " 968: 880,\n",
       " 1050: 881,\n",
       " 375: 882,\n",
       " 837: 883,\n",
       " 810: 884,\n",
       " 834: 885,\n",
       " 864: 886,\n",
       " 384: 887,\n",
       " 585: 888,\n",
       " 1000: 889,\n",
       " 1183: 890,\n",
       " 1189: 891,\n",
       " 1072: 892,\n",
       " 790: 893,\n",
       " 1190: 894,\n",
       " 1187: 895,\n",
       " 63: 896,\n",
       " 491: 897,\n",
       " 1184: 898,\n",
       " 167: 899,\n",
       " 575: 900,\n",
       " 722: 901,\n",
       " 1049: 902,\n",
       " 783: 903,\n",
       " 80: 904,\n",
       " 577: 905,\n",
       " 1179: 906,\n",
       " 944: 907,\n",
       " 1185: 908,\n",
       " 158: 909,\n",
       " 808: 910,\n",
       " 801: 911,\n",
       " 765: 912,\n",
       " 1186: 913,\n",
       " 1178: 914,\n",
       " 1522: 915,\n",
       " 1232: 916,\n",
       " 572: 917,\n",
       " 1311: 918,\n",
       " 650: 919,\n",
       " 1248: 920,\n",
       " 1136: 921,\n",
       " 266: 922,\n",
       " 909: 923,\n",
       " 680: 924,\n",
       " 1617: 925,\n",
       " 1062: 926,\n",
       " 1234: 927,\n",
       " 1127: 928,\n",
       " 329: 929,\n",
       " 1434: 930,\n",
       " 351: 931,\n",
       " 352: 932,\n",
       " 335: 933,\n",
       " 972: 934,\n",
       " 140: 935,\n",
       " 365: 936,\n",
       " 1508: 937,\n",
       " 832: 938,\n",
       " 110: 939,\n",
       " 85: 940,\n",
       " 1428: 941,\n",
       " 645: 942,\n",
       " 1044: 943,\n",
       " 941: 944,\n",
       " 1180: 945,\n",
       " 570: 946,\n",
       " 928: 947,\n",
       " 1078: 948,\n",
       " 533: 949,\n",
       " 805: 950,\n",
       " 811: 951,\n",
       " 90: 952,\n",
       " 1056: 953,\n",
       " 1100: 954,\n",
       " 1170: 955,\n",
       " 1017: 956,\n",
       " 1069: 957,\n",
       " 865: 958,\n",
       " 1218: 959,\n",
       " 906: 960,\n",
       " 465: 961,\n",
       " 679: 962,\n",
       " 953: 963,\n",
       " 922: 964,\n",
       " 6: 965,\n",
       " 1119: 966,\n",
       " 445: 967,\n",
       " 671: 968,\n",
       " 608: 969,\n",
       " 444: 970,\n",
       " 416: 971,\n",
       " 373: 972,\n",
       " 54: 973,\n",
       " 620: 974,\n",
       " 391: 975,\n",
       " 719: 976,\n",
       " 809: 977,\n",
       " 551: 978,\n",
       " 563: 979,\n",
       " 206: 980,\n",
       " 601: 981,\n",
       " 565: 982,\n",
       " 1558: 983,\n",
       " 1172: 984,\n",
       " 1086: 985,\n",
       " 104: 986,\n",
       " 1167: 987,\n",
       " 1400: 988,\n",
       " 586: 989,\n",
       " 743: 990,\n",
       " 929: 991,\n",
       " 112: 992,\n",
       " 771: 993,\n",
       " 1034: 994,\n",
       " 758: 995,\n",
       " 1217: 996,\n",
       " 930: 997,\n",
       " 1294: 998,\n",
       " 360: 999,\n",
       " ...}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(943, 1682)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_ids = user_list\n",
    "item_ids = item_list\n",
    "\n",
    "len(user_ids), len(item_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_df = data_loader.load_items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: tensor([[0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 2: tensor([[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 3: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 4: tensor([[0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 5: tensor([[0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 6: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 7: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]]), 8: tensor([[0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 9: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 10: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]]), 11: tensor([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 12: tensor([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 13: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 14: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 15: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 16: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 17: tensor([[0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0]]), 18: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 19: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 20: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 21: tensor([[0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]]), 22: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]]), 23: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 24: tensor([[0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 25: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 26: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 27: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 28: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 29: tensor([[0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 30: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 31: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0]]), 32: tensor([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 33: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0]]), 34: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 35: tensor([[0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 36: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 37: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 38: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0]]), 39: tensor([[0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]]), 40: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 41: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 42: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 43: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 44: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 45: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 46: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 47: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 48: tensor([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 49: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 50: tensor([[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0]]), 51: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1]]), 52: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 53: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 54: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 55: tensor([[0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0]]), 56: tensor([[0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 57: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 58: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 59: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 60: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 61: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 62: tensor([[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]]), 63: tensor([[0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 64: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 65: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 66: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 67: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 68: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0]]), 69: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0]]), 70: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 71: tensor([[0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]]), 72: tensor([[0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 73: tensor([[0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]]), 74: tensor([[0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 75: tensor([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 76: tensor([[0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 77: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 78: tensor([[0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 79: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 80: tensor([[0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]]), 81: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 82: tensor([[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]]), 83: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 84: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0]]), 85: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 86: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 87: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 88: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 89: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0]]), 90: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0]]), 91: tensor([[0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]]), 92: tensor([[0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 93: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 94: tensor([[0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 95: tensor([[0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]]), 96: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0]]), 97: tensor([[0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]]), 98: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 99: tensor([[0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]]), 100: tensor([[0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 101: tensor([[0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0]]), 102: tensor([[0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 103: tensor([[0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]]), 104: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 105: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 106: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 107: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 108: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 109: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]]), 110: tensor([[0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]]), 111: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 112: tensor([[0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 113: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 114: tensor([[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 115: tensor([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 116: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 117: tensor([[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 118: tensor([[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 119: tensor([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 120: tensor([[0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 121: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0]]), 122: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 123: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]), 124: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]]), 125: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 126: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 127: tensor([[0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 128: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 129: tensor([[0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0]]), 130: tensor([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 131: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 132: tensor([[0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]]), 133: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0]]), 134: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 135: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0]]), 136: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 137: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 138: tensor([[0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 139: tensor([[0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 140: tensor([[0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 141: tensor([[0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0]]), 142: tensor([[0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]]), 143: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]]), 144: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 145: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0]]), 146: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 147: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 148: tensor([[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 149: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 150: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 151: tensor([[0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 152: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]]), 153: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 154: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 155: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0]]), 156: tensor([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 157: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]]), 158: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 159: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]]), 160: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 161: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 162: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 163: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 164: tensor([[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0]]), 165: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 166: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 167: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 168: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 169: tensor([[0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 170: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 171: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]]), 172: tensor([[0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0]]), 173: tensor([[0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 174: tensor([[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 175: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]]), 176: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0]]), 177: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]]), 178: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 179: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]]), 180: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]]), 181: tensor([[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0]]), 182: tensor([[0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 183: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0]]), 184: tensor([[0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0]]), 185: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0]]), 186: tensor([[0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]]), 187: tensor([[0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 188: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]]), 189: tensor([[0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 190: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]]), 191: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]]), 192: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 193: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 194: tensor([[0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 195: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0]]), 196: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 197: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 198: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 199: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]]), 200: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]), 201: tensor([[0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]), 202: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 203: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]]), 204: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]]), 205: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]]), 206: tensor([[0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0]]), 207: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 208: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]), 209: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]]), 210: tensor([[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 211: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]]), 212: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 213: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 214: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0]]), 215: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 216: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 217: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0]]), 218: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 219: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]), 220: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 221: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 222: tensor([[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]]), 223: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 224: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 225: tensor([[0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 226: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 227: tensor([[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]]), 228: tensor([[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]]), 229: tensor([[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]]), 230: tensor([[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]]), 231: tensor([[0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 232: tensor([[0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]]), 233: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 234: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]), 235: tensor([[0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0]]), 236: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 237: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 238: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 239: tensor([[0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]]), 240: tensor([[0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 241: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0]]), 242: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 243: tensor([[0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 244: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 245: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0]]), 246: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 247: tensor([[0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 248: tensor([[0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 249: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 250: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]]), 251: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 252: tensor([[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0]]), 253: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 254: tensor([[0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 255: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 256: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 257: tensor([[0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]]), 258: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]]), 259: tensor([[0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 260: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0]]), 261: tensor([[0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 262: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 263: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 264: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0]]), 265: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 266: tensor([[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 267: tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 268: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 269: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 270: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0]]), 271: tensor([[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0]]), 272: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 273: tensor([[0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 274: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 275: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 276: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 277: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 278: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 279: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 280: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 281: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 282: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 283: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 284: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 285: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 286: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0]]), 287: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 288: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0]]), 289: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]]), 290: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 291: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]]), 292: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 293: tensor([[0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 294: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 295: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 296: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 297: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 298: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0]]), 299: tensor([[0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]]), 300: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 301: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 302: tensor([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]]), 303: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 304: tensor([[0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 305: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 306: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 307: tensor([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0]]), 308: tensor([[0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 309: tensor([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 310: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 311: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0]]), 312: tensor([[0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]]), 313: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 314: tensor([[0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 315: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 316: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 317: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 318: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]]), 319: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0]]), 320: tensor([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 321: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 322: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]]), 323: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 324: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]]), 325: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 326: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]]), 327: tensor([[0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]]), 328: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0]]), 329: tensor([[0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 330: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 331: tensor([[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 332: tensor([[0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 333: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]]), 334: tensor([[0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]]), 335: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 336: tensor([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 337: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 338: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 339: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 340: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 341: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 342: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]]), 343: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0]]), 344: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 345: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 346: tensor([[0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 347: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 348: tensor([[0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 349: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 350: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]]), 351: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]), 352: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]]), 353: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0]]), 354: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 355: tensor([[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0]]), 356: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]]), 357: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 358: tensor([[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0]]), 359: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 360: tensor([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 361: tensor([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 362: tensor([[0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]]), 363: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 364: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 365: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 366: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 367: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 368: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 369: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 370: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 371: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 372: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 373: tensor([[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]]), 374: tensor([[0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 375: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 376: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 377: tensor([[0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 378: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 379: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]), 380: tensor([[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]]), 381: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 382: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 383: tensor([[0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 384: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 385: tensor([[0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 386: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 387: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 388: tensor([[0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 389: tensor([[0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 390: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 391: tensor([[0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 392: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 393: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 394: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0]]), 395: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 396: tensor([[0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]), 397: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 398: tensor([[0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]]), 399: tensor([[0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 400: tensor([[0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 401: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 402: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0]]), 403: tensor([[0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 404: tensor([[0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 405: tensor([[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]]), 406: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0]]), 407: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 408: tensor([[0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 409: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 410: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 411: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0]]), 412: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 413: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]), 414: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 415: tensor([[0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]]), 416: tensor([[0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 417: tensor([[0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 418: tensor([[0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]]), 419: tensor([[0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]]), 420: tensor([[0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]]), 421: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 422: tensor([[0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 423: tensor([[0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0]]), 424: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]), 425: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 426: tensor([[0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0]]), 427: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 428: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 429: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]]), 430: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]]), 431: tensor([[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 432: tensor([[0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]]), 433: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 434: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]]), 435: tensor([[0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]]), 436: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]), 437: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]), 438: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]), 439: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]), 440: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]), 441: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]), 442: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]), 443: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]), 444: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0]]), 445: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]), 446: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]), 447: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]), 448: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]), 449: tensor([[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]]), 450: tensor([[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]]), 451: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0]]), 452: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]), 453: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]), 454: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 455: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 456: tensor([[0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 457: tensor([[0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 458: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 459: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 460: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 461: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 462: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 463: tensor([[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 464: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 465: tensor([[0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 466: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 467: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 468: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 469: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 470: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]]), 471: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]]), 472: tensor([[0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 473: tensor([[0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]]), 474: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0]]), 475: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 476: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 477: tensor([[0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 478: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 479: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]]), 480: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 481: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 482: tensor([[0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 483: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0]]), 484: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0]]), 485: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0]]), 486: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 487: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 488: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]]), 489: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0]]), 490: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0]]), 491: tensor([[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 492: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 493: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]]), 494: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 495: tensor([[0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 496: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 497: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 498: tensor([[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0]]), 499: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 500: tensor([[0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 501: tensor([[0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]]), 502: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]]), 503: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 504: tensor([[0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 505: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]]), 506: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 507: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 508: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 509: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 510: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]]), 511: tensor([[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]]), 512: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 513: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]]), 514: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 515: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]]), 516: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 517: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 518: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 519: tensor([[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 520: tensor([[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]]), 521: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]]), 522: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 523: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 524: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 525: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0]]), 526: tensor([[0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 527: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 528: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]]), 529: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 530: tensor([[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 531: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 532: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 533: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]]), 534: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 535: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 536: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 537: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 538: tensor([[0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]]), 539: tensor([[0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 540: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 541: tensor([[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 542: tensor([[0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0]]), 543: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]]), 544: tensor([[0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 545: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 546: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 547: tensor([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 548: tensor([[0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 549: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0]]), 550: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 551: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]), 552: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]]), 553: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 554: tensor([[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 555: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 556: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]]), 557: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]]), 558: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 559: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]), 560: tensor([[0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0]]), 561: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]), 562: tensor([[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]]), 563: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]), 564: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]), 565: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0]]), 566: tensor([[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 567: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]), 568: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0]]), 569: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]), 570: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]]), 571: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 572: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 573: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0]]), 574: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0]]), 575: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]]), 576: tensor([[0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 577: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]]), 578: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]]), 579: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 580: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 581: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 582: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 583: tensor([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 584: tensor([[0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 585: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 586: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 587: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]]), 588: tensor([[0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]]), 589: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]]), 590: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0]]), 591: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 592: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]]), 593: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]]), 594: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 595: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 596: tensor([[0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]]), 597: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 598: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 599: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 600: tensor([[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 601: tensor([[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]]), 602: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0]]), 603: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]]), 604: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 605: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]]), 606: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 607: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0]]), 608: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0]]), 609: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 610: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]]), 611: tensor([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0]]), 612: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 613: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 614: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 615: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 616: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0]]), 617: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 618: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 619: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 620: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 621: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]]), 622: tensor([[0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 623: tensor([[0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 624: tensor([[0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]]), 625: tensor([[0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 626: tensor([[0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 627: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 628: tensor([[0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 629: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]]), 630: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]]), 631: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0]]), 632: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 633: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 634: tensor([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 635: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]), 636: tensor([[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0]]), 637: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]), 638: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 639: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 640: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 641: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]]), 642: tensor([[0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]]), 643: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 644: tensor([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 645: tensor([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 646: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]]), 647: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]]), 648: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 649: tensor([[0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 650: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 651: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]]), 652: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 653: tensor([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0]]), 654: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]]), 655: tensor([[0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 656: tensor([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0]]), 657: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0]]), 658: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 659: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]]), 660: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 661: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]]), 662: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 663: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 664: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 665: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0]]), 666: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]), 667: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]), 668: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]), 669: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]), 670: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0]]), 671: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]), 672: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]), 673: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0]]), 674: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]), 675: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]), 676: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 677: tensor([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 678: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 679: tensor([[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 680: tensor([[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 681: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]), 682: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0]]), 683: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 684: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 685: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 686: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 687: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]]), 688: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 689: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 690: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]]), 691: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0]]), 692: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 693: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 694: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 695: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 696: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 697: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 698: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 699: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 700: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 701: tensor([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 702: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 703: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 704: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 705: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0]]), 706: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]), 707: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 708: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 709: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 710: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 711: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 712: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 713: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 714: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 715: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 716: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 717: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 718: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 719: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]]), 720: tensor([[0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 721: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 722: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 723: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 724: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 725: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 726: tensor([[0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 727: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 728: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]]), 729: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 730: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 731: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 732: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 733: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 734: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 735: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 736: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 737: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 738: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 739: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 740: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 741: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 742: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 743: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 744: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]]), 745: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 746: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 747: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 748: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0]]), 749: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 750: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 751: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0]]), 752: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 753: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 754: tensor([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 755: tensor([[0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0]]), 756: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 757: tensor([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 758: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0]]), 759: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 760: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]]), 761: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 762: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 763: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 764: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 765: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 766: tensor([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 767: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]), 768: tensor([[0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 769: tensor([[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0]]), 770: tensor([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]]), 771: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0]]), 772: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 773: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 774: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]), 775: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 776: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 777: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]), 778: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 779: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 780: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 781: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 782: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 783: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 784: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]), 785: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 786: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 787: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 788: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0]]), 789: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 790: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 791: tensor([[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 792: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 793: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 794: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 795: tensor([[0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 796: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 797: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]]), 798: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 799: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 800: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0]]), 801: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 802: tensor([[0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 803: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]]), 804: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 805: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]]), 806: tensor([[0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 807: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 808: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 809: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]]), 810: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 811: tensor([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 812: tensor([[0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 813: tensor([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 814: tensor([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 815: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 816: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]), 817: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 818: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 819: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 820: tensor([[0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 821: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 822: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 823: tensor([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0]]), 824: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 825: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0]]), 826: tensor([[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 827: tensor([[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 828: tensor([[0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 829: tensor([[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 830: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]]), 831: tensor([[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0]]), 832: tensor([[0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 833: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 834: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0]]), 835: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0]]), 836: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 837: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 838: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 839: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0]]), 840: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]]), 841: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 842: tensor([[0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 843: tensor([[0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 844: tensor([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 845: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 846: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 847: tensor([[0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 848: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0]]), 849: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 850: tensor([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 851: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 852: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 853: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]), 854: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]), 855: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0]]), 856: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 857: tensor([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 858: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]), 859: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]), 860: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0]]), 861: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]), 862: tensor([[0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 863: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 864: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 865: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 866: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 867: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 868: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 869: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 870: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 871: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 872: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 873: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 874: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 875: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 876: tensor([[0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 877: tensor([[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 878: tensor([[0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]]), 879: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0]]), 880: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 881: tensor([[0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 882: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 883: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 884: tensor([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 885: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]), 886: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0]]), 887: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 888: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 889: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 890: tensor([[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 891: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]]), 892: tensor([[0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 893: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 894: tensor([[0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 895: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0]]), 896: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 897: tensor([[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]]), 898: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 899: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 900: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 901: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 902: tensor([[0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]]), 903: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 904: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 905: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 906: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 907: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 908: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 909: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 910: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 911: tensor([[0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 912: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 913: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 914: tensor([[0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]]), 915: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 916: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0]]), 917: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 918: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 919: tensor([[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]]), 920: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 921: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 922: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]]), 923: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 924: tensor([[0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 925: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0]]), 926: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 927: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 928: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]), 929: tensor([[0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 930: tensor([[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 931: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0]]), 932: tensor([[0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 933: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 934: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 935: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]]), 936: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 937: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 938: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 939: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 940: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 941: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 942: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 943: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 944: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]]), 945: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0]]), 946: tensor([[0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 947: tensor([[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 948: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 949: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 950: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 951: tensor([[0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 952: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 953: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 954: tensor([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 955: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 956: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 957: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 958: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 959: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 960: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 961: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 962: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 963: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 964: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 965: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]]), 966: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 967: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 968: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]]), 969: tensor([[0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 970: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 971: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]]), 972: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 973: tensor([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 974: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 975: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 976: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0]]), 977: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 978: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]]), 979: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 980: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 981: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 982: tensor([[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 983: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 984: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 985: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 986: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 987: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 988: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 989: tensor([[0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]]), 990: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 991: tensor([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 992: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 993: tensor([[0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]]), 994: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 995: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 996: tensor([[0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 997: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 998: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 999: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1000: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]]), 1001: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1002: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1003: tensor([[0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]]), 1004: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]]), 1005: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1006: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]]), 1007: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1008: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1009: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1010: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1011: tensor([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1012: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1013: tensor([[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 1014: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1015: tensor([[0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1016: tensor([[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 1017: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1018: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1019: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 1020: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]]), 1021: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1022: tensor([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1023: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1024: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 1025: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 1026: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1027: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1028: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 1029: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1030: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1031: tensor([[0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1032: tensor([[0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1033: tensor([[0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1034: tensor([[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1035: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1036: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1037: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0]]), 1038: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 1039: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1040: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 1041: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 1042: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]]), 1043: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1044: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1045: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1046: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 1047: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1048: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 1049: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1050: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 1051: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1052: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]), 1053: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1054: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1055: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1056: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]), 1057: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1058: tensor([[0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1059: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1060: tensor([[0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1061: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1062: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1063: tensor([[0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1064: tensor([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]]), 1065: tensor([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]]), 1066: tensor([[0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1067: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1068: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1069: tensor([[0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 1070: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1071: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1072: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 1073: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 1074: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1075: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1076: tensor([[0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1077: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 1078: tensor([[0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1079: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]]), 1080: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1081: tensor([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1082: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1083: tensor([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 1084: tensor([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1085: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 1086: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1087: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1088: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1089: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0]]), 1090: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 1091: tensor([[0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]]), 1092: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1093: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1094: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1095: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1096: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 1097: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1098: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1099: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1100: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 1101: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]]), 1102: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 1103: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1104: tensor([[0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1105: tensor([[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 1106: tensor([[0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1107: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1108: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1109: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 1110: tensor([[0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0]]), 1111: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1112: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1113: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1114: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1115: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 1116: tensor([[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1117: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1118: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1119: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 1120: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1121: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]]), 1122: tensor([[0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1123: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1124: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0]]), 1125: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 1126: tensor([[0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1127: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1128: tensor([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1129: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0]]), 1130: tensor([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1131: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 1132: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 1133: tensor([[0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1134: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1135: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]]), 1136: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1137: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 1138: tensor([[0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1139: tensor([[0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 1140: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1141: tensor([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1142: tensor([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1143: tensor([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 1144: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1145: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1146: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1147: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1148: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1149: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1150: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1151: tensor([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1152: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0]]), 1153: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]]), 1154: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]]), 1155: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 1156: tensor([[0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1157: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]), 1158: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1159: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0]]), 1160: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 1161: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1162: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1163: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1164: tensor([[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1165: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1166: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1167: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1168: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1169: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1170: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1171: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1172: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1173: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 1174: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 1175: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 1176: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]]), 1177: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1178: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1179: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1180: tensor([[0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1181: tensor([[0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1182: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1183: tensor([[0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1184: tensor([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1185: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]]), 1186: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1187: tensor([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1188: tensor([[0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]]), 1189: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1190: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 1191: tensor([[0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1192: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1193: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1194: tensor([[0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1195: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1196: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1197: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1198: tensor([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 1199: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]), 1200: tensor([[0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1201: tensor([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1202: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1203: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0]]), 1204: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]]), 1205: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1206: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1207: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 1208: tensor([[0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 1209: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1210: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0]]), 1211: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 1212: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0]]), 1213: tensor([[0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 1214: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1215: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]]), 1216: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 1217: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 1218: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1219: tensor([[0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 1220: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1221: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1222: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1223: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1224: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1225: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1226: tensor([[0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1227: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1228: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1229: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 1230: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1231: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1232: tensor([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1233: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1234: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1235: tensor([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1236: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1237: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1238: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1239: tensor([[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 1240: tensor([[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]]), 1241: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1242: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1243: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]), 1244: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1245: tensor([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1246: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1247: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]]), 1248: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 1249: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1250: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1251: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1252: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1253: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 1254: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1255: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1256: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1257: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1258: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 1259: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 1260: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1261: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1262: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 1263: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1264: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1265: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1266: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1267: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1268: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1269: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 1270: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1271: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1272: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1273: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 1274: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0]]), 1275: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 1276: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1277: tensor([[0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1278: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]]), 1279: tensor([[0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1280: tensor([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1281: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1282: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1283: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1284: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]]), 1285: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1286: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0]]), 1287: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1288: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1289: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 1290: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 1291: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1292: tensor([[0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1293: tensor([[0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0]]), 1294: tensor([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1295: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1296: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1297: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 1298: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]]), 1299: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 1300: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 1301: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1302: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1303: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1304: tensor([[0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1305: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1306: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1307: tensor([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1308: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1309: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1310: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1311: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1312: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1313: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]]), 1314: tensor([[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 1315: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 1316: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1317: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1318: tensor([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1319: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1320: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1321: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1322: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1323: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1324: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 1325: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1326: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1327: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1328: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1329: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1330: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1331: tensor([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1332: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1333: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1334: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1335: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1336: tensor([[0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1337: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1338: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1339: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1340: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 1341: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1342: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1343: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1344: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1345: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1346: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1347: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1348: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1349: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1350: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1351: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1352: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1353: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 1354: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1355: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1356: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1357: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0]]), 1358: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1359: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1360: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1361: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1362: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1363: tensor([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1364: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1365: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1366: tensor([[0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1367: tensor([[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1368: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1369: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1370: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 1371: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]), 1372: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1373: tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1374: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1375: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1376: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1377: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 1378: tensor([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1379: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 1380: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1381: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1382: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1383: tensor([[0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1384: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1385: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 1386: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]]), 1387: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 1388: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1389: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1390: tensor([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1391: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1392: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1393: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 1394: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 1395: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1396: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1397: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1398: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1399: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 1400: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 1401: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1402: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1403: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1404: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1405: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1406: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 1407: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1408: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1409: tensor([[0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1410: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1411: tensor([[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]]), 1412: tensor([[0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1413: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1414: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1415: tensor([[0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1416: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]]), 1417: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1418: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1419: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]]), 1420: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1421: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1422: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0]]), 1423: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]]), 1424: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 1425: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1426: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1427: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1428: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1429: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 1430: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1431: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 1432: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1433: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1434: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 1435: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1436: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 1437: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1438: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1439: tensor([[0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1440: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1441: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 1442: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1443: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1444: tensor([[0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]]), 1445: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1446: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1447: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1448: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1449: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1450: tensor([[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 1451: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 1452: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]]), 1453: tensor([[0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1454: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]]), 1455: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]]), 1456: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1457: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1458: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0]]), 1459: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]]), 1460: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1461: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1462: tensor([[0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 1463: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1464: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1465: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1466: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1467: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1468: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1469: tensor([[0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1470: tensor([[0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1471: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 1472: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]]), 1473: tensor([[0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1474: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 1475: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1476: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]]), 1477: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0]]), 1478: tensor([[0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1479: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1480: tensor([[0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1481: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1482: tensor([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1483: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 1484: tensor([[0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1485: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0]]), 1486: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1487: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 1488: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1489: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1490: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1491: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 1492: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1493: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 1494: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1495: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1496: tensor([[0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1497: tensor([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1498: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1499: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1500: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1501: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]]), 1502: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 1503: tensor([[0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1504: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1505: tensor([[0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1506: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1507: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1508: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 1509: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1510: tensor([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1511: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1512: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1513: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1514: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1515: tensor([[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 1516: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1517: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1518: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1519: tensor([[0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1520: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]), 1521: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 1522: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 1523: tensor([[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1524: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1525: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 1526: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0]]), 1527: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1528: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1529: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]]), 1530: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1531: tensor([[0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1532: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1533: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1534: tensor([[0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1535: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1536: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1537: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1538: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1539: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1540: tensor([[0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1541: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1542: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1543: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1544: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1545: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 1546: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1547: tensor([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1548: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 1549: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 1550: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1551: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1552: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1553: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]]), 1554: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1555: tensor([[0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1556: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 1557: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1558: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1559: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 1560: tensor([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1561: tensor([[0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1562: tensor([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1563: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 1564: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1565: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1566: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1567: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1568: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 1569: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1570: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1571: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1572: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1573: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]), 1574: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]]), 1575: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1576: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1577: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1578: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1579: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 1580: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 1581: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]]), 1582: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]]), 1583: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1584: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1585: tensor([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1586: tensor([[0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1587: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]]), 1588: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1589: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1590: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1591: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1592: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1593: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1594: tensor([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1595: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 1596: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0]]), 1597: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1598: tensor([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 1599: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1600: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1601: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 1602: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1603: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1604: tensor([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0]]), 1605: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1606: tensor([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1607: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1608: tensor([[0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1609: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1610: tensor([[0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 1611: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1612: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 1613: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1614: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1615: tensor([[0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1616: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1617: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 1618: tensor([[0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1619: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1620: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1621: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 1622: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1623: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1624: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 1625: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0]]), 1626: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1627: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1628: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1629: tensor([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1630: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1631: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1632: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]]), 1633: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1634: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1635: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1636: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1637: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1638: tensor([[0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1639: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1640: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1641: tensor([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1642: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1643: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1644: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1645: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1646: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1647: tensor([[0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1648: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1649: tensor([[0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1650: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1651: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 1652: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 1653: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1654: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1655: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 1656: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 1657: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1658: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1659: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1660: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1661: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1662: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 1663: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]]), 1664: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1665: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1666: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1667: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1668: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1669: tensor([[0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]]), 1670: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 1671: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1672: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1673: tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]), 1674: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1675: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1676: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1677: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1678: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1679: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0]]), 1680: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), 1681: tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 1682: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "'''Generate one-hot encoding for movie genre'''\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Number of genre columns (k)\n",
    "num_genres = len(items_df.columns) - 1\n",
    "\n",
    "# Create a dictionary to store the hot encodings\n",
    "hot_encoding_dict = {}\n",
    "\n",
    "# Iterate over each row in the dataframe\n",
    "for index, row in items_df.iterrows():\n",
    "    movie_id = row['movie_id']\n",
    "    genres = [row[col] for col in row.index if col.startswith('genre_')]\n",
    "\n",
    "    # Create hot encoding for the genres\n",
    "    hot_encoding = [1 if genre == 1 else 0 for genre in genres]\n",
    "\n",
    "    # Store the hot encoding in the dictionary\n",
    "    hot_encoding_dict[movie_id] = torch.tensor(hot_encoding).unsqueeze(0)\n",
    "\n",
    "print(hot_encoding_dict)\n",
    "\n",
    "torch.save(hot_encoding_dict, \"movie_genre_hot_embeddings.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[97,\n",
       " 266,\n",
       " 811,\n",
       " 24,\n",
       " 31,\n",
       " 281,\n",
       " 569,\n",
       " 260,\n",
       " 332,\n",
       " 324,\n",
       " 423,\n",
       " 468,\n",
       " 287,\n",
       " 894,\n",
       " 869,\n",
       " 639,\n",
       " 539,\n",
       " 500,\n",
       " 482,\n",
       " 335,\n",
       " 849,\n",
       " 771,\n",
       " 926,\n",
       " 40,\n",
       " 364,\n",
       " 765,\n",
       " 308,\n",
       " 445,\n",
       " 714,\n",
       " 705,\n",
       " 77,\n",
       " 818,\n",
       " 596,\n",
       " 372,\n",
       " 166,\n",
       " 756,\n",
       " 251,\n",
       " 883,\n",
       " 437,\n",
       " 240,\n",
       " 108,\n",
       " 68,\n",
       " 175,\n",
       " 159,\n",
       " 140,\n",
       " 755,\n",
       " 732,\n",
       " 307,\n",
       " 533,\n",
       " 695,\n",
       " 803,\n",
       " 64,\n",
       " 236,\n",
       " 766,\n",
       " 900,\n",
       " 87,\n",
       " 693,\n",
       " 724,\n",
       " 779,\n",
       " 517,\n",
       " 661,\n",
       " 588,\n",
       " 327,\n",
       " 219,\n",
       " 210,\n",
       " 657,\n",
       " 71,\n",
       " 929,\n",
       " 650,\n",
       " 469,\n",
       " 847,\n",
       " 200,\n",
       " 480,\n",
       " 448,\n",
       " 522,\n",
       " 676,\n",
       " 941,\n",
       " 137,\n",
       " 209,\n",
       " 626,\n",
       " 214,\n",
       " 573,\n",
       " 356,\n",
       " 622,\n",
       " 73,\n",
       " 478,\n",
       " 45,\n",
       " 892,\n",
       " 591,\n",
       " 614,\n",
       " 507,\n",
       " 267,\n",
       " 169,\n",
       " 333,\n",
       " 452,\n",
       " 310,\n",
       " 261,\n",
       " 607,\n",
       " 939,\n",
       " 823,\n",
       " 434,\n",
       " 328,\n",
       " 731,\n",
       " 886,\n",
       " 303,\n",
       " 618,\n",
       " 61,\n",
       " 248,\n",
       " 708,\n",
       " 410,\n",
       " 211,\n",
       " 670,\n",
       " 389,\n",
       " 67,\n",
       " 352,\n",
       " 276,\n",
       " 524,\n",
       " 560,\n",
       " 381,\n",
       " 262,\n",
       " 449,\n",
       " 897,\n",
       " 586,\n",
       " 255,\n",
       " 282,\n",
       " 853,\n",
       " 498,\n",
       " 121,\n",
       " 111,\n",
       " 616,\n",
       " 425,\n",
       " 885,\n",
       " 193,\n",
       " 893,\n",
       " 930,\n",
       " 542,\n",
       " 371,\n",
       " 589,\n",
       " 426,\n",
       " 138,\n",
       " 312,\n",
       " 729,\n",
       " 306,\n",
       " 706,\n",
       " 508,\n",
       " 523,\n",
       " 863,\n",
       " 674,\n",
       " 395,\n",
       " 535,\n",
       " 79,\n",
       " 599,\n",
       " 794,\n",
       " 781,\n",
       " 430,\n",
       " 606,\n",
       " 529,\n",
       " 854,\n",
       " 66,\n",
       " 142,\n",
       " 785,\n",
       " 527,\n",
       " 775,\n",
       " 627,\n",
       " 311,\n",
       " 745,\n",
       " 681,\n",
       " 383,\n",
       " 612,\n",
       " 50,\n",
       " 686,\n",
       " 405,\n",
       " 413,\n",
       " 851,\n",
       " 830,\n",
       " 826,\n",
       " 938,\n",
       " 199,\n",
       " 543,\n",
       " 34,\n",
       " 32,\n",
       " 704,\n",
       " 450,\n",
       " 360,\n",
       " 833,\n",
       " 552,\n",
       " 110,\n",
       " 78,\n",
       " 216,\n",
       " 245,\n",
       " 83,\n",
       " 317,\n",
       " 351,\n",
       " 466,\n",
       " 345,\n",
       " 3,\n",
       " 102,\n",
       " 454,\n",
       " 632,\n",
       " 232,\n",
       " 6,\n",
       " 55,\n",
       " 322,\n",
       " 907,\n",
       " 353,\n",
       " 788,\n",
       " 98,\n",
       " 759,\n",
       " 774,\n",
       " 901,\n",
       " 600,\n",
       " 904,\n",
       " 665,\n",
       " 888,\n",
       " 26,\n",
       " 85,\n",
       " 11,\n",
       " 604,\n",
       " 334,\n",
       " 429,\n",
       " 668,\n",
       " 942,\n",
       " 119,\n",
       " 319,\n",
       " 520,\n",
       " 30,\n",
       " 56,\n",
       " 673,\n",
       " 320,\n",
       " 623,\n",
       " 723,\n",
       " 197,\n",
       " 553,\n",
       " 82,\n",
       " 678,\n",
       " 658,\n",
       " 584,\n",
       " 299,\n",
       " 828,\n",
       " 329,\n",
       " 205,\n",
       " 212,\n",
       " 570,\n",
       " 797,\n",
       " 315,\n",
       " 291,\n",
       " 463,\n",
       " 603,\n",
       " 440,\n",
       " 228,\n",
       " 807,\n",
       " 905,\n",
       " 297,\n",
       " 752,\n",
       " 689,\n",
       " 60,\n",
       " 343,\n",
       " 880,\n",
       " 409,\n",
       " 399,\n",
       " 548,\n",
       " 8,\n",
       " 156,\n",
       " 891,\n",
       " 421,\n",
       " 577,\n",
       " 735,\n",
       " 534,\n",
       " 293,\n",
       " 544,\n",
       " 879,\n",
       " 363,\n",
       " 483,\n",
       " 549,\n",
       " 871,\n",
       " 594,\n",
       " 631,\n",
       " 229,\n",
       " 922,\n",
       " 688,\n",
       " 213,\n",
       " 80,\n",
       " 149,\n",
       " 721,\n",
       " 443,\n",
       " 300,\n",
       " 424,\n",
       " 295,\n",
       " 134,\n",
       " 582,\n",
       " 651,\n",
       " 418,\n",
       " 1,\n",
       " 666,\n",
       " 515,\n",
       " 313,\n",
       " 864,\n",
       " 877,\n",
       " 495,\n",
       " 173,\n",
       " 848,\n",
       " 347,\n",
       " 366,\n",
       " 850,\n",
       " 91,\n",
       " 884,\n",
       " 182,\n",
       " 275,\n",
       " 843,\n",
       " 70,\n",
       " 292,\n",
       " 132,\n",
       " 301,\n",
       " 663,\n",
       " 145,\n",
       " 136,\n",
       " 165,\n",
       " 29,\n",
       " 530,\n",
       " 194,\n",
       " 737,\n",
       " 881,\n",
       " 170,\n",
       " 433,\n",
       " 141,\n",
       " 174,\n",
       " 7,\n",
       " 605,\n",
       " 870,\n",
       " 74,\n",
       " 644,\n",
       " 362,\n",
       " 239,\n",
       " 146,\n",
       " 908,\n",
       " 235,\n",
       " 221,\n",
       " 918,\n",
       " 602,\n",
       " 133,\n",
       " 934,\n",
       " 358,\n",
       " 186,\n",
       " 42,\n",
       " 868,\n",
       " 109,\n",
       " 839,\n",
       " 57,\n",
       " 835,\n",
       " 733,\n",
       " 531,\n",
       " 25,\n",
       " 406,\n",
       " 484,\n",
       " 919,\n",
       " 339,\n",
       " 52,\n",
       " 458,\n",
       " 866,\n",
       " 597,\n",
       " 265,\n",
       " 494,\n",
       " 789,\n",
       " 778,\n",
       " 935,\n",
       " 19,\n",
       " 814,\n",
       " 536,\n",
       " 378,\n",
       " 368,\n",
       " 84,\n",
       " 62,\n",
       " 382,\n",
       " 273,\n",
       " 286,\n",
       " 361,\n",
       " 355,\n",
       " 555,\n",
       " 279,\n",
       " 13,\n",
       " 183,\n",
       " 369,\n",
       " 431,\n",
       " 619,\n",
       " 224,\n",
       " 540,\n",
       " 659,\n",
       " 441,\n",
       " 479,\n",
       " 550,\n",
       " 754,\n",
       " 177,\n",
       " 635,\n",
       " 501,\n",
       " 933,\n",
       " 457,\n",
       " 164,\n",
       " 249,\n",
       " 598,\n",
       " 834,\n",
       " 376,\n",
       " 75,\n",
       " 114,\n",
       " 537,\n",
       " 528,\n",
       " 391,\n",
       " 105,\n",
       " 115,\n",
       " 412,\n",
       " 93,\n",
       " 532,\n",
       " 90,\n",
       " 337,\n",
       " 932,\n",
       " 867,\n",
       " 878,\n",
       " 571,\n",
       " 906,\n",
       " 576,\n",
       " 95,\n",
       " 12,\n",
       " 397,\n",
       " 502,\n",
       " 44,\n",
       " 43,\n",
       " 330,\n",
       " 168,\n",
       " 491,\n",
       " 825,\n",
       " 875,\n",
       " 713,\n",
       " 101,\n",
       " 427,\n",
       " 179,\n",
       " 417,\n",
       " 656,\n",
       " 831,\n",
       " 621,\n",
       " 178,\n",
       " 396,\n",
       " 696,\n",
       " 784,\n",
       " 645,\n",
       " 384,\n",
       " 258,\n",
       " 496,\n",
       " 336,\n",
       " 579,\n",
       " 16,\n",
       " 4,\n",
       " 486,\n",
       " 257,\n",
       " 514,\n",
       " 715,\n",
       " 667,\n",
       " 465,\n",
       " 394,\n",
       " 223,\n",
       " 180,\n",
       " 290,\n",
       " 545,\n",
       " 325,\n",
       " 558,\n",
       " 10,\n",
       " 250,\n",
       " 23,\n",
       " 357,\n",
       " 222,\n",
       " 629,\n",
       " 812,\n",
       " 927,\n",
       " 750,\n",
       " 341,\n",
       " 432,\n",
       " 519,\n",
       " 786,\n",
       " 546,\n",
       " 204,\n",
       " 595,\n",
       " 446,\n",
       " 238,\n",
       " 94,\n",
       " 526,\n",
       " 817,\n",
       " 634,\n",
       " 790,\n",
       " 285,\n",
       " 185,\n",
       " 758,\n",
       " 921,\n",
       " 435,\n",
       " 654,\n",
       " 154,\n",
       " 76,\n",
       " 451,\n",
       " 690,\n",
       " 757,\n",
       " 278,\n",
       " 69,\n",
       " 447,\n",
       " 592,\n",
       " 487,\n",
       " 189,\n",
       " 272,\n",
       " 698,\n",
       " 583,\n",
       " 237,\n",
       " 488,\n",
       " 89,\n",
       " 801,\n",
       " 118,\n",
       " 126,\n",
       " 858,\n",
       " 513,\n",
       " 824,\n",
       " 559,\n",
       " 127,\n",
       " 117,\n",
       " 474,\n",
       " 653,\n",
       " 58,\n",
       " 636,\n",
       " 859,\n",
       " 370,\n",
       " 269,\n",
       " 47,\n",
       " 350,\n",
       " 196,\n",
       " 943,\n",
       " 787,\n",
       " 694,\n",
       " 264,\n",
       " 444,\n",
       " 637,\n",
       " 305,\n",
       " 342,\n",
       " 911,\n",
       " 150,\n",
       " 125,\n",
       " 742,\n",
       " 51,\n",
       " 354,\n",
       " 873,\n",
       " 143,\n",
       " 471,\n",
       " 400,\n",
       " 590,\n",
       " 321,\n",
       " 20,\n",
       " 763,\n",
       " 746,\n",
       " 762,\n",
       " 408,\n",
       " 538,\n",
       " 585,\n",
       " 39,\n",
       " 176,\n",
       " 246,\n",
       " 782,\n",
       " 630,\n",
       " 711,\n",
       " 809,\n",
       " 155,\n",
       " 288,\n",
       " 568,\n",
       " 18,\n",
       " 128,\n",
       " 323,\n",
       " 256,\n",
       " 751,\n",
       " 909,\n",
       " 191,\n",
       " 116,\n",
       " 581,\n",
       " 181,\n",
       " 302,\n",
       " 716,\n",
       " 718,\n",
       " 683,\n",
       " 753,\n",
       " 652,\n",
       " 518,\n",
       " 928,\n",
       " 46,\n",
       " 856,\n",
       " 158,\n",
       " 937,\n",
       " 172,\n",
       " 17,\n",
       " 512,\n",
       " 49,\n",
       " 915,\n",
       " 793,\n",
       " 516,\n",
       " 887,\n",
       " 481,\n",
       " 284,\n",
       " 617,\n",
       " 226,\n",
       " 27,\n",
       " 738,\n",
       " 438,\n",
       " 896,\n",
       " 365,\n",
       " 230,\n",
       " 38,\n",
       " 910,\n",
       " 375,\n",
       " 470,\n",
       " 912,\n",
       " 802,\n",
       " 685,\n",
       " 195,\n",
       " 805,\n",
       " 815,\n",
       " 504,\n",
       " 914,\n",
       " 899,\n",
       " 580,\n",
       " 913,\n",
       " 163,\n",
       " 923,\n",
       " 153,\n",
       " 842,\n",
       " 671,\n",
       " 712,\n",
       " 112,\n",
       " 227,\n",
       " 821,\n",
       " 104,\n",
       " 422,\n",
       " 420,\n",
       " 707,\n",
       " 587,\n",
       " 736,\n",
       " 120,\n",
       " 54,\n",
       " 152,\n",
       " 404,\n",
       " 890,\n",
       " 208,\n",
       " 796,\n",
       " 719,\n",
       " 9,\n",
       " 761,\n",
       " 37,\n",
       " 453,\n",
       " 254,\n",
       " 304,\n",
       " 855,\n",
       " 572,\n",
       " 624,\n",
       " 669,\n",
       " 679,\n",
       " 263,\n",
       " 611,\n",
       " 298,\n",
       " 415,\n",
       " 151,\n",
       " 744,\n",
       " 641,\n",
       " 837,\n",
       " 551,\n",
       " 747,\n",
       " 489,\n",
       " 148,\n",
       " 147,\n",
       " 680,\n",
       " 895,\n",
       " 697,\n",
       " 660,\n",
       " 349,\n",
       " 464,\n",
       " 326,\n",
       " 187,\n",
       " 124,\n",
       " 804,\n",
       " 609,\n",
       " 144,\n",
       " 903,\n",
       " 198,\n",
       " 610,\n",
       " 280,\n",
       " 294,\n",
       " 401,\n",
       " 123,\n",
       " 184,\n",
       " 203,\n",
       " 439,\n",
       " 247,\n",
       " 416,\n",
       " 722,\n",
       " 840,\n",
       " 836,\n",
       " 130,\n",
       " 638,\n",
       " 403,\n",
       " 740,\n",
       " 844,\n",
       " 860,\n",
       " 220,\n",
       " 642,\n",
       " 862,\n",
       " 709,\n",
       " 760,\n",
       " 865,\n",
       " 625,\n",
       " 791,\n",
       " 717,\n",
       " 387,\n",
       " 916,\n",
       " 510,\n",
       " 268,\n",
       " 810,\n",
       " 442,\n",
       " 497,\n",
       " 113,\n",
       " 692,\n",
       " 233,\n",
       " 820,\n",
       " 608,\n",
       " 672,\n",
       " 374,\n",
       " 925,\n",
       " 795,\n",
       " 234,\n",
       " 741,\n",
       " 677,\n",
       " 318,\n",
       " 649,\n",
       " 411,\n",
       " 846,\n",
       " 710,\n",
       " 359,\n",
       " 259,\n",
       " 628,\n",
       " 633,\n",
       " 283,\n",
       " 377,\n",
       " 385,\n",
       " 225,\n",
       " 898,\n",
       " 768,\n",
       " 473,\n",
       " 348,\n",
       " 506,\n",
       " 640,\n",
       " 931,\n",
       " 874,\n",
       " 852,\n",
       " 620,\n",
       " 917,\n",
       " 646,\n",
       " 798,\n",
       " 557,\n",
       " 902,\n",
       " 578,\n",
       " 86,\n",
       " 243,\n",
       " 699,\n",
       " 160,\n",
       " 525,\n",
       " 36,\n",
       " 541,\n",
       " 171,\n",
       " 655,\n",
       " 838,\n",
       " 808,\n",
       " 799,\n",
       " 889,\n",
       " 734,\n",
       " 96,\n",
       " 564,\n",
       " 241,\n",
       " 743,\n",
       " 575,\n",
       " 691,\n",
       " 461,\n",
       " 554,\n",
       " 827,\n",
       " 207,\n",
       " 393,\n",
       " 398,\n",
       " 767,\n",
       " 800,\n",
       " 218,\n",
       " 5,\n",
       " 769,\n",
       " 643,\n",
       " 845,\n",
       " 613,\n",
       " 739,\n",
       " 547,\n",
       " 726,\n",
       " 684,\n",
       " 99,\n",
       " 728,\n",
       " 574,\n",
       " 407,\n",
       " 503,\n",
       " 48,\n",
       " 33,\n",
       " 780,\n",
       " 201,\n",
       " 135,\n",
       " 28,\n",
       " 829,\n",
       " 231,\n",
       " 490,\n",
       " 773,\n",
       " 379,\n",
       " 289,\n",
       " 419,\n",
       " 675,\n",
       " 392,\n",
       " 593,\n",
       " 499,\n",
       " 139,\n",
       " 63,\n",
       " 472,\n",
       " 648,\n",
       " 129,\n",
       " 920,\n",
       " 521,\n",
       " 65,\n",
       " 813,\n",
       " 15,\n",
       " 157,\n",
       " 41,\n",
       " 493,\n",
       " 380,\n",
       " 188,\n",
       " 764,\n",
       " 217,\n",
       " 792,\n",
       " 53,\n",
       " 338,\n",
       " 749,\n",
       " 720,\n",
       " 725,\n",
       " 296,\n",
       " 702,\n",
       " 252,\n",
       " 727,\n",
       " 462,\n",
       " 456,\n",
       " 940,\n",
       " 816,\n",
       " 270,\n",
       " 202,\n",
       " 162,\n",
       " 556,\n",
       " 730,\n",
       " 402,\n",
       " 703,\n",
       " 477,\n",
       " 822,\n",
       " 772,\n",
       " 106,\n",
       " 566,\n",
       " 390,\n",
       " 2,\n",
       " 882,\n",
       " 562,\n",
       " 81,\n",
       " 206,\n",
       " 35,\n",
       " 776,\n",
       " 509,\n",
       " 428,\n",
       " 455,\n",
       " 367,\n",
       " 92,\n",
       " 340,\n",
       " 565,\n",
       " 346,\n",
       " 777,\n",
       " 242,\n",
       " 14,\n",
       " 316,\n",
       " 601,\n",
       " 388,\n",
       " 274,\n",
       " 167,\n",
       " 841,\n",
       " 936,\n",
       " 647,\n",
       " 819,\n",
       " 485,\n",
       " 924,\n",
       " 505,\n",
       " 832,\n",
       " 244,\n",
       " 567,\n",
       " 876,\n",
       " 563,\n",
       " 687,\n",
       " 190,\n",
       " 783,\n",
       " 700,\n",
       " 476,\n",
       " 682,\n",
       " 511,\n",
       " 59,\n",
       " 475,\n",
       " 561,\n",
       " 857,\n",
       " 748,\n",
       " 253,\n",
       " 22,\n",
       " 314,\n",
       " 460,\n",
       " 161,\n",
       " 277,\n",
       " 192,\n",
       " 386,\n",
       " 806,\n",
       " 414,\n",
       " 492,\n",
       " 344,\n",
       " 770,\n",
       " 309,\n",
       " 662,\n",
       " 131,\n",
       " 664,\n",
       " 872,\n",
       " 100,\n",
       " 373,\n",
       " 88,\n",
       " 459,\n",
       " 331,\n",
       " 215,\n",
       " 467,\n",
       " 122,\n",
       " 615,\n",
       " 21,\n",
       " 701,\n",
       " 72,\n",
       " 107,\n",
       " 271,\n",
       " 861,\n",
       " 436,\n",
       " 103]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xiaoyicong\\AppData\\Local\\Temp\\ipykernel_27288\\721580580.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  item_input_emb = torch.load(\"movie_genre_hot_embeddings.pt\")\n"
     ]
    }
   ],
   "source": [
    "user_input_emb = {}\n",
    "item_input_emb = {}\n",
    "\n",
    "for user_id in user_ids:\n",
    "    user_input_emb[user_id] = torch.cat((umam_embeddings[user_id], umdm_embeddings[user_id], umum_embeddings[user_id]), dim = 1)\n",
    "\n",
    "\n",
    "item_input_emb_unordered = torch.load(\"movie_genre_hot_embeddings.pt\")\n",
    "\n",
    "for item_id in item_ids:\n",
    "    item_input_emb[item_id] = item_input_emb_unordered[item_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1682"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{97: tensor([[  0., 193., 115.,  ...,  44., 125.,  58.]]),\n",
       " 266: tensor([[ 0., 58., 26.,  ..., 21., 45., 13.]]),\n",
       " 811: tensor([[ 0., 72., 32.,  ..., 19., 42.,  7.]]),\n",
       " 24: tensor([[  0., 271., 114.,  ...,  50., 136.,  62.]]),\n",
       " 31: tensor([[ 0., 76., 36.,  ..., 18., 70., 16.]]),\n",
       " 281: tensor([[ 0., 88., 40.,  ..., 26., 50.,  7.]]),\n",
       " 569: tensor([[  0., 312., 130.,  ...,  56., 140.,  47.]]),\n",
       " 260: tensor([[ 0., 93., 27.,  ..., 20., 40.,  8.]]),\n",
       " 332: tensor([[   0., 1102.,  466.,  ...,  112.,  363.,  127.]]),\n",
       " 324: tensor([[  0., 297.,  92.,  ...,  59., 132.,  27.]]),\n",
       " 423: tensor([[  0., 308., 105.,  ...,  55., 124.,  22.]]),\n",
       " 468: tensor([[  0., 495., 215.,  ...,  92., 284., 120.]]),\n",
       " 287: tensor([[  0., 319., 122.,  ...,  49., 124.,  50.]]),\n",
       " 894: tensor([[  0., 729., 270.,  ..., 167., 445., 112.]]),\n",
       " 869: tensor([[  0., 171.,  69.,  ...,  33.,  93.,  23.]]),\n",
       " 639: tensor([[  0., 310., 169.,  ...,  77., 289., 119.]]),\n",
       " 539: tensor([[  0., 118.,  50.,  ...,  35., 109.,  37.]]),\n",
       " 500: tensor([[  0., 706., 321.,  ..., 135., 442., 167.]]),\n",
       " 482: tensor([[  0., 113.,  48.,  ...,  25.,  52.,  11.]]),\n",
       " 335: tensor([[  0., 119.,  47.,  ...,  21.,  44.,   8.]]),\n",
       " 849: tensor([[ 0., 79., 35.,  ..., 17., 46., 19.]]),\n",
       " 771: tensor([[  0., 207., 104.,  ...,  51., 140.,  56.]]),\n",
       " 926: tensor([[ 0., 76., 40.,  ..., 18., 40., 11.]]),\n",
       " 40: tensor([[  0., 135.,  61.,  ...,  29.,  65.,  12.]]),\n",
       " 364: tensor([[ 0., 57., 24.,  ..., 16., 40.,  8.]]),\n",
       " 765: tensor([[ 0., 57., 19.,  ..., 19., 45., 16.]]),\n",
       " 308: tensor([[   0., 1217.,  528.,  ...,  206.,  777.,  313.]]),\n",
       " 445: tensor([[  0., 592., 231.,  ..., 135., 252.,  82.]]),\n",
       " 714: tensor([[  0., 246., 102.,  ...,  40.,  88.,  32.]]),\n",
       " 705: tensor([[  0., 625., 315.,  ...,  64., 228., 103.]]),\n",
       " 77: tensor([[  0., 316., 168.,  ...,  55., 144.,  63.]]),\n",
       " 818: tensor([[ 0., 97., 42.,  ..., 18., 38.,  9.]]),\n",
       " 596: tensor([[ 0., 87., 48.,  ..., 19., 40., 13.]]),\n",
       " 372: tensor([[  0., 304.,  85.,  ...,  35., 131.,  43.]]),\n",
       " 166: tensor([[  0., 101.,  37.,  ...,  20.,  38.,  10.]]),\n",
       " 756: tensor([[  0., 440., 220.,  ...,  65., 213.,  95.]]),\n",
       " 251: tensor([[  0., 375., 167.,  ...,  63., 154.,  58.]]),\n",
       " 883: tensor([[  0., 847., 351.,  ..., 154., 516., 201.]]),\n",
       " 437: tensor([[  0., 727., 325.,  ..., 132., 520., 220.]]),\n",
       " 240: tensor([[ 0., 99., 43.,  ..., 22., 47., 12.]]),\n",
       " 108: tensor([[  0., 144.,  60.,  ...,  25.,  66.,  21.]]),\n",
       " 68: tensor([[  0., 170.,  69.,  ...,  28.,  68.,  22.]]),\n",
       " 175: tensor([[  0., 172.,  68.,  ...,  28.,  76.,  33.]]),\n",
       " 159: tensor([[  0., 493., 175.,  ...,  81., 211.,  50.]]),\n",
       " 140: tensor([[ 0., 63., 28.,  ..., 19., 41.,  9.]]),\n",
       " 755: tensor([[  0., 164.,  61.,  ...,  35.,  73.,  11.]]),\n",
       " 732: tensor([[ 0., 75., 35.,  ..., 17., 37.,  5.]]),\n",
       " 307: tensor([[  0., 376., 229.,  ...,  65., 224., 102.]]),\n",
       " 533: tensor([[  0., 984., 451.,  ..., 160., 516., 197.]]),\n",
       " 695: tensor([[  0., 127.,  51.,  ...,  31.,  66.,  17.]]),\n",
       " 803: tensor([[  0., 109.,  48.,  ...,  28.,  63.,   9.]]),\n",
       " 64: tensor([[  0., 800., 368.,  ..., 120., 399., 177.]]),\n",
       " 236: tensor([[  0., 467., 212.,  ...,  96., 245.,  87.]]),\n",
       " 766: tensor([[  0., 474., 241.,  ...,  85., 348., 143.]]),\n",
       " 900: tensor([[  0., 126.,  51.,  ...,  27.,  88.,  25.]]),\n",
       " 87: tensor([[  0., 914., 404.,  ..., 108., 419., 181.]]),\n",
       " 693: tensor([[  0., 684., 300.,  ...,  97., 308., 128.]]),\n",
       " 724: tensor([[  0., 283., 120.,  ...,  71., 145.,  28.]]),\n",
       " 779: tensor([[  0., 180.,  81.,  ...,  28.,  74.,  26.]]),\n",
       " 517: tensor([[  0., 163.,  94.,  ...,  28.,  73.,  23.]]),\n",
       " 661: tensor([[  0., 409., 220.,  ...,  83., 242., 106.]]),\n",
       " 588: tensor([[  0., 833., 400.,  ..., 111., 449., 185.]]),\n",
       " 327: tensor([[  0., 994., 397.,  ..., 165., 562., 223.]]),\n",
       " 219: tensor([[ 0., 84., 31.,  ..., 16., 50., 20.]]),\n",
       " 210: tensor([[  0., 493., 224.,  ...,  86., 264., 117.]]),\n",
       " 657: tensor([[  0., 144.,  51.,  ...,  28.,  57.,  17.]]),\n",
       " 71: tensor([[  0., 153.,  64.,  ...,  33.,  76.,  32.]]),\n",
       " 929: tensor([[  0., 183.,  84.,  ...,  40.,  98.,  41.]]),\n",
       " 650: tensor([[   0., 1095.,  536.,  ...,  162.,  619.,  252.]]),\n",
       " 469: tensor([[ 0., 89., 51.,  ..., 26., 85., 30.]]),\n",
       " 847: tensor([[  0., 506., 223.,  ...,  88., 292., 115.]]),\n",
       " 200: tensor([[  0., 935., 455.,  ..., 109., 432., 179.]]),\n",
       " 480: tensor([[  0., 216.,  92.,  ...,  50., 116.,  52.]]),\n",
       " 448: tensor([[ 0., 89., 35.,  ..., 25., 66., 10.]]),\n",
       " 522: tensor([[ 0., 85., 39.,  ..., 23., 60., 28.]]),\n",
       " 676: tensor([[  0., 312., 143.,  ...,  64., 142.,  43.]]),\n",
       " 941: tensor([[  0., 115.,  48.,  ...,  22.,  44.,  19.]]),\n",
       " 137: tensor([[  0., 327., 157.,  ...,  41.,  93.,  32.]]),\n",
       " 209: tensor([[ 0., 92., 36.,  ..., 27., 58., 15.]]),\n",
       " 626: tensor([[  0., 114.,  53.,  ...,  30.,  63.,  14.]]),\n",
       " 214: tensor([[  0., 411., 176.,  ...,  95., 256.,  99.]]),\n",
       " 573: tensor([[  0., 150.,  55.,  ...,  39., 102.,  38.]]),\n",
       " 356: tensor([[  0., 112.,  45.,  ...,  22.,  46.,   9.]]),\n",
       " 622: tensor([[  0., 939., 431.,  ..., 105., 452., 196.]]),\n",
       " 73: tensor([[  0., 169.,  62.,  ...,  56., 129.,  56.]]),\n",
       " 478: tensor([[  0., 407., 157.,  ...,  63., 226.,  94.]]),\n",
       " 45: tensor([[  0., 192.,  81.,  ...,  32.,  96.,  30.]]),\n",
       " 892: tensor([[  0., 850., 434.,  ..., 124., 451., 193.]]),\n",
       " 591: tensor([[  0., 242., 102.,  ...,  46., 167.,  70.]]),\n",
       " 614: tensor([[  0., 167.,  59.,  ...,  33.,  75.,  21.]]),\n",
       " 507: tensor([[  0., 305., 134.,  ...,  49., 111.,  26.]]),\n",
       " 267: tensor([[  0., 938., 401.,  ...,  86., 370., 169.]]),\n",
       " 169: tensor([[  0., 103.,  57.,  ...,  30.,  73.,  24.]]),\n",
       " 333: tensor([[  0., 108.,  47.,  ...,  25.,  51.,  18.]]),\n",
       " 452: tensor([[  0., 536., 277.,  ..., 206., 398., 161.]]),\n",
       " 310: tensor([[ 0., 67., 38.,  ..., 17., 40., 12.]]),\n",
       " 261: tensor([[  0., 122.,  49.,  ...,  23.,  50.,   7.]]),\n",
       " 607: tensor([[ 0., 93., 38.,  ..., 25., 74., 26.]]),\n",
       " 939: tensor([[  0., 208.,  81.,  ...,  37.,  96.,  21.]]),\n",
       " 823: tensor([[  0., 775., 321.,  ..., 105., 370., 170.]]),\n",
       " 434: tensor([[  0., 189.,  65.,  ...,  30.,  88.,  22.]]),\n",
       " 328: tensor([[   0., 1370.,  564.,  ...,  150.,  553.,  213.]]),\n",
       " 731: tensor([[  0., 160.,  87.,  ...,  53., 166.,  60.]]),\n",
       " 886: tensor([[   0., 1044.,  423.,  ...,  111.,  480.,  212.]]),\n",
       " 303: tensor([[   0., 1853.,  746.,  ...,  237.,  968.,  370.]]),\n",
       " 618: tensor([[  0., 831., 376.,  ..., 121., 431., 182.]]),\n",
       " 61: tensor([[  0., 101.,  41.,  ...,  21.,  40.,   6.]]),\n",
       " 248: tensor([[  0., 282., 125.,  ...,  48., 112.,  49.]]),\n",
       " 708: tensor([[  0., 406., 170.,  ...,  68., 205.,  44.]]),\n",
       " 410: tensor([[ 0., 98., 39.,  ..., 24., 52.,  9.]]),\n",
       " 211: tensor([[  0., 153.,  87.,  ...,  27.,  69.,  24.]]),\n",
       " 670: tensor([[  0., 117.,  61.,  ...,  31.,  91.,  38.]]),\n",
       " 389: tensor([[  0., 711., 316.,  ..., 136., 538., 206.]]),\n",
       " 67: tensor([[  0., 151.,  63.,  ...,  21.,  60.,  24.]]),\n",
       " 352: tensor([[  0., 303., 141.,  ...,  28.,  84.,  42.]]),\n",
       " 276: tensor([[   0., 2191.,  893.,  ...,  226., 1036.,  415.]]),\n",
       " 524: tensor([[  0., 974., 457.,  ..., 163., 609., 236.]]),\n",
       " 560: tensor([[  0., 321., 129.,  ...,  74., 195.,  68.]]),\n",
       " 381: tensor([[  0., 299., 162.,  ...,  66., 246.,  91.]]),\n",
       " 262: tensor([[  0., 542., 243.,  ...,  92., 318., 121.]]),\n",
       " 449: tensor([[  0., 196.,  77.,  ...,  51., 142.,  55.]]),\n",
       " 897: tensor([[  0., 761., 371.,  ..., 117., 370., 152.]]),\n",
       " 586: tensor([[   0., 1012.,  406.,  ...,   72.,  332.,  149.]]),\n",
       " 255: tensor([[  0., 411., 149.,  ...,  50., 165.,  58.]]),\n",
       " 282: tensor([[ 0., 90., 40.,  ..., 19., 44., 10.]]),\n",
       " 853: tensor([[  0., 181.,  72.,  ...,  40.,  79.,  11.]]),\n",
       " 498: tensor([[  0., 460., 203.,  ...,  92., 295., 117.]]),\n",
       " 121: tensor([[  0., 343., 141.,  ...,  64., 148.,  56.]]),\n",
       " 111: tensor([[ 0., 61., 28.,  ..., 19., 45.,  9.]]),\n",
       " 616: tensor([[  0., 210.,  73.,  ...,  40.,  85.,  13.]]),\n",
       " 425: tensor([[   0., 1024.,  405.,  ...,  124.,  400.,  158.]]),\n",
       " 885: tensor([[  0., 395., 205.,  ...,  62., 204.,  90.]]),\n",
       " 193: tensor([[  0., 525., 232.,  ...,  78., 239.,  87.]]),\n",
       " 893: tensor([[  0., 280., 120.,  ...,  42., 118.,  43.]]),\n",
       " 930: tensor([[  0., 263., 119.,  ...,  46., 125.,  40.]]),\n",
       " 542: tensor([[  0., 466., 191.,  ...,  90., 270., 119.]]),\n",
       " 371: tensor([[  0., 244., 128.,  ...,  40., 102.,  50.]]),\n",
       " 589: tensor([[  0., 166.,  63.,  ...,  40.,  83.,  12.]]),\n",
       " 426: tensor([[  0., 106.,  84.,  ...,  58., 196.,  60.]]),\n",
       " 138: tensor([[  0., 111.,  45.,  ...,  38., 101.,  38.]]),\n",
       " 312: tensor([[  0., 390., 223.,  ..., 119., 437., 147.]]),\n",
       " 729: tensor([[ 0., 93., 35.,  ..., 19., 40.,  8.]]),\n",
       " 306: tensor([[ 0., 64., 18.,  ..., 23., 58., 17.]]),\n",
       " 706: tensor([[  0., 162.,  62.,  ...,  27.,  58.,  20.]]),\n",
       " 508: tensor([[  0., 315., 160.,  ...,  57., 173.,  80.]]),\n",
       " 523: tensor([[  0., 202.,  91.,  ...,  55., 190.,  80.]]),\n",
       " 863: tensor([[  0., 329., 123.,  ...,  79., 168.,  26.]]),\n",
       " 674: tensor([[  0., 226.,  96.,  ...,  37.,  81.,  24.]]),\n",
       " 395: tensor([[  0., 273., 144.,  ...,  44., 114.,  44.]]),\n",
       " 535: tensor([[  0., 532., 251.,  ..., 122., 432., 162.]]),\n",
       " 79: tensor([[  0., 124.,  50.,  ...,  37., 106.,  25.]]),\n",
       " 599: tensor([[  0., 172.,  57.,  ...,  32.,  89.,  19.]]),\n",
       " 794: tensor([[  0., 120.,  44.,  ...,  31.,  76.,  31.]]),\n",
       " 781: tensor([[  0., 201.,  97.,  ...,  37.,  84.,  34.]]),\n",
       " 430: tensor([[  0., 254.,  94.,  ...,  51., 127.,  52.]]),\n",
       " 606: tensor([[  0., 932., 385.,  ..., 156., 468., 188.]]),\n",
       " 529: tensor([[  0., 196.,  71.,  ...,  40.,  80.,  12.]]),\n",
       " 854: tensor([[  0., 753., 303.,  ..., 154., 429., 140.]]),\n",
       " 66: tensor([[  0., 198.,  75.,  ...,  34.,  76.,  32.]]),\n",
       " 142: tensor([[  0., 110.,  47.,  ...,  25.,  66.,  24.]]),\n",
       " 785: tensor([[  0., 114.,  51.,  ...,  25.,  51.,  20.]]),\n",
       " 527: tensor([[  0., 387., 155.,  ...,  91., 266., 115.]]),\n",
       " 775: tensor([[  0., 122.,  56.,  ...,  26.,  55.,  10.]]),\n",
       " 627: tensor([[   0., 1013.,  413.,  ...,   93.,  361.,  160.]]),\n",
       " 311: tensor([[   0., 1086.,  535.,  ...,  143.,  586.,  253.]]),\n",
       " 745: tensor([[  0., 193.,  84.,  ...,  47., 120.,  50.]]),\n",
       " 681: tensor([[ 0., 67., 34.,  ..., 21., 40.,  6.]]),\n",
       " 383: tensor([[  0., 133.,  63.,  ...,  52., 142.,  54.]]),\n",
       " 612: tensor([[  0., 100.,  44.,  ...,  21.,  54.,  19.]]),\n",
       " 50: tensor([[ 0., 65., 19.,  ..., 21., 46., 13.]]),\n",
       " 686: tensor([[  0., 262., 115.,  ...,  54., 142.,  69.]]),\n",
       " 405: tensor([[   0., 1942.,  865.,  ...,  170., 1474.,  737.]]),\n",
       " 413: tensor([[  0., 236.,  93.,  ...,  45.,  99.,  25.]]),\n",
       " 851: tensor([[  0., 923., 344.,  ..., 133., 394., 100.]]),\n",
       " 830: tensor([[  0., 507., 258.,  ...,  62., 212.,  92.]]),\n",
       " 826: tensor([[  0., 686., 306.,  ...,  49., 229., 110.]]),\n",
       " 938: tensor([[  0., 515., 200.,  ...,  87., 216.,  57.]]),\n",
       " 199: tensor([[  0., 112.,  46.,  ...,  34.,  74.,  23.]]),\n",
       " 543: tensor([[  0., 775., 315.,  ..., 112., 388., 197.]]),\n",
       " 34: tensor([[ 0., 78., 41.,  ..., 18., 35.,  5.]]),\n",
       " 32: tensor([[  0., 197.,  81.,  ...,  39.,  82.,  30.]]),\n",
       " 704: tensor([[  0., 205., 104.,  ...,  64., 175.,  60.]]),\n",
       " 450: tensor([[   0., 1716.,  799.,  ...,  238., 1080.,  384.]]),\n",
       " 360: tensor([[  0., 375., 160.,  ...,  79., 199.,  71.]]),\n",
       " 833: tensor([[1.0000e+00, 1.0020e+03, 3.7500e+02,  ..., 1.4000e+02, 5.2100e+02,\n",
       "          2.2400e+02]]),\n",
       " 552: tensor([[  0., 359., 134.,  ...,  63., 164.,  49.]]),\n",
       " 110: tensor([[  0., 657., 262.,  ...,  54., 265., 118.]]),\n",
       " 78: tensor([[ 0., 73., 21.,  ..., 18., 41.,  9.]]),\n",
       " 216: tensor([[  0., 496., 217.,  ...,  76., 262., 117.]]),\n",
       " 245: tensor([[ 0., 96., 55.,  ..., 14., 43., 14.]]),\n",
       " 83: tensor([[  0., 682., 331.,  ...,  87., 308., 119.]]),\n",
       " 317: tensor([[  0., 109.,  41.,  ...,  22.,  43.,  10.]]),\n",
       " 351: tensor([[  0., 196.,  73.,  ...,  37.,  77.,  11.]]),\n",
       " 466: tensor([[  0., 590., 243.,  ...,  71., 188.,  75.]]),\n",
       " 345: tensor([[  0., 843., 379.,  ..., 136., 452., 161.]]),\n",
       " 3: tensor([[  0., 219.,  84.,  ...,  45., 102.,  22.]]),\n",
       " 102: tensor([[   0., 1050.,  462.,  ...,  116.,  432.,  183.]]),\n",
       " 454: tensor([[  0., 822., 409.,  ..., 150., 465., 161.]]),\n",
       " 632: tensor([[  0., 518., 237.,  ...,  76., 236., 104.]]),\n",
       " 232: tensor([[  0., 264., 120.,  ...,  67., 184.,  73.]]),\n",
       " 6: tensor([[  0., 541., 270.,  ..., 156., 418., 152.]]),\n",
       " 55: tensor([[  0., 200.,  87.,  ...,  21.,  42.,  17.]]),\n",
       " 322: tensor([[  0., 173.,  70.,  ...,  37., 102.,  45.]]),\n",
       " 907: tensor([[  0., 527., 255.,  ...,  93., 291.,  86.]]),\n",
       " 353: tensor([[  0., 115.,  57.,  ...,  25.,  50.,   8.]]),\n",
       " 788: tensor([[   0., 1155.,  504.,  ...,  120.,  495.,  199.]]),\n",
       " 98: tensor([[ 0., 61., 27.,  ..., 21., 53., 22.]]),\n",
       " 759: tensor([[  0., 191.,  85.,  ...,  30.,  64.,  22.]]),\n",
       " 774: tensor([[   0., 1025.,  466.,  ...,  105.,  448.,  211.]]),\n",
       " 901: tensor([[  0., 491., 257.,  ...,  76., 244., 103.]]),\n",
       " 600: tensor([[  0., 650., 284.,  ...,  34., 178.,  88.]]),\n",
       " 904: tensor([[  0., 172.,  82.,  ...,  27.,  94.,  35.]]),\n",
       " 665: tensor([[  0., 634., 267.,  ..., 104., 283., 104.]]),\n",
       " 888: tensor([[ 0., 48., 27.,  ..., 18., 40., 13.]]),\n",
       " 26: tensor([[  0., 488., 181.,  ...,  92., 211.,  62.]]),\n",
       " 85: tensor([[  0., 798., 378.,  ..., 171., 563., 224.]]),\n",
       " 11: tensor([[  0., 634., 303.,  ...,  90., 362., 162.]]),\n",
       " 604: tensor([[ 0., 86., 22.,  ..., 13., 54., 26.]]),\n",
       " 334: tensor([[   0., 1031.,  456.,  ...,  191.,  640.,  217.]]),\n",
       " 429: tensor([[   0., 1440.,  637.,  ...,  201.,  825.,  333.]]),\n",
       " 668: tensor([[  0., 200.,  99.,  ...,  37.,  90.,  25.]]),\n",
       " 942: tensor([[  0., 270., 147.,  ...,  58., 157.,  45.]]),\n",
       " 119: tensor([[  0., 831., 336.,  ..., 118., 354., 115.]]),\n",
       " 319: tensor([[ 1., 82., 31.,  ..., 21., 44.,  6.]]),\n",
       " 520: tensor([[ 0., 94., 46.,  ..., 19., 44.,  9.]]),\n",
       " 30: tensor([[  0., 191.,  95.,  ...,  37.,  83.,  27.]]),\n",
       " 56: tensor([[  0., 867., 422.,  ...,  92., 374., 170.]]),\n",
       " 673: tensor([[  0., 147.,  64.,  ...,  31.,  69.,  16.]]),\n",
       " 320: tensor([[  0., 907., 386.,  ...,  79., 308., 126.]]),\n",
       " 623: tensor([[  0., 164.,  87.,  ...,  38.,  90.,  41.]]),\n",
       " 723: tensor([[ 0., 83., 38.,  ..., 19., 44., 18.]]),\n",
       " 197: tensor([[  0., 740., 325.,  ...,  69., 230.,  91.]]),\n",
       " 553: tensor([[  0., 193., 105.,  ...,  65., 198.,  69.]]),\n",
       " 82: tensor([[  0., 520., 253.,  ..., 109., 330., 124.]]),\n",
       " 678: tensor([[  0., 137.,  59.,  ...,  25.,  56.,  19.]]),\n",
       " 658: tensor([[  0., 247., 106.,  ...,  51., 142.,  62.]]),\n",
       " 584: tensor([[  0., 123.,  86.,  ...,  12.,  48.,  24.]]),\n",
       " 299: tensor([[  0., 689., 310.,  ..., 158., 544., 216.]]),\n",
       " 828: tensor([[  0., 164.,  69.,  ...,  52., 186.,  64.]]),\n",
       " 329: tensor([[  0., 293., 127.,  ...,  51., 130.,  45.]]),\n",
       " 205: tensor([[ 0., 97., 36.,  ..., 21., 44.,  8.]]),\n",
       " 212: tensor([[ 0., 51., 25.,  ..., 21., 48., 21.]]),\n",
       " 570: tensor([[ 0., 73., 34.,  ..., 20., 43.,  9.]]),\n",
       " 797: tensor([[  0., 127.,  61.,  ...,  23.,  51.,  12.]]),\n",
       " 315: tensor([[  0., 307., 116.,  ...,  61., 173.,  76.]]),\n",
       " 291: tensor([[   0., 1196.,  497.,  ...,  126.,  587.,  237.]]),\n",
       " 463: tensor([[  0., 423., 172.,  ...,  87., 250.,  60.]]),\n",
       " 603: tensor([[  0., 289., 150.,  ...,  30.,  94.,  43.]]),\n",
       " 440: tensor([[  0., 121.,  59.,  ...,  38.,  91.,  29.]]),\n",
       " 228: tensor([[ 0., 74., 33.,  ..., 17., 40., 14.]]),\n",
       " 807: tensor([[  0., 783., 434.,  ..., 108., 406., 171.]]),\n",
       " 905: tensor([[  0., 196.,  69.,  ...,  36.,  80.,  20.]]),\n",
       " 297: tensor([[  1., 770., 330.,  ..., 134., 383., 155.]]),\n",
       " 752: tensor([[  0., 237.,  99.,  ...,  57., 123.,  23.]]),\n",
       " 689: tensor([[  0., 210.,  78.,  ...,  33.,  72.,  30.]]),\n",
       " 60: tensor([[  0., 518., 280.,  ..., 124., 408., 158.]]),\n",
       " 343: tensor([[  0., 846., 351.,  ..., 135., 470., 197.]]),\n",
       " 880: tensor([[   0., 1630.,  655.,  ...,  173.,  730.,  282.]]),\n",
       " 409: tensor([[  0., 419., 213.,  ..., 128., 382., 133.]]),\n",
       " 399: tensor([[   0., 1295.,  580.,  ...,  137.,  638.,  267.]]),\n",
       " 548: tensor([[  0., 758., 301.,  ..., 124., 303.,  93.]]),\n",
       " 8: tensor([[  0., 375., 176.,  ...,  40., 117.,  48.]]),\n",
       " 156: tensor([[  0., 126.,  44.,  ...,  26.,  74.,  33.]]),\n",
       " 891: tensor([[  0., 220.,  87.,  ...,  30.,  92.,  23.]]),\n",
       " 421: tensor([[  0., 272., 101.,  ...,  43., 124.,  54.]]),\n",
       " 577: tensor([[  0., 884., 387.,  ...,  88., 376., 168.]]),\n",
       " 735: tensor([[  0., 202.,  85.,  ...,  46., 106.,  27.]]),\n",
       " 534: tensor([[  0., 368., 135.,  ...,  56., 158.,  50.]]),\n",
       " 293: tensor([[   0., 1563.,  641.,  ...,  206.,  770.,  316.]]),\n",
       " 544: tensor([[  0., 136.,  63.,  ...,  30.,  61.,  11.]]),\n",
       " 879: tensor([[  0., 163.,  72.,  ...,  26.,  58.,  19.]]),\n",
       " 363: tensor([[   0., 1249.,  542.,  ...,  157.,  614.,  257.]]),\n",
       " 483: tensor([[  0., 230., 134.,  ...,  38., 118.,  45.]]),\n",
       " 549: tensor([[  0., 129.,  61.,  ...,  23.,  50.,  19.]]),\n",
       " 871: tensor([[  0., 439., 196.,  ...,  74., 201.,  67.]]),\n",
       " 594: tensor([[ 0., 82., 48.,  ..., 22., 50., 17.]]),\n",
       " 631: tensor([[ 0., 70., 31.,  ..., 20., 38.,  6.]]),\n",
       " 229: tensor([[  0., 128.,  55.,  ...,  25.,  55.,  13.]]),\n",
       " 922: tensor([[  0., 555., 272.,  ...,  66., 254., 116.]]),\n",
       " 688: tensor([[ 0., 84., 35.,  ..., 18., 40.,  4.]]),\n",
       " 213: tensor([[  0., 549., 242.,  ...,  90., 252., 109.]]),\n",
       " 80: tensor([[ 0., 83., 46.,  ..., 23., 57., 26.]]),\n",
       " 149: tensor([[  0., 128.,  58.,  ...,  27.,  66.,  11.]]),\n",
       " 721: tensor([[  0., 640., 287.,  ..., 116., 315.,  97.]]),\n",
       " 443: tensor([[  0., 104.,  46.,  ...,  21.,  47.,  13.]]),\n",
       " 300: tensor([[ 0., 87., 29.,  ..., 17., 40.,  7.]]),\n",
       " 424: tensor([[  0., 136.,  55.,  ...,  39.,  80.,  21.]]),\n",
       " 295: tensor([[  0., 712., 358.,  ...,  91., 391., 164.]]),\n",
       " 134: tensor([[  0., 105.,  41.,  ...,  24.,  49.,  11.]]),\n",
       " 582: tensor([[  0., 288., 118.,  ...,  51., 124.,  43.]]),\n",
       " 651: tensor([[ 0., 68., 30.,  ..., 17., 37.,  7.]]),\n",
       " 418: tensor([[ 0., 84., 34.,  ..., 17., 37.,  6.]]),\n",
       " 1: tensor([[  1., 957., 402.,  ..., 145., 538., 221.]]),\n",
       " 666: tensor([[  0., 788., 361.,  ..., 159., 488., 190.]]),\n",
       " 515: tensor([[  0., 141.,  65.,  ...,  37.,  75.,  11.]]),\n",
       " 313: tensor([[  0., 911., 431.,  ..., 153., 522., 210.]]),\n",
       " 864: tensor([[   0., 1228.,  523.,  ...,  150.,  587.,  255.]]),\n",
       " 877: tensor([[  0., 273., 116.,  ...,  49., 161.,  66.]]),\n",
       " 495: tensor([[  0., 886., 411.,  ...,  89., 426., 189.]]),\n",
       " 173: tensor([[  0., 172.,  68.,  ...,  36.,  80.,  12.]]),\n",
       " 848: tensor([[  0., 383., 213.,  ...,  93., 298., 126.]]),\n",
       " 347: tensor([[  0., 963., 405.,  ..., 134., 398., 152.]]),\n",
       " 366: tensor([[  0., 109.,  26.,  ...,   9.,  66.,  32.]]),\n",
       " 850: tensor([[  0., 208., 126.,  ...,  44., 102.,  46.]]),\n",
       " 91: tensor([[  0., 361., 185.,  ...,  74., 195.,  70.]]),\n",
       " 884: tensor([[ 0., 77., 24.,  ..., 25., 84., 32.]]),\n",
       " 182: tensor([[ 0., 95., 47.,  ..., 23., 56., 23.]]),\n",
       " 275: tensor([[  0., 387., 238.,  ...,  62., 190.,  90.]]),\n",
       " 843: tensor([[  0., 750., 367.,  ..., 108., 408., 180.]]),\n",
       " 70: tensor([[  0., 513., 277.,  ...,  80., 262., 118.]]),\n",
       " 292: tensor([[  0., 447., 190.,  ...,  94., 247.,  95.]]),\n",
       " 132: tensor([[ 0., 79., 27.,  ..., 16., 43., 15.]]),\n",
       " 301: tensor([[   0., 1223.,  528.,  ...,  148.,  550.,  238.]]),\n",
       " 663: tensor([[  0., 695., 263.,  ..., 116., 312.,  97.]]),\n",
       " 145: tensor([[   0., 1334.,  585.,  ...,  170.,  619.,  197.]]),\n",
       " 136: tensor([[  0., 121.,  44.,  ...,  27.,  70.,  24.]]),\n",
       " 165: tensor([[  0., 101.,  57.,  ...,  23.,  60.,  23.]]),\n",
       " 29: tensor([[  0., 133.,  65.,  ...,  27.,  65.,  21.]]),\n",
       " 530: tensor([[  0., 183.,  85.,  ...,  34.,  89.,  30.]]),\n",
       " 194: tensor([[   0., 1113.,  527.,  ...,  168.,  606.,  263.]]),\n",
       " 737: tensor([[  0., 112.,  48.,  ...,  25.,  66.,  32.]]),\n",
       " 881: tensor([[   0., 1013.,  475.,  ...,  139.,  517.,  214.]]),\n",
       " 170: tensor([[  0., 114.,  43.,  ...,  22.,  44.,   4.]]),\n",
       " 433: tensor([[  0., 149.,  72.,  ...,  34.,  80.,  24.]]),\n",
       " 141: tensor([[  0., 505., 207.,  ...,  83., 210.,  50.]]),\n",
       " 174: tensor([[  0., 535., 241.,  ...,  84., 354., 127.]]),\n",
       " 7: tensor([[   0., 1256.,  596.,  ...,  184.,  795.,  324.]]),\n",
       " 605: tensor([[  0., 376., 171.,  ...,  74., 180.,  59.]]),\n",
       " 870: tensor([[  0., 981., 420.,  ..., 142., 535., 223.]]),\n",
       " 74: tensor([[  0., 144.,  57.,  ...,  37.,  76.,  22.]]),\n",
       " 644: tensor([[  0., 222.,  79.,  ...,  40.,  82.,  18.]]),\n",
       " 362: tensor([[  0., 129.,  49.,  ...,  24.,  49.,  11.]]),\n",
       " 239: tensor([[  0., 334., 165.,  ...,  93., 315., 117.]]),\n",
       " 146: tensor([[  0., 110.,  51.,  ...,  21.,  53.,   9.]]),\n",
       " 908: tensor([[  0., 235., 113.,  ...,  56., 147.,  60.]]),\n",
       " 235: tensor([[  0., 276., 129.,  ...,  61., 182.,  77.]]),\n",
       " 221: tensor([[  0., 752., 318.,  ...,  82., 290., 122.]]),\n",
       " 918: tensor([[  0., 167.,  87.,  ...,  54., 199.,  80.]]),\n",
       " 602: tensor([[  0., 139.,  64.,  ...,  27.,  57.,  17.]]),\n",
       " 133: tensor([[  0., 111.,  54.,  ...,  25.,  50.,  10.]]),\n",
       " 934: tensor([[  0., 483., 239.,  ..., 107., 345., 140.]]),\n",
       " 358: tensor([[ 0., 60., 31.,  ..., 19., 81., 38.]]),\n",
       " 186: tensor([[  0., 504., 189.,  ...,  58., 177.,  56.]]),\n",
       " 42: tensor([[  0., 719., 348.,  ..., 109., 366., 151.]]),\n",
       " 868: tensor([[  0., 801., 372.,  ...,  94., 416., 188.]]),\n",
       " 109: tensor([[   0., 1136.,  494.,  ...,  122.,  468.,  193.]]),\n",
       " 839: tensor([[  0., 228.,  88.,  ...,  45., 109.,  28.]]),\n",
       " 57: tensor([[  0., 446., 184.,  ...,  76., 210.,  68.]]),\n",
       " 835: tensor([[  0., 296., 149.,  ...,  69., 201.,  76.]]),\n",
       " 733: tensor([[  0., 390., 142.,  ...,  71., 197.,  42.]]),\n",
       " 531: tensor([[  0., 124.,  64.,  ...,  26.,  58.,  10.]]),\n",
       " 25: tensor([[  0., 236., 118.,  ...,  55., 156.,  62.]]),\n",
       " 406: tensor([[  0., 926., 407.,  ..., 174., 679., 274.]]),\n",
       " 484: tensor([[  0., 750., 353.,  ...,  89., 278., 129.]]),\n",
       " 919: tensor([[  0., 742., 314.,  ..., 149., 422., 121.]]),\n",
       " 339: tensor([[  0., 926., 369.,  ..., 142., 506., 212.]]),\n",
       " 52: tensor([[  0., 179.,  73.,  ...,  41., 110.,  38.]]),\n",
       " 458: tensor([[  0., 641., 240.,  ..., 131., 366., 128.]]),\n",
       " 866: tensor([[ 0., 61., 27.,  ..., 13., 35.,  9.]]),\n",
       " 597: tensor([[  0., 204.,  76.,  ...,  41.,  79.,  25.]]),\n",
       " 265: tensor([[  0., 271., 106.,  ...,  36.,  90.,  28.]]),\n",
       " 494: tensor([[  0., 183.,  86.,  ...,  41.,  94.,  36.]]),\n",
       " 789: tensor([[  0., 106.,  37.,  ...,  31.,  65.,  21.]]),\n",
       " 778: tensor([[  0., 293., 143.,  ...,  37., 130.,  59.]]),\n",
       " 935: tensor([[  0., 216.,  83.,  ...,  29.,  78.,  24.]]),\n",
       " 19: tensor([[ 0., 68., 41.,  ..., 15., 40., 19.]]),\n",
       " 814: tensor([[  0., 113.,  28.,  ...,  10.,  70.,  34.]]),\n",
       " 536: tensor([[  0., 579., 336.,  ...,  86., 326., 136.]]),\n",
       " 378: tensor([[   0., 1518.,  659.,  ...,  169.,  746.,  302.]]),\n",
       " 368: tensor([[  0., 203.,  72.,  ...,  17.,  90.,  41.]]),\n",
       " 84: tensor([[  0., 359., 152.,  ...,  54., 136.,  49.]]),\n",
       " 62: tensor([[  0., 897., 403.,  ..., 130., 458., 190.]]),\n",
       " 382: tensor([[  0., 153.,  54.,  ...,  53., 102.,  40.]]),\n",
       " 273: tensor([[ 0., 62., 29.,  ..., 16., 44.,  7.]]),\n",
       " 286: tensor([[  0., 944., 427.,  ..., 137., 553., 213.]]),\n",
       " 361: tensor([[  0., 351., 131.,  ...,  74., 242., 106.]]),\n",
       " 355: tensor([[ 0., 84., 33.,  ..., 17., 42., 11.]]),\n",
       " 555: tensor([[  0., 229., 120.,  ...,  43., 104.,  38.]]),\n",
       " 279: tensor([[   0., 1477.,  628.,  ...,  170.,  868.,  318.]]),\n",
       " 13: tensor([[   0., 1988.,  859.,  ...,  290., 1193.,  422.]]),\n",
       " 183: tensor([[  0., 319., 168.,  ...,  30., 106.,  47.]]),\n",
       " 369: tensor([[ 0., 67., 40.,  ..., 20., 43., 12.]]),\n",
       " 431: tensor([[  0., 120.,  51.,  ...,  19.,  42.,   7.]]),\n",
       " 619: tensor([[  0., 648., 258.,  ...,  59., 178.,  70.]]),\n",
       " 224: tensor([[  0., 569., 262.,  ...,  61., 274.,  91.]]),\n",
       " 540: tensor([[  0., 284., 114.,  ...,  52., 126.,  41.]]),\n",
       " 659: tensor([[  0., 617., 289.,  ..., 106., 379., 153.]]),\n",
       " 441: tensor([[ 0., 92., 42.,  ..., 20., 39., 14.]]),\n",
       " 479: tensor([[  0., 804., 387.,  ..., 142., 401., 156.]]),\n",
       " 550: tensor([[  0., 169.,  84.,  ...,  37.,  79.,  19.]]),\n",
       " 754: tensor([[  0., 183.,  62.,  ...,  26.,  66.,  12.]]),\n",
       " 177: tensor([[  0., 553., 224.,  ...,  88., 230.,  89.]]),\n",
       " 635: tensor([[  0., 147.,  54.,  ...,  32.,  65.,  14.]]),\n",
       " 501: tensor([[  0., 335., 124.,  ...,  73., 143.,  43.]]),\n",
       " 933: tensor([[  0., 905., 359.,  ...,  91., 368., 168.]]),\n",
       " 457: tensor([[   0., 1115.,  483.,  ...,  142.,  554.,  236.]]),\n",
       " 164: tensor([[  0., 370., 139.,  ...,  52., 126.,  28.]]),\n",
       " 249: tensor([[  0., 753., 307.,  ..., 110., 321., 132.]]),\n",
       " 598: tensor([[  0., 109.,  44.,  ...,  25.,  49.,  13.]]),\n",
       " 834: tensor([[  0., 247.,  98.,  ...,  51., 107.,  28.]]),\n",
       " 376: tensor([[ 0., 75., 37.,  ..., 26., 60., 20.]]),\n",
       " 75: tensor([[  0., 344., 145.,  ...,  59., 156.,  46.]]),\n",
       " 114: tensor([[  0., 157.,  74.,  ...,  34.,  96.,  40.]]),\n",
       " 537: tensor([[   0., 1331.,  604.,  ...,  254.,  951.,  346.]]),\n",
       " 528: tensor([[  0., 228., 112.,  ...,  39., 104.,  44.]]),\n",
       " 391: tensor([[  0., 428., 177.,  ...,  92., 242.,  96.]]),\n",
       " 105: tensor([[ 0., 93., 33.,  ..., 23., 45., 13.]]),\n",
       " 115: tensor([[  0., 437., 185.,  ...,  62., 182.,  73.]]),\n",
       " 412: tensor([[  0., 226.,  94.,  ...,  33., 102.,  47.]]),\n",
       " 93: tensor([[ 0., 66., 26.,  ..., 15., 40., 11.]]),\n",
       " 532: tensor([[1.0000e+00, 1.0900e+03, 4.9400e+02,  ..., 1.4800e+02, 5.4000e+02,\n",
       "          1.8000e+02]]),\n",
       " 90: tensor([[  0., 673., 301.,  ..., 159., 577., 194.]]),\n",
       " 337: tensor([[  0., 184.,  93.,  ...,  22.,  68.,  29.]]),\n",
       " 932: tensor([[  0., 594., 303.,  ..., 118., 469., 180.]]),\n",
       " 867: tensor([[  0., 446., 196.,  ...,  72., 188.,  81.]]),\n",
       " 878: tensor([[  0., 289., 157.,  ...,  81., 267., 110.]]),\n",
       " 571: tensor([[ 0., 54., 28.,  ..., 12., 40., 15.]]),\n",
       " 906: tensor([[  0., 195.,  68.,  ...,  32.,  80.,  20.]]),\n",
       " 576: tensor([[  0., 167.,  56.,  ...,  32.,  72.,  25.]]),\n",
       " 95: tensor([[   0., 1000.,  493.,  ...,  137.,  554.,  248.]]),\n",
       " 12: tensor([[  0., 219., 107.,  ...,  34., 102.,  45.]]),\n",
       " 397: tensor([[  0., 320., 148.,  ...,  77., 198.,  72.]]),\n",
       " 502: tensor([[  0., 121.,  58.,  ...,  33.,  68.,  11.]]),\n",
       " 44: tensor([[  0., 597., 304.,  ...,  99., 302., 137.]]),\n",
       " 43: tensor([[  0., 871., 390.,  ..., 135., 441., 171.]]),\n",
       " 330: tensor([[  0., 518., 269.,  ...,  87., 292., 124.]]),\n",
       " 168: tensor([[  0., 350., 154.,  ...,  56., 137.,  38.]]),\n",
       " 491: tensor([[ 0., 99., 30.,  ..., 24., 64., 23.]]),\n",
       " 825: tensor([[  0., 689., 282.,  ..., 101., 286.,  71.]]),\n",
       " 875: tensor([[  0., 271., 121.,  ...,  63., 178.,  75.]]),\n",
       " 713: tensor([[  0., 117.,  48.,  ...,  22.,  50.,   8.]]),\n",
       " 101: tensor([[  0., 309., 123.,  ...,  47., 133.,  37.]]),\n",
       " 427: tensor([[  0., 105.,  45.,  ...,  25.,  53.,   9.]]),\n",
       " 179: tensor([[  0., 159.,  64.,  ...,  31.,  77.,  12.]]),\n",
       " 417: tensor([[   0., 1446.,  624.,  ...,  173.,  730.,  306.]]),\n",
       " 656: tensor([[  0., 118.,  41.,  ...,  20.,  45.,   6.]]),\n",
       " 831: tensor([[  0., 373., 160.,  ...,  68., 145.,  49.]]),\n",
       " 621: tensor([[  0., 769., 324.,  ...,  91., 339., 145.]]),\n",
       " 178: tensor([[   0., 1254.,  540.,  ...,  186.,  541.,  189.]]),\n",
       " 396: tensor([[  0., 304., 117.,  ...,  43., 108.,  26.]]),\n",
       " 696: tensor([[  0., 109.,  44.,  ...,  20.,  51.,  13.]]),\n",
       " 784: tensor([[  0., 178.,  76.,  ...,  37.,  75.,  11.]]),\n",
       " 645: tensor([[  0., 356., 141.,  ...,  72., 244., 113.]]),\n",
       " 384: tensor([[  0., 112.,  44.,  ...,  22.,  43.,  10.]]),\n",
       " 258: tensor([[ 0., 90., 42.,  ..., 21., 44., 10.]]),\n",
       " 496: tensor([[  0., 391., 199.,  ...,  68., 256., 106.]]),\n",
       " 336: tensor([[  0., 460., 186.,  ...,  59., 260., 101.]]),\n",
       " 579: tensor([[  0., 273., 129.,  ...,  54., 148.,  63.]]),\n",
       " 16: tensor([[  0., 556., 222.,  ...,  90., 280., 130.]]),\n",
       " 4: tensor([[  0., 112.,  55.,  ...,  18.,  44.,  16.]]),\n",
       " 486: tensor([[  0., 624., 241.,  ..., 133., 336.,  80.]]),\n",
       " 257: tensor([[  0., 114.,  66.,  ...,  37., 108.,  32.]]),\n",
       " 514: tensor([[  0., 641., 326.,  ..., 126., 384., 162.]]),\n",
       " 715: tensor([[  0., 853., 349.,  ...,  89., 334., 151.]]),\n",
       " 667: tensor([[  0., 136.,  64.,  ...,  38.,  92.,  33.]]),\n",
       " 465: tensor([[  0., 205.,  99.,  ...,  56., 157.,  67.]]),\n",
       " 394: tensor([[  0., 781., 364.,  ...,  74., 297., 140.]]),\n",
       " 223: tensor([[  0., 434., 181.,  ...,  74., 203.,  53.]]),\n",
       " 180: tensor([[  0., 250., 117.,  ...,  33., 126.,  59.]]),\n",
       " 290: tensor([[  0., 609., 349.,  ...,  87., 299., 124.]]),\n",
       " 545: tensor([[  0., 809., 408.,  ...,  80., 324., 149.]]),\n",
       " 325: tensor([[  0., 353., 192.,  ...,  82., 279., 108.]]),\n",
       " 558: tensor([[ 0., 38.,  9.,  ..., 15., 38.,  8.]]),\n",
       " 10: tensor([[  0., 527., 226.,  ..., 107., 363., 142.]]),\n",
       " 250: tensor([[  0., 455., 195.,  ...,  93., 241.,  85.]]),\n",
       " 23: tensor([[  0., 505., 247.,  ...,  98., 302., 137.]]),\n",
       " 357: tensor([[  0., 359., 148.,  ...,  49., 151.,  42.]]),\n",
       " 222: tensor([[   0., 1648.,  723.,  ...,  161.,  774.,  320.]]),\n",
       " 629: tensor([[  0., 550., 228.,  ...,  89., 236.,  89.]]),\n",
       " 812: tensor([[  0., 103.,  43.,  ...,  20.,  40.,   7.]]),\n",
       " 927: tensor([[  0., 509., 242.,  ...,  60., 240., 104.]]),\n",
       " 750: tensor([[  0., 130.,  52.,  ...,  29.,  61.,   9.]]),\n",
       " 341: tensor([[ 0., 69., 24.,  ..., 20., 38.,  7.]]),\n",
       " 432: tensor([[  0., 288., 112.,  ...,  54., 124.,  42.]]),\n",
       " 519: tensor([[  0., 148.,  63.,  ...,  36.,  77.,  16.]]),\n",
       " 786: tensor([[  0., 479., 241.,  ...,  83., 234.,  98.]]),\n",
       " 546: tensor([[  0., 261.,  98.,  ...,  33., 117.,  44.]]),\n",
       " 204: tensor([[  0., 113.,  52.,  ...,  35.,  79.,  20.]]),\n",
       " 595: tensor([[  0., 363., 129.,  ...,  73., 185.,  49.]]),\n",
       " 446: tensor([[  0., 148.,  57.,  ...,  30.,  64.,   9.]]),\n",
       " 238: tensor([[  0., 126.,  55.,  ...,  25.,  57.,  18.]]),\n",
       " 94: tensor([[   0., 1585.,  678.,  ...,  187.,  797.,  340.]]),\n",
       " 526: tensor([[  0., 308., 120.,  ...,  63., 134.,  33.]]),\n",
       " 817: tensor([[  0., 207.,  82.,  ...,  35.,  72.,  28.]]),\n",
       " 634: tensor([[  0., 525., 197.,  ..., 101., 265.,  64.]]),\n",
       " 790: tensor([[   0., 1017.,  447.,  ...,  103.,  461.,  195.]]),\n",
       " 285: tensor([[  0., 104.,  49.,  ...,  29.,  64.,  24.]]),\n",
       " 185: tensor([[  0., 141.,  61.,  ...,  32.,  94.,  34.]]),\n",
       " 758: tensor([[   0., 1284.,  542.,  ...,  206.,  706.,  252.]]),\n",
       " 921: tensor([[  0., 438., 247.,  ...,  70., 219.,  76.]]),\n",
       " 435: tensor([[   0., 1492.,  643.,  ...,  191.,  754.,  307.]]),\n",
       " 654: tensor([[  0., 605., 281.,  ..., 101., 292., 114.]]),\n",
       " 154: tensor([[  0., 134.,  62.,  ...,  40., 101.,  43.]]),\n",
       " 76: tensor([[  0., 265., 102.,  ...,  54., 157.,  64.]]),\n",
       " 451: tensor([[  0., 296., 122.,  ...,  75., 157.,  22.]]),\n",
       " 690: tensor([[  0., 509., 207.,  ...,  59., 230., 103.]]),\n",
       " 757: tensor([[  0., 989., 429.,  ...,  94., 332., 152.]]),\n",
       " 278: tensor([[ 0., 67., 39.,  ..., 18., 43., 14.]]),\n",
       " 69: tensor([[  0., 318., 125.,  ...,  58., 124.,  45.]]),\n",
       " 447: tensor([[  0., 642., 269.,  ...,  92., 275., 103.]]),\n",
       " 592: tensor([[   0., 1193.,  487.,  ...,  252.,  686.,  215.]]),\n",
       " 487: tensor([[   0., 1156.,  475.,  ...,  120.,  458.,  192.]]),\n",
       " 189: tensor([[  0., 450., 219.,  ..., 113., 366., 131.]]),\n",
       " 272: tensor([[  0., 206.,  93.,  ...,  34., 102.,  46.]]),\n",
       " 698: tensor([[  0., 339., 184.,  ...,  82., 252.,  94.]]),\n",
       " 583: tensor([[ 0., 93., 47.,  ..., 17., 54., 24.]]),\n",
       " 237: tensor([[  0., 111.,  43.,  ...,  37.,  96.,  42.]]),\n",
       " 488: tensor([[  0., 559., 282.,  ..., 111., 307., 114.]]),\n",
       " 89: tensor([[  0., 224.,  99.,  ...,  50., 151.,  58.]]),\n",
       " 801: tensor([[  0., 111.,  54.,  ...,  26.,  51.,  10.]]),\n",
       " 118: tensor([[  0., 265., 106.,  ...,  40., 142.,  66.]]),\n",
       " 126: tensor([[  0., 190.,  80.,  ...,  37.,  84.,  14.]]),\n",
       " 858: tensor([[  0., 103.,  30.,  ...,  19.,  40.,   6.]]),\n",
       " 513: tensor([[  0., 176.,  85.,  ...,  20.,  44.,  19.]]),\n",
       " 824: tensor([[ 0., 70., 33.,  ..., 19., 38.,  7.]]),\n",
       " 559: tensor([[  0., 328., 147.,  ...,  56., 150.,  65.]]),\n",
       " 127: tensor([[  0., 106.,  72.,  ...,  15.,  46.,  18.]]),\n",
       " 117: tensor([[  0., 375., 163.,  ...,  67., 170.,  69.]]),\n",
       " 474: tensor([[  0., 941., 453.,  ..., 198., 647., 242.]]),\n",
       " 653: tensor([[   0., 1270.,  562.,  ...,  146.,  566.,  238.]]),\n",
       " 58: tensor([[  0., 445., 209.,  ...,  93., 299., 122.]]),\n",
       " 636: tensor([[ 0., 54., 29.,  ..., 14., 40., 13.]]),\n",
       " 859: tensor([[  0., 115.,  43.,  ...,  25.,  72.,  20.]]),\n",
       " 370: tensor([[  0., 243., 124.,  ...,  51., 149.,  54.]]),\n",
       " 269: tensor([[  0., 960., 390.,  ..., 168., 641., 257.]]),\n",
       " 47: tensor([[ 0., 68., 25.,  ..., 19., 45.,  7.]]),\n",
       " 350: tensor([[  0., 185., 107.,  ...,  37., 100.,  40.]]),\n",
       " 196: tensor([[ 0., 81., 32.,  ..., 24., 75., 27.]]),\n",
       " 943: tensor([[  0., 847., 382.,  ...,  83., 334., 152.]]),\n",
       " 787: tensor([[  0., 189.,  82.,  ...,  43.,  97.,  20.]]),\n",
       " 694: tensor([[  0., 457., 252.,  ...,  93., 311., 122.]]),\n",
       " 264: tensor([[  0., 299., 110.,  ...,  60., 236., 101.]]),\n",
       " 444: tensor([[  0., 106.,  46.,  ...,  19.,  43.,  12.]]),\n",
       " 637: tensor([[  0., 452., 172.,  ...,  80., 201.,  48.]]),\n",
       " 305: tensor([[  0., 686., 310.,  ..., 136., 437., 179.]]),\n",
       " 342: tensor([[  0., 538., 207.,  ..., 117., 394., 136.]]),\n",
       " 911: tensor([[  0., 258., 146.,  ...,  62., 195.,  87.]]),\n",
       " 150: tensor([[  0., 127.,  48.,  ...,  30.,  64.,  25.]]),\n",
       " 125: tensor([[  0., 555., 269.,  ...,  97., 364., 147.]]),\n",
       " 742: tensor([[  0., 123.,  44.,  ...,  24.,  52.,  21.]]),\n",
       " 51: tensor([[ 0., 99., 51.,  ..., 16., 46., 19.]]),\n",
       " 354: tensor([[  0., 471., 223.,  ..., 127., 453., 167.]]),\n",
       " 873: tensor([[ 0., 83., 38.,  ..., 19., 40.,  6.]]),\n",
       " 143: tensor([[ 0., 78., 32.,  ..., 20., 38.,  8.]]),\n",
       " 471: tensor([[ 0., 67., 50.,  ..., 21., 62., 28.]]),\n",
       " 400: tensor([[ 0., 81., 37.,  ..., 21., 43.,  9.]]),\n",
       " 590: tensor([[  0., 151.,  44.,  ...,  33.,  93.,  23.]]),\n",
       " 321: tensor([[  0., 243., 111.,  ...,  81., 246.,  91.]]),\n",
       " 20: tensor([[  0., 221., 112.,  ...,  37.,  96.,  39.]]),\n",
       " 763: tensor([[  0., 388., 174.,  ...,  79., 258., 107.]]),\n",
       " 746: tensor([[  0., 463., 212.,  ...,  44., 134.,  64.]]),\n",
       " 762: tensor([[ 0., 68., 30.,  ..., 15., 39., 11.]]),\n",
       " 408: tensor([[  0., 118.,  50.,  ...,  26.,  51.,  15.]]),\n",
       " 538: tensor([[  0., 387., 192.,  ...,  60., 162.,  72.]]),\n",
       " 585: tensor([[  0.,  67.,  27.,  ...,  29., 135.,  53.]]),\n",
       " 39: tensor([[ 0., 75., 32.,  ..., 19., 42., 10.]]),\n",
       " 176: tensor([[  0., 249.,  87.,  ...,  54., 122.,  39.]]),\n",
       " 246: tensor([[  0., 887., 384.,  ...,  78., 390., 165.]]),\n",
       " 782: tensor([[  0., 638., 259.,  ..., 232., 331.,  55.]]),\n",
       " 630: tensor([[  0., 520., 204.,  ...,  73., 215.,  69.]]),\n",
       " 711: tensor([[  0., 696., 320.,  ..., 109., 430., 176.]]),\n",
       " 809: tensor([[  0., 103.,  39.,  ...,  19.,  40.,   8.]]),\n",
       " 155: tensor([[  0., 111.,  46.,  ...,  20.,  43.,   8.]]),\n",
       " 288: tensor([[  0., 271., 130.,  ...,  56., 146.,  58.]]),\n",
       " 568: tensor([[  0., 100.,  46.,  ...,  46., 159.,  57.]]),\n",
       " 18: tensor([[  0., 573., 279.,  ..., 152., 550., 218.]]),\n",
       " 128: tensor([[  0., 594., 325.,  ..., 106., 366., 137.]]),\n",
       " 323: tensor([[  0., 438., 164.,  ...,  76., 176.,  54.]]),\n",
       " 256: tensor([[   0., 1115.,  495.,  ...,   91.,  408.,  152.]]),\n",
       " 751: tensor([[  0., 700., 331.,  ...,  96., 333., 136.]]),\n",
       " 909: tensor([[ 0., 60., 24.,  ..., 17., 51., 13.]]),\n",
       " 191: tensor([[  0., 102.,  41.,  ...,  23.,  52.,   9.]]),\n",
       " 116: tensor([[  0., 506., 189.,  ..., 107., 267.,  66.]]),\n",
       " 581: tensor([[ 0., 85., 33.,  ..., 22., 53., 15.]]),\n",
       " 181: tensor([[1.0000e+00, 1.1300e+03, 4.5400e+02,  ..., 1.9900e+02, 6.6800e+02,\n",
       "          1.2100e+02]]),\n",
       " 302: tensor([[ 0., 83., 40.,  ..., 19., 41.,  5.]]),\n",
       " 716: tensor([[  0., 834., 440.,  ..., 154., 538., 205.]]),\n",
       " 718: tensor([[  0., 221.,  77.,  ...,  28.,  77.,  23.]]),\n",
       " 683: tensor([[  0., 285., 118.,  ...,  61., 141.,  33.]]),\n",
       " 753: tensor([[  0., 240., 121.,  ...,  48., 112.,  46.]]),\n",
       " 652: tensor([[  0., 127.,  45.,  ...,  20.,  44.,   9.]]),\n",
       " 518: tensor([[  0., 333., 113.,  ...,  48., 145.,  41.]]),\n",
       " 928: tensor([[ 0., 93., 38.,  ..., 28., 64., 19.]]),\n",
       " 46: tensor([[  0., 139.,  63.,  ...,  22.,  50.,  14.]]),\n",
       " 856: tensor([[  0., 129.,  51.,  ...,  26.,  51.,   8.]]),\n",
       " 158: tensor([[  0., 896., 387.,  ...,  98., 346., 145.]]),\n",
       " 937: tensor([[  0., 100.,  42.,  ...,  32.,  79.,  20.]]),\n",
       " 172: tensor([[ 0., 36., 12.,  ..., 11., 53., 18.]]),\n",
       " 17: tensor([[  0., 110.,  39.,  ...,  26.,  56.,  18.]]),\n",
       " 512: tensor([[ 0., 96., 45.,  ..., 17., 40., 19.]]),\n",
       " 49: tensor([[  0., 675., 282.,  ..., 106., 418., 175.]]),\n",
       " 915: tensor([[  0., 105.,  47.,  ...,  23.,  50.,   8.]]),\n",
       " 793: tensor([[  0., 288., 107.,  ...,  49., 108.,  36.]]),\n",
       " 516: tensor([[ 0., 89., 47.,  ..., 17., 42., 18.]]),\n",
       " 887: tensor([[  0., 615., 310.,  ...,  90., 334., 123.]]),\n",
       " 481: tensor([[  0., 146.,  94.,  ...,  40., 112.,  45.]]),\n",
       " 284: tensor([[  0., 170.,  68.,  ...,  38.,  86.,  15.]]),\n",
       " 617: tensor([[  0., 236.,  96.,  ...,  39., 212.,  88.]]),\n",
       " 226: tensor([[  0., 184.,  67.,  ...,  38.,  99.,  39.]]),\n",
       " 27: tensor([[  0., 104.,  41.,  ...,  19.,  50.,  14.]]),\n",
       " 738: tensor([[  0., 680., 331.,  ...,  93., 296., 141.]]),\n",
       " 438: tensor([[  0., 146.,  65.,  ...,  22.,  66.,  16.]]),\n",
       " 896: tensor([[   0., 1553.,  691.,  ...,  173.,  711.,  296.]]),\n",
       " 365: tensor([[  0., 140.,  54.,  ...,  45., 109.,  25.]]),\n",
       " 230: tensor([[  0., 500., 260.,  ...,  85., 266., 111.]]),\n",
       " 38: tensor([[  0., 376., 183.,  ...,  62., 239.,  98.]]),\n",
       " 910: tensor([[  0., 370., 144.,  ...,  55., 120.,  44.]]),\n",
       " 375: tensor([[  0., 149.,  49.,  ...,  10.,  56.,  28.]]),\n",
       " 470: tensor([[  0., 232.,  83.,  ...,  45., 109.,  32.]]),\n",
       " 912: tensor([[  0., 105.,  55.,  ...,  43., 106.,  44.]]),\n",
       " 802: tensor([[  0., 259.,  89.,  ...,  39., 148.,  55.]]),\n",
       " 685: tensor([[ 0., 70., 22.,  ..., 16., 34.,  6.]]),\n",
       " 195: tensor([[  0., 357., 172.,  ...,  56., 186.,  76.]]),\n",
       " 805: tensor([[  0., 949., 430.,  ..., 141., 537., 235.]]),\n",
       " 815: tensor([[  0., 617., 308.,  ..., 106., 364., 153.]]),\n",
       " 504: tensor([[  0., 851., 350.,  ..., 114., 500., 197.]]),\n",
       " 914: tensor([[ 0., 61., 33.,  ...,  8., 40., 20.]]),\n",
       " 899: tensor([[  0., 639., 284.,  ...,  90., 270., 118.]]),\n",
       " 580: tensor([[  0., 224.,  85.,  ...,  41.,  94.,  32.]]),\n",
       " 913: tensor([[  0., 476., 234.,  ...,  98., 264., 115.]]),\n",
       " 163: tensor([[  0., 100.,  40.,  ...,  22.,  46.,  17.]]),\n",
       " 923: tensor([[  0., 362., 140.,  ...,  64., 147.,  38.]]),\n",
       " 153: tensor([[  0., 157.,  73.,  ...,  21.,  46.,  19.]]),\n",
       " 842: tensor([[ 0., 86., 29.,  ..., 22., 45.,  9.]]),\n",
       " 671: tensor([[  0., 805., 325.,  ...,  67., 244., 111.]]),\n",
       " 712: tensor([[  0., 473., 290.,  ...,  58., 324., 153.]]),\n",
       " 112: tensor([[  0., 224.,  82.,  ...,  40.,  84.,  14.]]),\n",
       " 227: tensor([[  0., 224.,  80.,  ...,  48., 112.,  32.]]),\n",
       " 821: tensor([[  0., 251., 117.,  ...,  45., 123.,  45.]]),\n",
       " 104: tensor([[  0., 483., 171.,  ...,  96., 220.,  54.]]),\n",
       " 422: tensor([[  1., 380., 120.,  ...,  61., 194.,  69.]]),\n",
       " 420: tensor([[ 0., 59., 19.,  ..., 25., 70., 21.]]),\n",
       " 707: tensor([[  0., 447., 196.,  ..., 115., 431., 146.]]),\n",
       " 587: tensor([[  0., 374., 148.,  ...,  83., 181.,  25.]]),\n",
       " 736: tensor([[  0., 107.,  40.,  ...,  24.,  42.,  10.]]),\n",
       " 120: tensor([[  0., 146.,  56.,  ...,  24.,  52.,  20.]]),\n",
       " 54: tensor([[  0., 358., 137.,  ...,  56., 130.,  39.]]),\n",
       " 152: tensor([[  0., 340., 172.,  ...,  52., 211.,  79.]]),\n",
       " 404: tensor([[  0., 190.,  75.,  ...,  39.,  84.,  13.]]),\n",
       " 890: tensor([[  0., 347., 195.,  ...,  73., 237., 104.]]),\n",
       " 208: tensor([[ 0., 82., 36.,  ..., 24., 66., 32.]]),\n",
       " 796: tensor([[   0., 1477.,  688.,  ...,  177.,  714.,  279.]]),\n",
       " 719: tensor([[  0., 282., 124.,  ...,  50., 134.,  50.]]),\n",
       " 9: tensor([[ 0., 80., 37.,  ..., 19., 44., 14.]]),\n",
       " 761: tensor([[  0., 282., 110.,  ...,  49., 128.,  38.]]),\n",
       " 37: tensor([[  0., 462., 195.,  ...,  36., 114.,  52.]]),\n",
       " 453: tensor([[  0., 678., 277.,  ...,  85., 312., 134.]]),\n",
       " 254: tensor([[  0., 448., 286.,  ...,  75., 317., 127.]]),\n",
       " 304: tensor([[  0., 111.,  47.,  ...,  24.,  49.,  13.]]),\n",
       " 855: tensor([[ 0., 21., 11.,  ..., 14., 46., 21.]]),\n",
       " 572: tensor([[ 0., 61., 25.,  ..., 13., 38., 10.]]),\n",
       " 624: tensor([[  0., 600., 215.,  ..., 118., 276.,  66.]]),\n",
       " 669: tensor([[  0., 377., 168.,  ...,  85., 200.,  77.]]),\n",
       " 679: tensor([[  0., 192., 109.,  ...,  48., 124.,  56.]]),\n",
       " 263: tensor([[  0., 423., 221.,  ...,  96., 245.,  93.]]),\n",
       " 611: tensor([[  0., 141.,  53.,  ...,  32.,  75.,  14.]]),\n",
       " 298: tensor([[  0., 406., 209.,  ...,  96., 254., 103.]]),\n",
       " 415: tensor([[ 0., 87., 36.,  ..., 22., 46., 16.]]),\n",
       " 151: tensor([[  0., 866., 404.,  ..., 160., 608., 232.]]),\n",
       " 744: tensor([[ 0., 97., 37.,  ..., 22., 53., 18.]]),\n",
       " 641: tensor([[ 0., 81., 39.,  ..., 26., 80., 27.]]),\n",
       " 837: tensor([[  0., 161.,  66.,  ...,  31.,  91.,  21.]]),\n",
       " 551: tensor([[   0., 1602.,  673.,  ...,  164.,  657.,  262.]]),\n",
       " 747: tensor([[  0., 719., 351.,  ..., 167., 575., 214.]]),\n",
       " 489: tensor([[  0., 399., 162.,  ...,  91., 188.,  26.]]),\n",
       " 148: tensor([[  0., 177.,  96.,  ...,  46., 129.,  54.]]),\n",
       " 147: tensor([[ 0., 62., 31.,  ..., 16., 40.,  7.]]),\n",
       " 680: tensor([[  0., 151.,  64.,  ...,  36.,  76.,  29.]]),\n",
       " 895: tensor([[ 0., 88., 50.,  ..., 19., 39., 13.]]),\n",
       " 697: tensor([[  0., 479., 171.,  ...,  87., 206.,  46.]]),\n",
       " 660: tensor([[  0., 945., 458.,  ..., 120., 447., 186.]]),\n",
       " 349: tensor([[  0., 153.,  55.,  ...,  23.,  80.,  18.]]),\n",
       " 464: tensor([[  0., 226.,  83.,  ...,  48., 104.,  29.]]),\n",
       " 326: tensor([[  0., 639., 312.,  ...,  96., 373., 160.]]),\n",
       " 187: tensor([[  0., 149.,  71.,  ...,  38., 112.,  53.]]),\n",
       " 124: tensor([[  0., 154.,  64.,  ...,  18.,  48.,  21.]]),\n",
       " 804: tensor([[   0., 1268.,  578.,  ...,  165.,  664.,  276.]]),\n",
       " 609: tensor([[ 0., 58., 28.,  ..., 24., 52., 12.]]),\n",
       " 144: tensor([[  0., 761., 293.,  ..., 138., 406., 145.]]),\n",
       " 903: tensor([[  0., 565., 214.,  ...,  93., 268., 109.]]),\n",
       " 198: tensor([[  0., 735., 337.,  ..., 116., 359., 159.]]),\n",
       " 610: tensor([[  0., 268., 137.,  ...,  55., 149.,  59.]]),\n",
       " 280: tensor([[   0., 1142.,  500.,  ...,  115.,  514.,  204.]]),\n",
       " 294: tensor([[  0., 702., 266.,  ..., 124., 298.,  81.]]),\n",
       " 401: tensor([[  0., 526., 239.,  ..., 111., 303., 110.]]),\n",
       " 123: tensor([[  0., 120.,  36.,  ...,  41., 107.,  42.]]),\n",
       " 184: tensor([[  0., 777., 340.,  ..., 128., 493., 187.]]),\n",
       " 203: tensor([[  0., 201.,  78.,  ...,  38.,  86.,  23.]]),\n",
       " 439: tensor([[  0., 122.,  38.,  ...,  28.,  58.,  19.]]),\n",
       " 247: tensor([[  0., 108.,  49.,  ...,  25.,  52.,  20.]]),\n",
       " 416: tensor([[   0., 1832.,  794.,  ...,  249.,  979.,  338.]]),\n",
       " 722: tensor([[  0., 211.,  78.,  ...,  36.,  84.,  25.]]),\n",
       " 840: tensor([[  0., 495., 246.,  ..., 127., 387., 161.]]),\n",
       " 836: tensor([[  0., 163.,  71.,  ...,  39.,  99.,  36.]]),\n",
       " 130: tensor([[1.0000e+00, 1.5600e+03, 6.5400e+02,  ..., 1.8400e+02, 6.9800e+02,\n",
       "          2.5200e+02]]),\n",
       " 638: tensor([[  0., 403., 210.,  ...,  49., 134.,  66.]]),\n",
       " 403: tensor([[  0., 268., 110.,  ...,  39., 100.,  37.]]),\n",
       " 740: tensor([[ 0., 79., 32.,  ..., 19., 37.,  9.]]),\n",
       " 844: tensor([[  0., 368., 179.,  ...,  56., 162.,  75.]]),\n",
       " 860: tensor([[  0., 255., 121.,  ...,  46., 138.,  38.]]),\n",
       " 220: tensor([[ 0., 66., 29.,  ..., 17., 41., 12.]]),\n",
       " 642: tensor([[   0., 1113.,  557.,  ...,  115.,  635.,  265.]]),\n",
       " 862: tensor([[  0., 649., 321.,  ..., 107., 328., 134.]]),\n",
       " 709: tensor([[  0., 864., 334.,  ...,  63., 276., 126.]]),\n",
       " 760: tensor([[  0., 168.,  68.,  ...,  27.,  82.,  29.]]),\n",
       " 865: tensor([[  0., 247., 109.,  ...,  45., 131.,  51.]]),\n",
       " 625: tensor([[  0., 375., 184.,  ...,  71., 212.,  92.]]),\n",
       " 791: tensor([[  0., 126.,  58.,  ...,  22.,  49.,  10.]]),\n",
       " 717: tensor([[  0., 455., 177.,  ...,  74., 183.,  48.]]),\n",
       " 387: tensor([[   0., 1088.,  463.,  ...,  156.,  597.,  261.]]),\n",
       " 916: tensor([[   0., 1334.,  560.,  ...,  159.,  626.,  267.]]),\n",
       " 510: tensor([[  0., 127.,  51.,  ...,  29.,  59.,  10.]]),\n",
       " 268: tensor([[1.0000e+00, 1.4000e+03, 5.7500e+02,  ..., 1.4600e+02, 6.5600e+02,\n",
       "          2.8700e+02]]),\n",
       " 810: tensor([[  0., 106.,  40.,  ...,  26.,  52.,   5.]]),\n",
       " 442: tensor([[  0., 748., 306.,  ...,  74., 286., 124.]]),\n",
       " 497: tensor([[   0., 1223.,  559.,  ...,  119.,  558.,  279.]]),\n",
       " 113: tensor([[  0., 218.,  72.,  ...,  42.,  97.,  24.]]),\n",
       " 692: tensor([[  0., 155.,  46.,  ...,  30.,  80.,  24.]]),\n",
       " 233: tensor([[  0., 302., 136.,  ...,  82., 219.,  91.]]),\n",
       " 820: tensor([[ 0., 78., 33.,  ..., 21., 42., 13.]]),\n",
       " 608: tensor([[  0., 488., 210.,  ...,  96., 317., 114.]]),\n",
       " 672: tensor([[ 0., 95., 40.,  ..., 20., 56., 12.]]),\n",
       " 374: tensor([[   0., 1261.,  553.,  ...,  155.,  537.,  184.]]),\n",
       " 925: tensor([[  0., 143.,  41.,  ...,  17.,  64.,  23.]]),\n",
       " 795: tensor([[  0., 626., 287.,  ...,  72., 318., 159.]]),\n",
       " 234: tensor([[   0., 1226.,  590.,  ...,  217.,  901.,  316.]]),\n",
       " 741: tensor([[  0., 450., 228.,  ...,  64., 208.,  90.]]),\n",
       " 677: tensor([[  0., 175.,  74.,  ...,  37.,  97.,  28.]]),\n",
       " 318: tensor([[  0., 568., 265.,  ...,  97., 348., 127.]]),\n",
       " 649: tensor([[  0., 150.,  58.,  ...,  23.,  48.,  16.]]),\n",
       " 411: tensor([[  0., 317., 174.,  ...,  37., 116.,  54.]]),\n",
       " 846: tensor([[   0., 1274.,  583.,  ...,  146.,  810.,  355.]]),\n",
       " 710: tensor([[  0., 349., 166.,  ...,  70., 169.,  65.]]),\n",
       " 359: tensor([[  0., 172.,  73.,  ...,  26.,  54.,  22.]]),\n",
       " 259: tensor([[  0., 191.,  99.,  ...,  37.,  92.,  38.]]),\n",
       " 628: tensor([[ 0., 89., 37.,  ..., 26., 51., 10.]]),\n",
       " 633: tensor([[  0., 304., 134.,  ...,  39., 116.,  48.]]),\n",
       " 283: tensor([[  0., 161.,  74.,  ...,  32., 104.,  45.]]),\n",
       " 377: tensor([[  0., 121.,  42.,  ...,  31.,  64.,  25.]]),\n",
       " 385: tensor([[  0., 722., 312.,  ..., 144., 536., 205.]]),\n",
       " 225: tensor([[ 0., 75., 41.,  ..., 18., 53., 20.]]),\n",
       " 898: tensor([[  0., 123.,  51.,  ...,  28.,  56.,  15.]]),\n",
       " 768: tensor([[  0., 277., 119.,  ...,  54., 128.,  40.]]),\n",
       " 473: tensor([[  0., 106.,  25.,  ...,  28.,  68.,  18.]]),\n",
       " 348: tensor([[  0., 251., 121.,  ...,  37., 116.,  31.]]),\n",
       " 506: tensor([[  0., 978., 447.,  ..., 121., 481., 199.]]),\n",
       " 640: tensor([[  0., 505., 196.,  ...,  60., 218.,  88.]]),\n",
       " 931: tensor([[  0., 264., 100.,  ...,  49., 118.,  24.]]),\n",
       " 874: tensor([[  0., 107.,  40.,  ...,  27.,  67.,  18.]]),\n",
       " 852: tensor([[  0., 267., 114.,  ...,  38.,  97.,  38.]]),\n",
       " 620: tensor([[  0., 321., 178.,  ...,  58., 218.,  87.]]),\n",
       " 917: tensor([[  0., 127.,  44.,  ...,  28.,  70.,  19.]]),\n",
       " 646: tensor([[  0., 148.,  57.,  ...,  32.,  68.,  15.]]),\n",
       " 798: tensor([[  0., 722., 399.,  ...,  98., 472., 174.]]),\n",
       " 557: tensor([[  0., 209.,  83.,  ...,  39., 104.,  27.]]),\n",
       " 902: tensor([[  0., 216.,  89.,  ...,  43.,  96.,  30.]]),\n",
       " 578: tensor([[  0., 117.,  45.,  ...,  21.,  47.,  16.]]),\n",
       " 86: tensor([[ 0., 79., 24.,  ..., 19., 43.,  8.]]),\n",
       " 243: tensor([[  0., 198.,  88.,  ...,  50., 154.,  58.]]),\n",
       " 699: tensor([[  0., 490., 206.,  ..., 104., 279.,  72.]]),\n",
       " 160: tensor([[  0., 445., 152.,  ...,  81., 237.,  91.]]),\n",
       " 525: tensor([[  0., 263.,  97.,  ...,  50., 108.,  31.]]),\n",
       " 36: tensor([[ 0., 56., 21.,  ..., 15., 33.,  5.]]),\n",
       " 541: tensor([[  0., 440., 240.,  ...,  69., 265., 119.]]),\n",
       " 171: tensor([[ 0., 90., 47.,  ..., 22., 52., 10.]]),\n",
       " 655: tensor([[   0., 1885.,  772.,  ...,  283., 1168.,  416.]]),\n",
       " 838: tensor([[  0., 349., 170.,  ...,  72., 186.,  72.]]),\n",
       " 808: tensor([[  0., 116.,  53.,  ...,  21.,  46.,  10.]]),\n",
       " 799: tensor([[ 0., 74., 33.,  ..., 18., 47., 15.]]),\n",
       " 889: tensor([[   0., 1205.,  493.,  ...,  184.,  648.,  265.]]),\n",
       " 734: tensor([[  0., 214., 127.,  ...,  48., 128.,  49.]]),\n",
       " 96: tensor([[  0., 213.,  95.,  ...,  38., 111.,  51.]]),\n",
       " 564: tensor([[  0., 211., 102.,  ...,  30.,  67.,  19.]]),\n",
       " 241: tensor([[ 0., 99., 44.,  ..., 23., 44., 11.]]),\n",
       " 743: tensor([[  0., 117.,  48.,  ...,  32.,  70.,  19.]]),\n",
       " 575: tensor([[ 0., 97., 49.,  ..., 22., 52., 19.]]),\n",
       " 691: tensor([[ 0., 98., 46.,  ..., 18., 64., 23.]]),\n",
       " 461: tensor([[ 0., 86., 42.,  ..., 18., 46., 13.]]),\n",
       " 554: tensor([[  0., 553., 267.,  ...,  76., 228.,  91.]]),\n",
       " 827: tensor([[  0., 116.,  54.,  ...,  28.,  55.,  13.]]),\n",
       " 207: tensor([[  0., 957., 359.,  ..., 143., 448., 169.]]),\n",
       " 393: tensor([[   0., 1759.,  795.,  ...,  195.,  884.,  333.]]),\n",
       " 398: tensor([[  0., 507., 264.,  ..., 105., 342., 137.]]),\n",
       " 767: tensor([[ 0., 96., 44.,  ..., 26., 71., 26.]]),\n",
       " 800: tensor([[  0., 143.,  64.,  ...,  25.,  56.,  19.]]),\n",
       " 218: tensor([[  0., 177.,  74.,  ...,  37., 106.,  51.]]),\n",
       " 5: tensor([[  1., 581., 273.,  ...,  69., 350., 160.]]),\n",
       " 769: tensor([[  0., 123.,  55.,  ...,  20.,  56.,  17.]]),\n",
       " 643: tensor([[  0., 802., 329.,  ..., 121., 411., 181.]]),\n",
       " 845: tensor([[ 0., 67., 32.,  ..., 18., 51., 15.]]),\n",
       " 613: tensor([[ 0., 89., 38.,  ..., 20., 55., 21.]]),\n",
       " 739: tensor([[  0., 150.,  70.,  ...,  31.,  66.,  28.]]),\n",
       " 547: tensor([[ 0., 75., 33.,  ..., 19., 46.,  9.]]),\n",
       " 726: tensor([[  0., 110.,  49.,  ...,  18.,  46.,  10.]]),\n",
       " 684: tensor([[  0., 326., 170.,  ...,  53., 172.,  76.]]),\n",
       " 99: tensor([[  0., 675., 300.,  ...,  98., 272.,  85.]]),\n",
       " 728: tensor([[  0., 109.,  29.,  ...,  21.,  50.,  13.]]),\n",
       " 574: tensor([[  0., 157.,  66.,  ...,  33.,  87.,  16.]]),\n",
       " 407: tensor([[  0., 814., 377.,  ..., 126., 452., 194.]]),\n",
       " 503: tensor([[  0., 416., 180.,  ...,  98., 318., 122.]]),\n",
       " 48: tensor([[  0., 196., 112.,  ...,  49., 128.,  50.]]),\n",
       " 33: tensor([[  0., 112.,  48.,  ...,  24.,  47.,   9.]]),\n",
       " 780: tensor([[  0., 185., 112.,  ...,  48., 110.,  46.]]),\n",
       " 201: tensor([[   0., 1356.,  548.,  ...,  184.,  757.,  304.]]),\n",
       " 135: tensor([[  0., 326., 142.,  ...,  27., 110.,  52.]]),\n",
       " 28: tensor([[  0., 315., 160.,  ...,  43., 158.,  69.]]),\n",
       " 829: tensor([[  0., 153.,  50.,  ...,  42., 128.,  46.]]),\n",
       " 231: tensor([[ 0., 98., 45.,  ..., 16., 42., 15.]]),\n",
       " 490: tensor([[  0., 207.,  85.,  ...,  45., 107.,  33.]]),\n",
       " 773: tensor([[  0., 571., 246.,  ...,  86., 318., 161.]]),\n",
       " 379: tensor([[  0., 712., 321.,  ..., 111., 389., 173.]]),\n",
       " 289: tensor([[  0., 151.,  61.,  ...,  20.,  54.,  22.]]),\n",
       " 419: tensor([[  0., 119.,  65.,  ...,  26.,  62.,  21.]]),\n",
       " 675: tensor([[ 0., 63., 27.,  ..., 23., 57., 15.]]),\n",
       " 392: tensor([[  0., 318., 132.,  ...,  88., 219.,  61.]]),\n",
       " 593: tensor([[  0., 621., 282.,  ...,  90., 312., 113.]]),\n",
       " 499: tensor([[  0., 364., 179.,  ...,  72., 200.,  77.]]),\n",
       " 139: tensor([[ 0., 62., 22.,  ..., 19., 44., 14.]]),\n",
       " 63: tensor([[  0., 281., 108.,  ...,  69., 181.,  45.]]),\n",
       " 472: tensor([[   0., 1149.,  548.,  ...,  122.,  526.,  232.]]),\n",
       " 648: tensor([[   0., 1170.,  548.,  ...,  147.,  592.,  241.]]),\n",
       " 129: tensor([[  0., 116.,  47.,  ...,  23.,  52.,  12.]]),\n",
       " 920: tensor([[  0., 112.,  50.,  ...,  24.,  50.,  10.]]),\n",
       " 521: tensor([[  0., 719., 322.,  ...,  95., 300., 134.]]),\n",
       " 65: tensor([[  0., 294., 123.,  ...,  54., 160.,  73.]]),\n",
       " 813: tensor([[  0., 101.,  57.,  ...,  25.,  54.,   6.]]),\n",
       " 15: tensor([[  0., 428., 168.,  ...,  77., 199.,  46.]]),\n",
       " 157: tensor([[  0., 216.,  88.,  ...,  44., 100.,  31.]]),\n",
       " 41: tensor([[  0., 179.,  98.,  ...,  44., 104.,  48.]]),\n",
       " 493: tensor([[  0., 640., 277.,  ..., 108., 267., 105.]]),\n",
       " 380: tensor([[  0., 486., 256.,  ..., 100., 322., 143.]]),\n",
       " 188: tensor([[  0., 536., 238.,  ...,  76., 224.,  93.]]),\n",
       " 764: tensor([[  0., 470., 216.,  ...,  73., 218.,  78.]]),\n",
       " 217: tensor([[  0., 589., 244.,  ...,  37., 152.,  72.]]),\n",
       " 792: tensor([[  0., 219.,  74.,  ...,  35.,  93.,  29.]]),\n",
       " 53: tensor([[  0., 158.,  70.,  ...,  24.,  56.,  23.]]),\n",
       " 338: tensor([[  0., 137.,  72.,  ...,  56., 149.,  54.]]),\n",
       " 749: tensor([[   0., 1278.,  587.,  ...,  161.,  610.,  237.]]),\n",
       " 720: tensor([[ 0., 78., 36.,  ..., 20., 55., 12.]]),\n",
       " 725: tensor([[  0., 109.,  44.,  ...,  23.,  50.,  14.]]),\n",
       " 296: tensor([[  0., 485., 203.,  ..., 117., 287.,  96.]]),\n",
       " 702: tensor([[  0., 130.,  80.,  ...,  24.,  57.,  18.]]),\n",
       " 252: tensor([[ 0., 83., 25.,  ..., 16., 42., 13.]]),\n",
       " 727: tensor([[   0., 1526.,  647.,  ...,  154.,  640.,  265.]]),\n",
       " 462: tensor([[  0., 137.,  61.,  ...,  30.,  65.,  12.]]),\n",
       " 456: tensor([[  0., 729., 298.,  ..., 114., 424., 176.]]),\n",
       " 940: tensor([[  0., 377., 178.,  ...,  76., 213.,  86.]]),\n",
       " 816: tensor([[  0., 114.,  47.,  ...,  24.,  48.,   8.]]),\n",
       " 270: tensor([[  0., 521., 192.,  ...,  69., 272., 114.]]),\n",
       " 202: tensor([[ 0., 46., 28.,  ..., 18., 40., 15.]]),\n",
       " 162: tensor([[  0., 222., 109.,  ...,  33.,  84.,  36.]]),\n",
       " 556: tensor([[  0., 103.,  41.,  ...,  36.,  88.,  32.]]),\n",
       " 730: tensor([[  0., 210.,  79.,  ...,  36.,  76.,  22.]]),\n",
       " 402: tensor([[  0., 231.,  96.,  ...,  48., 136.,  52.]]),\n",
       " 703: tensor([[  0., 232.,  94.,  ...,  40.,  94.,  31.]]),\n",
       " 477: tensor([[  0., 110.,  53.,  ...,  18.,  70.,  25.]]),\n",
       " 822: tensor([[ 0., 56., 25.,  ..., 15., 49., 22.]]),\n",
       " 772: tensor([[  0., 166.,  71.,  ...,  33.,  65.,  11.]]),\n",
       " 106: tensor([[  0., 197., 102.,  ...,  44., 127.,  52.]]),\n",
       " 566: tensor([[  0., 579., 255.,  ...,  96., 298., 134.]]),\n",
       " 390: tensor([[  0., 108.,  42.,  ...,  26.,  60.,  13.]]),\n",
       " 2: tensor([[  0., 224.,  99.,  ...,  48., 117.,  28.]]),\n",
       " 882: tensor([[  0., 540., 299.,  ...,  88., 274., 117.]]),\n",
       " 562: tensor([[  0., 306., 151.,  ...,  50., 144.,  64.]]),\n",
       " 81: tensor([[  0., 229.,  94.,  ...,  43., 116.,  37.]]),\n",
       " 206: tensor([[  0., 173.,  73.,  ...,  36.,  97.,  22.]]),\n",
       " 35: tensor([[  0., 120.,  47.,  ...,  25.,  50.,   7.]]),\n",
       " 776: tensor([[  0., 274., 114.,  ...,  47., 207.,  89.]]),\n",
       " 509: tensor([[  0., 143.,  74.,  ...,  27.,  65.,  14.]]),\n",
       " 428: tensor([[  0., 201.,  78.,  ...,  46., 103.,  15.]]),\n",
       " 455: tensor([[  0., 794., 341.,  ..., 112., 373., 148.]]),\n",
       " 367: tensor([[  0., 220.,  61.,  ...,  27., 116.,  47.]]),\n",
       " 92: tensor([[   0., 1604.,  685.,  ...,  176.,  772.,  312.]]),\n",
       " 340: tensor([[  0., 130.,  92.,  ...,  31.,  88.,  41.]]),\n",
       " 565: tensor([[ 0., 39.,  6.,  ..., 16., 68., 31.]]),\n",
       " 346: tensor([[   0., 1063.,  441.,  ...,   98.,  386.,  170.]]),\n",
       " 777: tensor([[  0., 128.,  57.,  ...,  28.,  72.,  31.]]),\n",
       " 242: tensor([[ 0., 53., 25.,  ..., 13., 35.,  8.]]),\n",
       " 14: tensor([[  0., 305., 134.,  ...,  72., 195.,  80.]]),\n",
       " 316: tensor([[  0., 213., 108.,  ...,  56., 148.,  56.]]),\n",
       " 601: tensor([[  0., 492., 237.,  ...,  90., 294., 123.]]),\n",
       " 388: tensor([[  0., 235.,  93.,  ...,  39., 104.,  33.]]),\n",
       " 274: tensor([[  0., 298., 125.,  ...,  55., 142.,  42.]]),\n",
       " 167: tensor([[  0., 156.,  77.,  ...,  30., 130.,  55.]]),\n",
       " 841: tensor([[  0., 139.,  46.,  ...,  26.,  57.,  11.]]),\n",
       " 936: tensor([[  0., 445., 172.,  ..., 106., 257.,  66.]]),\n",
       " 647: tensor([[  0., 280., 126.,  ...,  48., 116.,  48.]]),\n",
       " 819: tensor([[ 0., 93., 39.,  ..., 22., 53., 13.]]),\n",
       " 485: tensor([[ 0., 88., 41.,  ..., 20., 51., 11.]]),\n",
       " 924: tensor([[  0., 319., 138.,  ...,  55., 162.,  68.]]),\n",
       " 505: tensor([[  0., 487., 262.,  ...,  79., 223.,  92.]]),\n",
       " 832: tensor([[  0., 117.,  49.,  ...,  24.,  49.,  13.]]),\n",
       " 244: tensor([[  0., 856., 359.,  ..., 112., 473., 186.]]),\n",
       " 567: tensor([[  0., 354., 157.,  ...,  99., 304., 108.]]),\n",
       " 876: tensor([[ 0., 48., 24.,  ..., 16., 42., 17.]]),\n",
       " 563: tensor([[  0., 145.,  84.,  ...,  18.,  60.,  20.]]),\n",
       " 687: tensor([[ 0., 82., 36.,  ..., 19., 42., 11.]]),\n",
       " 190: tensor([[  0., 330., 132.,  ...,  51., 116.,  33.]]),\n",
       " 783: tensor([[  0., 118.,  40.,  ...,  29.,  58.,  11.]]),\n",
       " 700: tensor([[  0., 111.,  56.,  ...,  19.,  42.,  21.]]),\n",
       " 476: tensor([[  0., 314., 136.,  ...,  38., 166.,  80.]]),\n",
       " 682: tensor([[   0., 1806.,  747.,  ...,  193.,  789.,  319.]]),\n",
       " 511: tensor([[ 0., 96., 41.,  ..., 24., 46., 10.]]),\n",
       " 59: tensor([[   0., 1169.,  535.,  ...,  189.,  760.,  306.]]),\n",
       " 475: tensor([[ 0., 72., 34.,  ..., 17., 38., 12.]]),\n",
       " 561: tensor([[   0., 1157.,  506.,  ...,  168.,  700.,  290.]]),\n",
       " 857: tensor([[ 0., 56., 33.,  ..., 17., 44.,  9.]]),\n",
       " 748: tensor([[  0., 402., 186.,  ...,  78., 216.,  94.]]),\n",
       " 253: tensor([[  0., 418., 179.,  ...,  70., 194.,  78.]]),\n",
       " 22: tensor([[  0., 621., 310.,  ...,  70., 253., 114.]]),\n",
       " 314: tensor([[  0., 877., 363.,  ...,  99., 479., 177.]]),\n",
       " 460: tensor([[  0., 235.,  79.,  ...,  52., 126.,  29.]]),\n",
       " 161: tensor([[  0., 202., 112.,  ...,  45., 114.,  44.]]),\n",
       " 277: tensor([[  0., 245., 101.,  ...,  46., 106.,  32.]]),\n",
       " 192: tensor([[  0., 137.,  51.,  ...,  26.,  68.,  19.]]),\n",
       " 386: tensor([[  0., 170.,  67.,  ...,  21.,  46.,  19.]]),\n",
       " 806: tensor([[  0., 572., 253.,  ...,  91., 278., 118.]]),\n",
       " 414: tensor([[  0., 101.,  45.,  ...,  24.,  49.,  17.]]),\n",
       " 492: tensor([[  0., 132.,  57.,  ...,  40., 110.,  45.]]),\n",
       " 344: tensor([[  0., 696., 318.,  ..., 128., 375., 132.]]),\n",
       " 770: tensor([[  0., 252.,  95.,  ...,  57., 128.,  40.]]),\n",
       " 309: tensor([[ 0., 61., 20.,  ..., 17., 36.,  7.]]),\n",
       " 662: tensor([[ 0., 52., 22.,  ..., 17., 39., 12.]]),\n",
       " 131: tensor([[ 0., 81., 29.,  ..., 25., 58., 15.]]),\n",
       " 664: tensor([[  0., 600., 281.,  ..., 101., 331., 141.]]),\n",
       " 872: tensor([[  0., 316., 136.,  ...,  46., 139.,  35.]]),\n",
       " 100: tensor([[  0., 217.,  85.,  ...,  53., 100.,  14.]]),\n",
       " 373: tensor([[  0., 799., 372.,  ..., 110., 464., 199.]]),\n",
       " 88: tensor([[ 0., 70., 31.,  ..., 18., 38.,  8.]]),\n",
       " 459: tensor([[  0., 568., 246.,  ...,  87., 232.,  69.]]),\n",
       " 331: tensor([[  0., 152.,  67.,  ...,  35., 135.,  49.]]),\n",
       " 215: tensor([[  0., 403., 191.,  ...,  67., 186.,  87.]]),\n",
       " 467: tensor([[  0., 181.,  63.,  ...,  40.,  88.,  30.]]),\n",
       " 122: tensor([[  0., 177.,  88.,  ...,  25., 122.,  60.]]),\n",
       " 615: tensor([[  0., 240., 106.,  ...,  69., 202.,  86.]]),\n",
       " 21: tensor([[  0., 617., 225.,  ...,  84., 346., 103.]]),\n",
       " 701: tensor([[  0., 128.,  49.,  ...,  30.,  65.,  13.]]),\n",
       " 72: tensor([[  0., 623., 289.,  ...,  87., 274., 125.]]),\n",
       " 107: tensor([[ 0., 87., 39.,  ..., 21., 42.,  9.]]),\n",
       " 271: tensor([[  0., 921., 425.,  ..., 170., 552., 209.]]),\n",
       " 861: tensor([[ 0., 56., 27.,  ..., 24., 85., 32.]]),\n",
       " 436: tensor([[  0., 581., 212.,  ...,  57., 290., 115.]]),\n",
       " 103: tensor([[  0., 170.,  72.,  ...,  26.,  58.,  24.]])}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_input_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([943, 15267]), torch.Size([1682, 19]))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_input_emb = torch.cat(tuple([user_input_emb[i] for i in user_input_emb]), dim = 0).float()\n",
    "item_input_emb = torch.cat(tuple([item_input_emb[i] for i in item_input_emb]), dim = 0).float()\n",
    "\n",
    "user_input_emb.shape, item_input_emb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 64\n",
    "\n",
    "\n",
    "# Pretrained embeddings\n",
    "user_pretrained_embedding = nn.Embedding(num_embeddings = len(user_ids), embedding_dim = embedding_dim)\n",
    "item_pretrained_embedding = nn.Embedding(num_embeddings = len(item_ids), embedding_dim = embedding_dim)\n",
    "\n",
    "# Project user_input_emb and item_input_emb to embedding_dim\n",
    "user_input_MLP = nn.Linear(user_input_emb.shape[1], embedding_dim)\n",
    "item_input_MLP = nn.Linear(item_input_emb.shape[1], embedding_dim)\n",
    "\n",
    "# Concat pretrained and input embedding\n",
    "user_input_emb_2 = user_input_MLP(user_input_emb)\n",
    "item_input_emb_2 = item_input_MLP(item_input_emb)\n",
    "\n",
    "user_embedding = torch.cat((user_pretrained_embedding.weight, user_input_emb_2), dim = -1)\n",
    "item_embedding = torch.cat((item_pretrained_embedding.weight, item_input_emb_2), dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([943, 128]), torch.Size([1682, 128]))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_embedding.size(), item_embedding.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(user_pretrained_embedding.weight, \"user_pretrained_embedding.pt\")\n",
    "torch.save(item_pretrained_embedding.weight, \"item_pretrained_embedding.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.1741e+00,  1.7420e-01,  8.8006e-01,  ..., -7.2083e+01,\n",
       "           8.5437e+01, -5.7284e+00],\n",
       "         [-1.4877e+00,  7.9509e-01, -3.4168e-01,  ..., -1.6482e+02,\n",
       "           1.9473e+02, -1.0673e+01],\n",
       "         [-2.5587e+00,  1.1711e+00, -9.0951e-01,  ..., -2.4612e+02,\n",
       "           2.7303e+02, -1.6895e+01],\n",
       "         ...,\n",
       "         [-4.1602e-01,  4.9267e-01,  7.6463e-01,  ..., -1.1972e+02,\n",
       "           1.3837e+02, -6.9562e+00],\n",
       "         [-1.2013e+00, -2.3896e-03, -8.9671e-02,  ..., -4.4966e+01,\n",
       "           5.9529e+01, -1.2013e+00],\n",
       "         [ 4.2495e-01, -1.9999e+00, -9.2644e-01,  ..., -4.1952e+01,\n",
       "           4.9369e+01,  5.4042e-02]], grad_fn=<CatBackward0>),\n",
       " tensor([[ 1.1741e+00,  1.7420e-01,  8.8006e-01,  5.0402e-01, -1.1589e+00,\n",
       "           5.6583e-01, -2.8851e-02, -1.2438e-01,  1.8789e+00,  7.9068e-01,\n",
       "          -4.7534e-01,  5.9629e-02,  8.1550e-01,  5.3771e-01, -5.3345e-01,\n",
       "          -1.5870e+00,  9.9692e-01, -2.8098e-01, -9.4895e-01, -2.9070e+00,\n",
       "          -1.9732e+00, -2.8063e+00,  1.3808e+00,  4.9596e-01,  2.4061e-01,\n",
       "           1.3184e+00, -3.4852e-01,  9.9240e-01,  5.7137e-01, -4.2479e-01,\n",
       "          -2.2496e-02, -2.2645e+00,  7.7638e-01,  2.0166e-01,  1.2118e+00,\n",
       "          -1.0200e+00, -9.5355e-01,  1.1493e+00,  1.0605e+00,  4.9708e-01,\n",
       "           1.2273e+00,  1.1912e+00, -8.1293e-01, -9.9637e-01,  2.0426e-01,\n",
       "          -4.4666e-01, -1.8471e+00,  1.9668e+00, -1.6279e+00,  9.0579e-04,\n",
       "          -2.2314e+00, -2.0697e-01, -5.3597e-01,  6.0738e-01,  1.9817e-01,\n",
       "           6.8449e-01,  5.8757e-01,  3.1609e-01, -8.5419e-01,  1.7998e+00,\n",
       "           1.5867e+00, -7.7808e-01, -2.7277e-01, -1.0501e+00,  3.1502e+01,\n",
       "          -2.5664e+02, -1.3077e+02, -2.5473e+02, -4.6502e+02, -2.2057e+02,\n",
       "          -2.1723e+02, -1.1400e+01, -6.1848e+01,  8.0966e+01, -6.2167e+01,\n",
       "           4.0637e+02,  4.7026e+01,  1.2908e+02,  8.6916e+01, -2.5567e+01,\n",
       "           8.9789e+01, -1.0807e+02,  4.8080e+02, -9.5389e+00, -1.1041e+02,\n",
       "          -1.1199e+02, -1.6087e+02,  9.7283e+01, -7.4866e+00, -2.9554e+02,\n",
       "          -1.9836e+02,  3.2649e+01,  3.0018e+02, -2.0438e+02,  1.2740e+02,\n",
       "           4.3307e+01, -5.9228e+00,  4.9162e+01,  1.1331e+02,  1.6821e+02,\n",
       "          -2.8793e+02,  2.5107e+02,  6.0495e+01, -2.4641e+02,  1.2444e+02,\n",
       "          -3.7059e+01,  8.6258e+01,  3.3623e+02, -2.0826e+02, -3.6903e+01,\n",
       "           7.1243e+01,  1.4623e+02, -9.9210e+01,  1.8733e+02,  6.6497e+01,\n",
       "           2.7378e+01,  1.2952e+02,  1.3531e+02, -2.5971e+01, -3.7429e+01,\n",
       "           2.6177e+02, -8.7655e+01,  8.7377e+01, -1.9457e+02,  1.7215e+02,\n",
       "          -7.2083e+01,  8.5437e+01, -5.7284e+00]]))"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_embedding, nn.Embedding.from_pretrained(user_embedding)(torch.tensor([0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 20:28:53,912 - Recommender model - [INFO]: Using CPU\n",
      "2024-11-20 20:28:53,914 - Recommender model - [INFO]: Preparing data...\n",
      "2024-11-20 20:28:54,163 - Recommender model - [INFO]: Data Loaded.\n",
      "2024-11-20 20:28:54,815 - Recommender model - [INFO]: Generating negative samples...\n",
      "2024-11-20 20:28:55,511 - Recommender model - [INFO]: Generating negative samples...\n",
      "d:\\NUS\\CS5284_graph_ml\\project\\dinghui101\\models\\recommender.py:182: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  umam_embeddings = torch.load(self.input_embedding_paths['umam'])\n",
      "d:\\NUS\\CS5284_graph_ml\\project\\dinghui101\\models\\recommender.py:183: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  umdm_embeddings = torch.load(self.input_embedding_paths['umdm'])\n",
      "d:\\NUS\\CS5284_graph_ml\\project\\dinghui101\\models\\recommender.py:184: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  umum_embeddings = torch.load(self.input_embedding_paths['umum'])\n",
      "d:\\NUS\\CS5284_graph_ml\\project\\dinghui101\\models\\recommender.py:185: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  user_content_embedding = torch.load(self.input_embedding_paths['user_content'])\n",
      "d:\\NUS\\CS5284_graph_ml\\project\\dinghui101\\models\\recommender.py:194: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  item_input_emb = torch.load(self.input_embedding_paths[\"item_content\"])\n",
      "d:\\NUS\\CS5284_graph_ml\\project\\dinghui101\\models\\recommender.py:201: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.user_pretrained_embedding = torch.load(self.input_embedding_paths[\"user_pretrained\"])\n",
      "d:\\NUS\\CS5284_graph_ml\\project\\dinghui101\\models\\recommender.py:202: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.item_pretrained_embedding = torch.load(self.input_embedding_paths[\"item_pretrained\"])\n",
      "2024-11-20 20:28:55,999 - Recommender model - [INFO]: building graph...\n",
      "2024-11-20 20:28:56,027 - Recommender model - [INFO]: initilizing model...\n"
     ]
    }
   ],
   "source": [
    "from models.recommender import RecommenderModel\n",
    "import os\n",
    "\n",
    "rec_model = RecommenderModel(\n",
    "        size=\"100k\",\n",
    "        num_layers=3,\n",
    "        num_negatives=4,\n",
    "        embedding_dim=64,\n",
    "        learning_rate=0.01,\n",
    "        epochs=500,\n",
    "        batch_size=1024,\n",
    "        device='cpu',\n",
    "        input_embedding_paths={\"umam\": os.path.join(\"data\", \"umam_embeddings.pt\"),\n",
    "                               \"umdm\": os.path.join(\"data\", \"umdm_embeddings.pt\"),\n",
    "                               \"umum\": os.path.join(\"data\", \"umum_embeddings.pt\"),\n",
    "                               \"user_content\": os.path.join(\"data\", \"user_content_based_embeddings.pt\"),\n",
    "                               \"item_content\": os.path.join(\"data\", \"movie_genre_hot_embeddings.pt\"),\n",
    "                               \"user_pretrained\": os.path.join(\"data\", \"user_pretrained_embedding.pt\"),\n",
    "                               \"item_pretrained\": os.path.join(\"data\", \"item_pretrained_embedding.pt\") }\n",
    "    )\n",
    "\n",
    "rec_model.prepare_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of Recommender(\n",
       "  (mlp_u): Linear(in_features=16092, out_features=64, bias=True)\n",
       "  (mlp_i): Linear(in_features=19, out_features=64, bias=True)\n",
       "  (transformer_layer): TransformerLayer(\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec_model.model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 20:30:50,807 - Recommender model - [INFO]: Preparing data...\n",
      "2024-11-20 20:30:51,240 - Recommender model - [INFO]: Data Loaded.\n",
      "2024-11-20 20:30:51,663 - Recommender model - [INFO]: Generating negative samples...\n",
      "2024-11-20 20:30:52,301 - Recommender model - [INFO]: Generating negative samples...\n",
      "d:\\NUS\\CS5284_graph_ml\\project\\dinghui101\\models\\recommender.py:182: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  umam_embeddings = torch.load(self.input_embedding_paths['umam'])\n",
      "d:\\NUS\\CS5284_graph_ml\\project\\dinghui101\\models\\recommender.py:183: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  umdm_embeddings = torch.load(self.input_embedding_paths['umdm'])\n",
      "d:\\NUS\\CS5284_graph_ml\\project\\dinghui101\\models\\recommender.py:184: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  umum_embeddings = torch.load(self.input_embedding_paths['umum'])\n",
      "d:\\NUS\\CS5284_graph_ml\\project\\dinghui101\\models\\recommender.py:185: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  user_content_embedding = torch.load(self.input_embedding_paths['user_content'])\n",
      "d:\\NUS\\CS5284_graph_ml\\project\\dinghui101\\models\\recommender.py:194: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  item_input_emb = torch.load(self.input_embedding_paths[\"item_content\"])\n",
      "d:\\NUS\\CS5284_graph_ml\\project\\dinghui101\\models\\recommender.py:201: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.user_pretrained_embedding = torch.load(self.input_embedding_paths[\"user_pretrained\"])\n",
      "d:\\NUS\\CS5284_graph_ml\\project\\dinghui101\\models\\recommender.py:202: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.item_pretrained_embedding = torch.load(self.input_embedding_paths[\"item_pretrained\"])\n",
      "2024-11-20 20:30:52,793 - Recommender model - [INFO]: building graph...\n",
      "2024-11-20 20:30:52,819 - Recommender model - [INFO]: initilizing model...\n",
      "2024-11-20 20:30:53,347 - Recommender model - [INFO]: Epoch 1/500, BPR Loss: 0.7095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500, BPR Loss: 0.7095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 20:30:53,759 - Recommender model - [INFO]: Epoch 2/500, BPR Loss: 0.6827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/500, BPR Loss: 0.6827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 20:30:54,232 - Recommender model - [INFO]: Epoch 3/500, BPR Loss: 0.6749\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/500, BPR Loss: 0.6749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 20:30:54,671 - Recommender model - [INFO]: Epoch 4/500, BPR Loss: 0.6944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/500, BPR Loss: 0.6944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 20:30:55,086 - Recommender model - [INFO]: Epoch 5/500, BPR Loss: 0.7171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/500, BPR Loss: 0.7171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 20:30:55,524 - Recommender model - [INFO]: Epoch 6/500, BPR Loss: 0.6805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/500, BPR Loss: 0.6805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 20:30:55,842 - Recommender model - [INFO]: Epoch 7/500, BPR Loss: 0.6789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/500, BPR Loss: 0.6789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 20:30:56,273 - Recommender model - [INFO]: Epoch 8/500, BPR Loss: 0.6874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/500, BPR Loss: 0.6874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 20:30:56,696 - Recommender model - [INFO]: Epoch 9/500, BPR Loss: 0.6712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/500, BPR Loss: 0.6712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 20:30:57,104 - Recommender model - [INFO]: Epoch 10/500, BPR Loss: 0.6733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/500, Recall@K: 0.022269353128313893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 20:30:58,147 - Recommender model - [INFO]: Epoch 11/500, BPR Loss: 0.6751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/500, BPR Loss: 0.6751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 20:30:58,447 - Recommender model - [INFO]: Epoch 12/500, BPR Loss: 0.6709\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/500, BPR Loss: 0.6709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 20:30:58,796 - Recommender model - [INFO]: Epoch 13/500, BPR Loss: 0.6687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/500, BPR Loss: 0.6687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 20:30:59,153 - Recommender model - [INFO]: Epoch 14/500, BPR Loss: 0.6690\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/500, BPR Loss: 0.6690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 20:30:59,628 - Recommender model - [INFO]: Epoch 15/500, BPR Loss: 0.6706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/500, BPR Loss: 0.6706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 20:31:00,084 - Recommender model - [INFO]: Epoch 16/500, BPR Loss: 0.6700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/500, BPR Loss: 0.6700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 20:31:00,501 - Recommender model - [INFO]: Epoch 17/500, BPR Loss: 0.6682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/500, BPR Loss: 0.6682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 20:31:00,910 - Recommender model - [INFO]: Epoch 18/500, BPR Loss: 0.6678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/500, BPR Loss: 0.6678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 20:31:01,220 - Recommender model - [INFO]: Epoch 19/500, BPR Loss: 0.6684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/500, BPR Loss: 0.6684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 20:31:01,662 - Recommender model - [INFO]: Epoch 20/500, BPR Loss: 0.6695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/500, Recall@K: 0.021208907741251327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 20:31:02,728 - Recommender model - [INFO]: Epoch 21/500, BPR Loss: 0.6688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/500, BPR Loss: 0.6688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 20:31:03,109 - Recommender model - [INFO]: Epoch 22/500, BPR Loss: 0.6680\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/500, BPR Loss: 0.6680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 20:31:03,462 - Recommender model - [INFO]: Epoch 23/500, BPR Loss: 0.6681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/500, BPR Loss: 0.6681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 20:31:03,929 - Recommender model - [INFO]: Epoch 24/500, BPR Loss: 0.6680\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/500, BPR Loss: 0.6680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 20:31:04,429 - Recommender model - [INFO]: Epoch 25/500, BPR Loss: 0.6680\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/500, BPR Loss: 0.6680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 20:31:04,897 - Recommender model - [INFO]: Epoch 26/500, BPR Loss: 0.6682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/500, BPR Loss: 0.6682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 20:31:05,329 - Recommender model - [INFO]: Epoch 27/500, BPR Loss: 0.6680\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/500, BPR Loss: 0.6680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 20:31:05,820 - Recommender model - [INFO]: Epoch 28/500, BPR Loss: 0.6678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/500, BPR Loss: 0.6678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 20:31:06,293 - Recommender model - [INFO]: Epoch 29/500, BPR Loss: 0.6677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/500, BPR Loss: 0.6677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 20:31:06,648 - Recommender model - [INFO]: Epoch 30/500, BPR Loss: 0.6680\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/500, Recall@K: 0.02014846235418876\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m calculate_ranking_metrics, calculate_rating_metrics\n\u001b[0;32m      3\u001b[0m rec_model\u001b[38;5;241m.\u001b[39mprepare_training_data()\n\u001b[1;32m----> 4\u001b[0m \u001b[43mrec_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m predictions \u001b[38;5;241m=\u001b[39m rec_model\u001b[38;5;241m.\u001b[39mpredict()\n\u001b[0;32m      7\u001b[0m ratings \u001b[38;5;241m=\u001b[39m calculate_rating_metrics(rec_model\u001b[38;5;241m.\u001b[39mtest_pre, predictions)\n",
      "File \u001b[1;32md:\\NUS\\CS5284_graph_ml\\project\\dinghui101\\models\\recommender.py:414\u001b[0m, in \u001b[0;36mRecommenderModel.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    411\u001b[0m \u001b[38;5;66;03m# Combine losses\u001b[39;00m\n\u001b[0;32m    412\u001b[0m loss \u001b[38;5;241m=\u001b[39m bpr_loss \u001b[38;5;66;03m#+ self.train_alpha * mse_loss\u001b[39;00m\n\u001b[1;32m--> 414\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, BPR Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\xiaoyicong\\anaconda3\\envs\\dinghui101\\lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\xiaoyicong\\anaconda3\\envs\\dinghui101\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\xiaoyicong\\anaconda3\\envs\\dinghui101\\lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from utils.metrics import calculate_ranking_metrics, calculate_rating_metrics\n",
    "\n",
    "rec_model.prepare_training_data()\n",
    "rec_model.train()\n",
    "predictions = rec_model.predict()\n",
    "\n",
    "ratings = calculate_rating_metrics(rec_model.test_pre, predictions)\n",
    "\n",
    "top_k_scores = rec_model.recommend_k()\n",
    "rankings = calculate_ranking_metrics(rec_model.test_pre, top_k_scores, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of LightGCN(\n",
       "  (user_embedding): Embedding(943, 64)\n",
       "  (item_embedding): Embedding(1682, 64)\n",
       ")>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgcn_model.model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 17:43:44,489 - LGCN2 model - [INFO]: Using CPU\n",
      "2024-11-20 17:43:44,489 - LGCN2 model - [INFO]: Preparing data...\n",
      "2024-11-20 17:43:44,740 - LGCN2 model - [INFO]: Data Loaded.\n",
      "2024-11-20 17:43:45,222 - LGCN2 model - [INFO]: Generating negative samples...\n",
      "2024-11-20 17:43:45,740 - LGCN2 model - [INFO]: Generating negative samples...\n",
      "d:\\NUS\\CS5284_graph_ml\\project\\dinghui101\\models\\lightgcn_model_exp.py:173: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  umam_embeddings = torch.load(paths['umam'])\n",
      "d:\\NUS\\CS5284_graph_ml\\project\\dinghui101\\models\\lightgcn_model_exp.py:174: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  umdm_embeddings = torch.load(paths['umdm'])\n",
      "d:\\NUS\\CS5284_graph_ml\\project\\dinghui101\\models\\lightgcn_model_exp.py:175: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  umum_embeddings = torch.load(paths['umum'])\n",
      "d:\\NUS\\CS5284_graph_ml\\project\\dinghui101\\models\\lightgcn_model_exp.py:176: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  user_content_embedding = torch.load(paths['user_content'])\n",
      "d:\\NUS\\CS5284_graph_ml\\project\\dinghui101\\models\\lightgcn_model_exp.py:185: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  item_input_emb = torch.load(paths[\"item_content\"])\n",
      "d:\\NUS\\CS5284_graph_ml\\project\\dinghui101\\models\\lightgcn_model_exp.py:192: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.user_pretrained_embedding = torch.load(paths[\"user_pretrained\"])\n",
      "d:\\NUS\\CS5284_graph_ml\\project\\dinghui101\\models\\lightgcn_model_exp.py:193: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.item_pretrained_embedding = torch.load(paths[\"item_pretrained\"])\n",
      "2024-11-20 17:43:46,226 - LGCN2 model - [INFO]: building graph...\n",
      "2024-11-20 17:43:46,249 - LGCN2 model - [INFO]: initilizing model...\n"
     ]
    }
   ],
   "source": [
    "from models.lightgcn_model_exp import LightGCNModel2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "lgcn_model2 = LightGCNModel2(\n",
    "        size=\"100k\",\n",
    "        num_layers=3,\n",
    "        num_negatives=4,\n",
    "        embedding_dim=64,\n",
    "        learning_rate=0.01,\n",
    "        epochs=1000,\n",
    "        batch_size=1024,\n",
    "        device='cpu'\n",
    "    )\n",
    "\n",
    "lgcn_model2.prepare_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of LightGCN2(\n",
       "  (mlp_u): Linear(in_features=16092, out_features=64, bias=True)\n",
       "  (mlp_i): Linear(in_features=19, out_features=64, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgcn_model2.model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 17:43:57,658 - LGCN2 model - [INFO]: Epoch 1/1000, BPR Loss: 5.1499\n",
      "2024-11-20 17:43:57,959 - LGCN2 model - [INFO]: Epoch 2/1000, BPR Loss: 5.1676\n",
      "2024-11-20 17:43:58,271 - LGCN2 model - [INFO]: Epoch 3/1000, BPR Loss: 5.1679\n",
      "2024-11-20 17:43:58,637 - LGCN2 model - [INFO]: Epoch 4/1000, BPR Loss: 5.1680\n",
      "2024-11-20 17:43:58,958 - LGCN2 model - [INFO]: Epoch 5/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:43:59,287 - LGCN2 model - [INFO]: Epoch 6/1000, BPR Loss: 5.1680\n",
      "2024-11-20 17:43:59,587 - LGCN2 model - [INFO]: Epoch 7/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:43:59,915 - LGCN2 model - [INFO]: Epoch 8/1000, BPR Loss: 5.1683\n",
      "2024-11-20 17:44:00,197 - LGCN2 model - [INFO]: Epoch 9/1000, BPR Loss: 5.1682\n",
      "2024-11-20 17:44:00,490 - LGCN2 model - [INFO]: Epoch 10/1000, BPR Loss: 5.1683\n",
      "2024-11-20 17:44:00,777 - LGCN2 model - [INFO]: Epoch 11/1000, BPR Loss: 5.1683\n",
      "2024-11-20 17:44:01,054 - LGCN2 model - [INFO]: Epoch 12/1000, BPR Loss: 5.1683\n",
      "2024-11-20 17:44:01,354 - LGCN2 model - [INFO]: Epoch 13/1000, BPR Loss: 5.1685\n",
      "2024-11-20 17:44:01,655 - LGCN2 model - [INFO]: Epoch 14/1000, BPR Loss: 5.1685\n",
      "2024-11-20 17:44:01,920 - LGCN2 model - [INFO]: Epoch 15/1000, BPR Loss: 5.1684\n",
      "2024-11-20 17:44:02,220 - LGCN2 model - [INFO]: Epoch 16/1000, BPR Loss: 5.1685\n",
      "2024-11-20 17:44:02,520 - LGCN2 model - [INFO]: Epoch 17/1000, BPR Loss: 5.1685\n",
      "2024-11-20 17:44:02,791 - LGCN2 model - [INFO]: Epoch 18/1000, BPR Loss: 5.1685\n",
      "2024-11-20 17:44:03,087 - LGCN2 model - [INFO]: Epoch 19/1000, BPR Loss: 5.1686\n",
      "2024-11-20 17:44:03,324 - LGCN2 model - [INFO]: Epoch 20/1000, BPR Loss: 5.1686\n",
      "2024-11-20 17:44:03,570 - LGCN2 model - [INFO]: Epoch 21/1000, BPR Loss: 5.1686\n",
      "2024-11-20 17:44:03,787 - LGCN2 model - [INFO]: Epoch 22/1000, BPR Loss: 5.1686\n",
      "2024-11-20 17:44:04,048 - LGCN2 model - [INFO]: Epoch 23/1000, BPR Loss: 5.1686\n",
      "2024-11-20 17:44:04,290 - LGCN2 model - [INFO]: Epoch 24/1000, BPR Loss: 5.1686\n",
      "2024-11-20 17:44:04,521 - LGCN2 model - [INFO]: Epoch 25/1000, BPR Loss: 5.1688\n",
      "2024-11-20 17:44:04,787 - LGCN2 model - [INFO]: Epoch 26/1000, BPR Loss: 5.1688\n",
      "2024-11-20 17:44:05,020 - LGCN2 model - [INFO]: Epoch 27/1000, BPR Loss: 5.1688\n",
      "2024-11-20 17:44:05,272 - LGCN2 model - [INFO]: Epoch 28/1000, BPR Loss: 5.1687\n",
      "2024-11-20 17:44:05,537 - LGCN2 model - [INFO]: Epoch 29/1000, BPR Loss: 5.1688\n",
      "2024-11-20 17:44:05,773 - LGCN2 model - [INFO]: Epoch 30/1000, BPR Loss: 5.1688\n",
      "2024-11-20 17:44:06,003 - LGCN2 model - [INFO]: Epoch 31/1000, BPR Loss: 5.1688\n",
      "2024-11-20 17:44:06,270 - LGCN2 model - [INFO]: Epoch 32/1000, BPR Loss: 5.1689\n",
      "2024-11-20 17:44:06,503 - LGCN2 model - [INFO]: Epoch 33/1000, BPR Loss: 5.1689\n",
      "2024-11-20 17:44:06,737 - LGCN2 model - [INFO]: Epoch 34/1000, BPR Loss: 5.1689\n",
      "2024-11-20 17:44:07,003 - LGCN2 model - [INFO]: Epoch 35/1000, BPR Loss: 5.1690\n",
      "2024-11-20 17:44:07,254 - LGCN2 model - [INFO]: Epoch 36/1000, BPR Loss: 5.1689\n",
      "2024-11-20 17:44:07,473 - LGCN2 model - [INFO]: Epoch 37/1000, BPR Loss: 5.1690\n",
      "2024-11-20 17:44:07,705 - LGCN2 model - [INFO]: Epoch 38/1000, BPR Loss: 5.1690\n",
      "2024-11-20 17:44:07,964 - LGCN2 model - [INFO]: Epoch 39/1000, BPR Loss: 5.1690\n",
      "2024-11-20 17:44:08,206 - LGCN2 model - [INFO]: Epoch 40/1000, BPR Loss: 5.1690\n",
      "2024-11-20 17:44:08,439 - LGCN2 model - [INFO]: Epoch 41/1000, BPR Loss: 5.1691\n",
      "2024-11-20 17:44:08,687 - LGCN2 model - [INFO]: Epoch 42/1000, BPR Loss: 5.1691\n",
      "2024-11-20 17:44:08,939 - LGCN2 model - [INFO]: Epoch 43/1000, BPR Loss: 5.1692\n",
      "2024-11-20 17:44:09,173 - LGCN2 model - [INFO]: Epoch 44/1000, BPR Loss: 5.1692\n",
      "2024-11-20 17:44:09,422 - LGCN2 model - [INFO]: Epoch 45/1000, BPR Loss: 5.1693\n",
      "2024-11-20 17:44:09,671 - LGCN2 model - [INFO]: Epoch 46/1000, BPR Loss: 5.1693\n",
      "2024-11-20 17:44:09,923 - LGCN2 model - [INFO]: Epoch 47/1000, BPR Loss: 5.1693\n",
      "2024-11-20 17:44:10,170 - LGCN2 model - [INFO]: Epoch 48/1000, BPR Loss: 5.1694\n",
      "2024-11-20 17:44:10,471 - LGCN2 model - [INFO]: Epoch 49/1000, BPR Loss: 5.1694\n",
      "2024-11-20 17:44:10,723 - LGCN2 model - [INFO]: Epoch 50/1000, BPR Loss: 5.1695\n",
      "2024-11-20 17:44:10,975 - LGCN2 model - [INFO]: Epoch 51/1000, BPR Loss: 5.1695\n",
      "2024-11-20 17:44:11,253 - LGCN2 model - [INFO]: Epoch 52/1000, BPR Loss: 5.1695\n",
      "2024-11-20 17:44:11,505 - LGCN2 model - [INFO]: Epoch 53/1000, BPR Loss: 5.1695\n",
      "2024-11-20 17:44:11,737 - LGCN2 model - [INFO]: Epoch 54/1000, BPR Loss: 5.1695\n",
      "2024-11-20 17:44:11,987 - LGCN2 model - [INFO]: Epoch 55/1000, BPR Loss: 5.1694\n",
      "2024-11-20 17:44:12,225 - LGCN2 model - [INFO]: Epoch 56/1000, BPR Loss: 5.1693\n",
      "2024-11-20 17:44:12,487 - LGCN2 model - [INFO]: Epoch 57/1000, BPR Loss: 5.1693\n",
      "2024-11-20 17:44:12,737 - LGCN2 model - [INFO]: Epoch 58/1000, BPR Loss: 5.1693\n",
      "2024-11-20 17:44:12,987 - LGCN2 model - [INFO]: Epoch 59/1000, BPR Loss: 5.1693\n",
      "2024-11-20 17:44:13,254 - LGCN2 model - [INFO]: Epoch 60/1000, BPR Loss: 5.1693\n",
      "2024-11-20 17:44:13,521 - LGCN2 model - [INFO]: Epoch 61/1000, BPR Loss: 5.1694\n",
      "2024-11-20 17:44:13,774 - LGCN2 model - [INFO]: Epoch 62/1000, BPR Loss: 5.1694\n",
      "2024-11-20 17:44:14,026 - LGCN2 model - [INFO]: Epoch 63/1000, BPR Loss: 5.1693\n",
      "2024-11-20 17:44:14,304 - LGCN2 model - [INFO]: Epoch 64/1000, BPR Loss: 5.1693\n",
      "2024-11-20 17:44:14,554 - LGCN2 model - [INFO]: Epoch 65/1000, BPR Loss: 5.1693\n",
      "2024-11-20 17:44:14,803 - LGCN2 model - [INFO]: Epoch 66/1000, BPR Loss: 5.1693\n",
      "2024-11-20 17:44:15,053 - LGCN2 model - [INFO]: Epoch 67/1000, BPR Loss: 5.1693\n",
      "2024-11-20 17:44:15,304 - LGCN2 model - [INFO]: Epoch 68/1000, BPR Loss: 5.1693\n",
      "2024-11-20 17:44:15,570 - LGCN2 model - [INFO]: Epoch 69/1000, BPR Loss: 5.1694\n",
      "2024-11-20 17:44:15,827 - LGCN2 model - [INFO]: Epoch 70/1000, BPR Loss: 5.1694\n",
      "2024-11-20 17:44:16,073 - LGCN2 model - [INFO]: Epoch 71/1000, BPR Loss: 5.1694\n",
      "2024-11-20 17:44:16,354 - LGCN2 model - [INFO]: Epoch 72/1000, BPR Loss: 5.1695\n",
      "2024-11-20 17:44:16,624 - LGCN2 model - [INFO]: Epoch 73/1000, BPR Loss: 5.1695\n",
      "2024-11-20 17:44:16,853 - LGCN2 model - [INFO]: Epoch 74/1000, BPR Loss: 5.1695\n",
      "2024-11-20 17:44:17,103 - LGCN2 model - [INFO]: Epoch 75/1000, BPR Loss: 5.1695\n",
      "2024-11-20 17:44:17,356 - LGCN2 model - [INFO]: Epoch 76/1000, BPR Loss: 5.1694\n",
      "2024-11-20 17:44:17,603 - LGCN2 model - [INFO]: Epoch 77/1000, BPR Loss: 5.1689\n",
      "2024-11-20 17:44:17,837 - LGCN2 model - [INFO]: Epoch 78/1000, BPR Loss: 5.1695\n",
      "2024-11-20 17:44:18,087 - LGCN2 model - [INFO]: Epoch 79/1000, BPR Loss: 5.1692\n",
      "2024-11-20 17:44:18,337 - LGCN2 model - [INFO]: Epoch 80/1000, BPR Loss: 5.1675\n",
      "2024-11-20 17:44:18,586 - LGCN2 model - [INFO]: Epoch 81/1000, BPR Loss: 5.1686\n",
      "2024-11-20 17:44:18,839 - LGCN2 model - [INFO]: Epoch 82/1000, BPR Loss: 5.1676\n",
      "2024-11-20 17:44:19,086 - LGCN2 model - [INFO]: Epoch 83/1000, BPR Loss: 5.1674\n",
      "2024-11-20 17:44:19,353 - LGCN2 model - [INFO]: Epoch 84/1000, BPR Loss: 5.1675\n",
      "2024-11-20 17:44:19,629 - LGCN2 model - [INFO]: Epoch 85/1000, BPR Loss: 5.1678\n",
      "2024-11-20 17:44:19,887 - LGCN2 model - [INFO]: Epoch 86/1000, BPR Loss: 5.1679\n",
      "2024-11-20 17:44:20,137 - LGCN2 model - [INFO]: Epoch 87/1000, BPR Loss: 5.1676\n",
      "2024-11-20 17:44:20,387 - LGCN2 model - [INFO]: Epoch 88/1000, BPR Loss: 5.1675\n",
      "2024-11-20 17:44:20,653 - LGCN2 model - [INFO]: Epoch 89/1000, BPR Loss: 5.1668\n",
      "2024-11-20 17:44:20,938 - LGCN2 model - [INFO]: Epoch 90/1000, BPR Loss: 5.1664\n",
      "2024-11-20 17:44:21,189 - LGCN2 model - [INFO]: Epoch 91/1000, BPR Loss: 5.1663\n",
      "2024-11-20 17:44:21,437 - LGCN2 model - [INFO]: Epoch 92/1000, BPR Loss: 5.1659\n",
      "2024-11-20 17:44:21,720 - LGCN2 model - [INFO]: Epoch 93/1000, BPR Loss: 5.1659\n",
      "2024-11-20 17:44:21,970 - LGCN2 model - [INFO]: Epoch 94/1000, BPR Loss: 5.1659\n",
      "2024-11-20 17:44:22,237 - LGCN2 model - [INFO]: Epoch 95/1000, BPR Loss: 5.1659\n",
      "2024-11-20 17:44:22,487 - LGCN2 model - [INFO]: Epoch 96/1000, BPR Loss: 5.1663\n",
      "2024-11-20 17:44:22,737 - LGCN2 model - [INFO]: Epoch 97/1000, BPR Loss: 5.1662\n",
      "2024-11-20 17:44:22,987 - LGCN2 model - [INFO]: Epoch 98/1000, BPR Loss: 5.1663\n",
      "2024-11-20 17:44:23,303 - LGCN2 model - [INFO]: Epoch 99/1000, BPR Loss: 5.1662\n",
      "2024-11-20 17:44:23,555 - LGCN2 model - [INFO]: Epoch 100/1000, BPR Loss: 5.1662\n",
      "2024-11-20 17:44:23,803 - LGCN2 model - [INFO]: Epoch 101/1000, BPR Loss: 5.1661\n",
      "2024-11-20 17:44:24,070 - LGCN2 model - [INFO]: Epoch 102/1000, BPR Loss: 5.1661\n",
      "2024-11-20 17:44:24,303 - LGCN2 model - [INFO]: Epoch 103/1000, BPR Loss: 5.1661\n",
      "2024-11-20 17:44:24,586 - LGCN2 model - [INFO]: Epoch 104/1000, BPR Loss: 5.1661\n",
      "2024-11-20 17:44:24,837 - LGCN2 model - [INFO]: Epoch 105/1000, BPR Loss: 5.1661\n",
      "2024-11-20 17:44:25,120 - LGCN2 model - [INFO]: Epoch 106/1000, BPR Loss: 5.1661\n",
      "2024-11-20 17:44:25,387 - LGCN2 model - [INFO]: Epoch 107/1000, BPR Loss: 5.1661\n",
      "2024-11-20 17:44:25,654 - LGCN2 model - [INFO]: Epoch 108/1000, BPR Loss: 5.1662\n",
      "2024-11-20 17:44:25,905 - LGCN2 model - [INFO]: Epoch 109/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:44:26,207 - LGCN2 model - [INFO]: Epoch 110/1000, BPR Loss: 5.1659\n",
      "2024-11-20 17:44:26,470 - LGCN2 model - [INFO]: Epoch 111/1000, BPR Loss: 5.1659\n",
      "2024-11-20 17:44:26,740 - LGCN2 model - [INFO]: Epoch 112/1000, BPR Loss: 5.1659\n",
      "2024-11-20 17:44:27,080 - LGCN2 model - [INFO]: Epoch 113/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:27,354 - LGCN2 model - [INFO]: Epoch 114/1000, BPR Loss: 5.1659\n",
      "2024-11-20 17:44:27,603 - LGCN2 model - [INFO]: Epoch 115/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:27,870 - LGCN2 model - [INFO]: Epoch 116/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:28,137 - LGCN2 model - [INFO]: Epoch 117/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:28,387 - LGCN2 model - [INFO]: Epoch 118/1000, BPR Loss: 5.1657\n",
      "2024-11-20 17:44:28,671 - LGCN2 model - [INFO]: Epoch 119/1000, BPR Loss: 5.1657\n",
      "2024-11-20 17:44:28,920 - LGCN2 model - [INFO]: Epoch 120/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:29,203 - LGCN2 model - [INFO]: Epoch 121/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:29,470 - LGCN2 model - [INFO]: Epoch 122/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:29,737 - LGCN2 model - [INFO]: Epoch 123/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:29,988 - LGCN2 model - [INFO]: Epoch 124/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:30,258 - LGCN2 model - [INFO]: Epoch 125/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:30,520 - LGCN2 model - [INFO]: Epoch 126/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:30,803 - LGCN2 model - [INFO]: Epoch 127/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:31,070 - LGCN2 model - [INFO]: Epoch 128/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:31,337 - LGCN2 model - [INFO]: Epoch 129/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:31,587 - LGCN2 model - [INFO]: Epoch 130/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:31,870 - LGCN2 model - [INFO]: Epoch 131/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:32,123 - LGCN2 model - [INFO]: Epoch 132/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:32,373 - LGCN2 model - [INFO]: Epoch 133/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:32,622 - LGCN2 model - [INFO]: Epoch 134/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:32,887 - LGCN2 model - [INFO]: Epoch 135/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:33,153 - LGCN2 model - [INFO]: Epoch 136/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:33,403 - LGCN2 model - [INFO]: Epoch 137/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:33,685 - LGCN2 model - [INFO]: Epoch 138/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:33,941 - LGCN2 model - [INFO]: Epoch 139/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:34,220 - LGCN2 model - [INFO]: Epoch 140/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:34,481 - LGCN2 model - [INFO]: Epoch 141/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:34,753 - LGCN2 model - [INFO]: Epoch 142/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:35,021 - LGCN2 model - [INFO]: Epoch 143/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:35,290 - LGCN2 model - [INFO]: Epoch 144/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:35,553 - LGCN2 model - [INFO]: Epoch 145/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:35,837 - LGCN2 model - [INFO]: Epoch 146/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:36,107 - LGCN2 model - [INFO]: Epoch 147/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:36,387 - LGCN2 model - [INFO]: Epoch 148/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:36,654 - LGCN2 model - [INFO]: Epoch 149/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:36,906 - LGCN2 model - [INFO]: Epoch 150/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:37,187 - LGCN2 model - [INFO]: Epoch 151/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:37,453 - LGCN2 model - [INFO]: Epoch 152/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:37,703 - LGCN2 model - [INFO]: Epoch 153/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:37,976 - LGCN2 model - [INFO]: Epoch 154/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:38,237 - LGCN2 model - [INFO]: Epoch 155/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:38,504 - LGCN2 model - [INFO]: Epoch 156/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:38,770 - LGCN2 model - [INFO]: Epoch 157/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:39,047 - LGCN2 model - [INFO]: Epoch 158/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:39,320 - LGCN2 model - [INFO]: Epoch 159/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:39,589 - LGCN2 model - [INFO]: Epoch 160/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:39,859 - LGCN2 model - [INFO]: Epoch 161/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:40,148 - LGCN2 model - [INFO]: Epoch 162/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:40,403 - LGCN2 model - [INFO]: Epoch 163/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:40,679 - LGCN2 model - [INFO]: Epoch 164/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:40,937 - LGCN2 model - [INFO]: Epoch 165/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:41,203 - LGCN2 model - [INFO]: Epoch 166/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:41,470 - LGCN2 model - [INFO]: Epoch 167/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:41,748 - LGCN2 model - [INFO]: Epoch 168/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:42,005 - LGCN2 model - [INFO]: Epoch 169/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:42,292 - LGCN2 model - [INFO]: Epoch 170/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:42,570 - LGCN2 model - [INFO]: Epoch 171/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:42,839 - LGCN2 model - [INFO]: Epoch 172/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:43,106 - LGCN2 model - [INFO]: Epoch 173/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:43,371 - LGCN2 model - [INFO]: Epoch 174/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:43,656 - LGCN2 model - [INFO]: Epoch 175/1000, BPR Loss: 5.1657\n",
      "2024-11-20 17:44:43,920 - LGCN2 model - [INFO]: Epoch 176/1000, BPR Loss: 5.1657\n",
      "2024-11-20 17:44:44,187 - LGCN2 model - [INFO]: Epoch 177/1000, BPR Loss: 5.1657\n",
      "2024-11-20 17:44:44,470 - LGCN2 model - [INFO]: Epoch 178/1000, BPR Loss: 5.1657\n",
      "2024-11-20 17:44:44,736 - LGCN2 model - [INFO]: Epoch 179/1000, BPR Loss: 5.1657\n",
      "2024-11-20 17:44:44,989 - LGCN2 model - [INFO]: Epoch 180/1000, BPR Loss: 5.1657\n",
      "2024-11-20 17:44:45,257 - LGCN2 model - [INFO]: Epoch 181/1000, BPR Loss: 5.1657\n",
      "2024-11-20 17:44:45,544 - LGCN2 model - [INFO]: Epoch 182/1000, BPR Loss: 5.1657\n",
      "2024-11-20 17:44:45,805 - LGCN2 model - [INFO]: Epoch 183/1000, BPR Loss: 5.1657\n",
      "2024-11-20 17:44:46,087 - LGCN2 model - [INFO]: Epoch 184/1000, BPR Loss: 5.1657\n",
      "2024-11-20 17:44:46,337 - LGCN2 model - [INFO]: Epoch 185/1000, BPR Loss: 5.1657\n",
      "2024-11-20 17:44:46,603 - LGCN2 model - [INFO]: Epoch 186/1000, BPR Loss: 5.1657\n",
      "2024-11-20 17:44:46,886 - LGCN2 model - [INFO]: Epoch 187/1000, BPR Loss: 5.1657\n",
      "2024-11-20 17:44:47,156 - LGCN2 model - [INFO]: Epoch 188/1000, BPR Loss: 5.1657\n",
      "2024-11-20 17:44:47,420 - LGCN2 model - [INFO]: Epoch 189/1000, BPR Loss: 5.1657\n",
      "2024-11-20 17:44:47,708 - LGCN2 model - [INFO]: Epoch 190/1000, BPR Loss: 5.1657\n",
      "2024-11-20 17:44:47,970 - LGCN2 model - [INFO]: Epoch 191/1000, BPR Loss: 5.1657\n",
      "2024-11-20 17:44:48,272 - LGCN2 model - [INFO]: Epoch 192/1000, BPR Loss: 5.1657\n",
      "2024-11-20 17:44:48,520 - LGCN2 model - [INFO]: Epoch 193/1000, BPR Loss: 5.1657\n",
      "2024-11-20 17:44:48,789 - LGCN2 model - [INFO]: Epoch 194/1000, BPR Loss: 5.1657\n",
      "2024-11-20 17:44:49,057 - LGCN2 model - [INFO]: Epoch 195/1000, BPR Loss: 5.1657\n",
      "2024-11-20 17:44:49,356 - LGCN2 model - [INFO]: Epoch 196/1000, BPR Loss: 5.1657\n",
      "2024-11-20 17:44:49,606 - LGCN2 model - [INFO]: Epoch 197/1000, BPR Loss: 5.1657\n",
      "2024-11-20 17:44:49,871 - LGCN2 model - [INFO]: Epoch 198/1000, BPR Loss: 5.1657\n",
      "2024-11-20 17:44:50,136 - LGCN2 model - [INFO]: Epoch 199/1000, BPR Loss: 5.1657\n",
      "2024-11-20 17:44:50,403 - LGCN2 model - [INFO]: Epoch 200/1000, BPR Loss: 5.1657\n",
      "2024-11-20 17:44:50,705 - LGCN2 model - [INFO]: Epoch 201/1000, BPR Loss: 5.1657\n",
      "2024-11-20 17:44:50,970 - LGCN2 model - [INFO]: Epoch 202/1000, BPR Loss: 5.1657\n",
      "2024-11-20 17:44:51,206 - LGCN2 model - [INFO]: Epoch 203/1000, BPR Loss: 5.1657\n",
      "2024-11-20 17:44:51,470 - LGCN2 model - [INFO]: Epoch 204/1000, BPR Loss: 5.1657\n",
      "2024-11-20 17:44:51,721 - LGCN2 model - [INFO]: Epoch 205/1000, BPR Loss: 5.1657\n",
      "2024-11-20 17:44:52,003 - LGCN2 model - [INFO]: Epoch 206/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:52,303 - LGCN2 model - [INFO]: Epoch 207/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:52,570 - LGCN2 model - [INFO]: Epoch 208/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:52,853 - LGCN2 model - [INFO]: Epoch 209/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:53,137 - LGCN2 model - [INFO]: Epoch 210/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:53,403 - LGCN2 model - [INFO]: Epoch 211/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:53,654 - LGCN2 model - [INFO]: Epoch 212/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:53,925 - LGCN2 model - [INFO]: Epoch 213/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:54,220 - LGCN2 model - [INFO]: Epoch 214/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:54,503 - LGCN2 model - [INFO]: Epoch 215/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:54,803 - LGCN2 model - [INFO]: Epoch 216/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:55,121 - LGCN2 model - [INFO]: Epoch 217/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:55,423 - LGCN2 model - [INFO]: Epoch 218/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:55,686 - LGCN2 model - [INFO]: Epoch 219/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:56,000 - LGCN2 model - [INFO]: Epoch 220/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:56,320 - LGCN2 model - [INFO]: Epoch 221/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:56,603 - LGCN2 model - [INFO]: Epoch 222/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:56,890 - LGCN2 model - [INFO]: Epoch 223/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:57,186 - LGCN2 model - [INFO]: Epoch 224/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:57,503 - LGCN2 model - [INFO]: Epoch 225/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:57,806 - LGCN2 model - [INFO]: Epoch 226/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:58,086 - LGCN2 model - [INFO]: Epoch 227/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:58,386 - LGCN2 model - [INFO]: Epoch 228/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:58,735 - LGCN2 model - [INFO]: Epoch 229/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:44:59,105 - LGCN2 model - [INFO]: Epoch 230/1000, BPR Loss: 5.1657\n",
      "2024-11-20 17:44:59,403 - LGCN2 model - [INFO]: Epoch 231/1000, BPR Loss: 5.1657\n",
      "2024-11-20 17:44:59,704 - LGCN2 model - [INFO]: Epoch 232/1000, BPR Loss: 5.1657\n",
      "2024-11-20 17:44:59,986 - LGCN2 model - [INFO]: Epoch 233/1000, BPR Loss: 5.1657\n",
      "2024-11-20 17:45:00,261 - LGCN2 model - [INFO]: Epoch 234/1000, BPR Loss: 5.1657\n",
      "2024-11-20 17:45:00,586 - LGCN2 model - [INFO]: Epoch 235/1000, BPR Loss: 5.1657\n",
      "2024-11-20 17:45:00,870 - LGCN2 model - [INFO]: Epoch 236/1000, BPR Loss: 5.1657\n",
      "2024-11-20 17:45:01,171 - LGCN2 model - [INFO]: Epoch 237/1000, BPR Loss: 5.1657\n",
      "2024-11-20 17:45:01,486 - LGCN2 model - [INFO]: Epoch 238/1000, BPR Loss: 5.1657\n",
      "2024-11-20 17:45:01,820 - LGCN2 model - [INFO]: Epoch 239/1000, BPR Loss: 5.1657\n",
      "2024-11-20 17:45:02,103 - LGCN2 model - [INFO]: Epoch 240/1000, BPR Loss: 5.1657\n",
      "2024-11-20 17:45:02,420 - LGCN2 model - [INFO]: Epoch 241/1000, BPR Loss: 5.1657\n",
      "2024-11-20 17:45:02,753 - LGCN2 model - [INFO]: Epoch 242/1000, BPR Loss: 5.1657\n",
      "2024-11-20 17:45:03,036 - LGCN2 model - [INFO]: Epoch 243/1000, BPR Loss: 5.1657\n",
      "2024-11-20 17:45:03,320 - LGCN2 model - [INFO]: Epoch 244/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:03,622 - LGCN2 model - [INFO]: Epoch 245/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:03,919 - LGCN2 model - [INFO]: Epoch 246/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:04,260 - LGCN2 model - [INFO]: Epoch 247/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:04,570 - LGCN2 model - [INFO]: Epoch 248/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:04,886 - LGCN2 model - [INFO]: Epoch 249/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:05,203 - LGCN2 model - [INFO]: Epoch 250/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:05,603 - LGCN2 model - [INFO]: Epoch 251/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:05,974 - LGCN2 model - [INFO]: Epoch 252/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:06,286 - LGCN2 model - [INFO]: Epoch 253/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:06,636 - LGCN2 model - [INFO]: Epoch 254/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:07,003 - LGCN2 model - [INFO]: Epoch 255/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:07,336 - LGCN2 model - [INFO]: Epoch 256/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:07,670 - LGCN2 model - [INFO]: Epoch 257/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:08,020 - LGCN2 model - [INFO]: Epoch 258/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:08,336 - LGCN2 model - [INFO]: Epoch 259/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:08,676 - LGCN2 model - [INFO]: Epoch 260/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:09,011 - LGCN2 model - [INFO]: Epoch 261/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:09,340 - LGCN2 model - [INFO]: Epoch 262/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:09,669 - LGCN2 model - [INFO]: Epoch 263/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:10,004 - LGCN2 model - [INFO]: Epoch 264/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:10,337 - LGCN2 model - [INFO]: Epoch 265/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:10,687 - LGCN2 model - [INFO]: Epoch 266/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:11,039 - LGCN2 model - [INFO]: Epoch 267/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:11,389 - LGCN2 model - [INFO]: Epoch 268/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:11,670 - LGCN2 model - [INFO]: Epoch 269/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:11,986 - LGCN2 model - [INFO]: Epoch 270/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:12,487 - LGCN2 model - [INFO]: Epoch 271/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:12,930 - LGCN2 model - [INFO]: Epoch 272/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:13,353 - LGCN2 model - [INFO]: Epoch 273/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:13,798 - LGCN2 model - [INFO]: Epoch 274/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:14,127 - LGCN2 model - [INFO]: Epoch 275/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:14,486 - LGCN2 model - [INFO]: Epoch 276/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:14,821 - LGCN2 model - [INFO]: Epoch 277/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:15,155 - LGCN2 model - [INFO]: Epoch 278/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:15,524 - LGCN2 model - [INFO]: Epoch 279/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:15,853 - LGCN2 model - [INFO]: Epoch 280/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:16,136 - LGCN2 model - [INFO]: Epoch 281/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:16,420 - LGCN2 model - [INFO]: Epoch 282/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:16,721 - LGCN2 model - [INFO]: Epoch 283/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:17,056 - LGCN2 model - [INFO]: Epoch 284/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:17,640 - LGCN2 model - [INFO]: Epoch 285/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:18,264 - LGCN2 model - [INFO]: Epoch 286/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:18,922 - LGCN2 model - [INFO]: Epoch 287/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:19,536 - LGCN2 model - [INFO]: Epoch 288/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:20,171 - LGCN2 model - [INFO]: Epoch 289/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:20,803 - LGCN2 model - [INFO]: Epoch 290/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:21,454 - LGCN2 model - [INFO]: Epoch 291/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:22,093 - LGCN2 model - [INFO]: Epoch 292/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:22,803 - LGCN2 model - [INFO]: Epoch 293/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:23,451 - LGCN2 model - [INFO]: Epoch 294/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:24,087 - LGCN2 model - [INFO]: Epoch 295/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:24,703 - LGCN2 model - [INFO]: Epoch 296/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:25,339 - LGCN2 model - [INFO]: Epoch 297/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:26,022 - LGCN2 model - [INFO]: Epoch 298/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:26,678 - LGCN2 model - [INFO]: Epoch 299/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:27,370 - LGCN2 model - [INFO]: Epoch 300/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:28,004 - LGCN2 model - [INFO]: Epoch 301/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:28,587 - LGCN2 model - [INFO]: Epoch 302/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:29,219 - LGCN2 model - [INFO]: Epoch 303/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:29,870 - LGCN2 model - [INFO]: Epoch 304/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:30,504 - LGCN2 model - [INFO]: Epoch 305/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:31,086 - LGCN2 model - [INFO]: Epoch 306/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:31,719 - LGCN2 model - [INFO]: Epoch 307/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:32,353 - LGCN2 model - [INFO]: Epoch 308/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:33,007 - LGCN2 model - [INFO]: Epoch 309/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:33,636 - LGCN2 model - [INFO]: Epoch 310/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:34,237 - LGCN2 model - [INFO]: Epoch 311/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:34,919 - LGCN2 model - [INFO]: Epoch 312/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:35,562 - LGCN2 model - [INFO]: Epoch 313/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:36,241 - LGCN2 model - [INFO]: Epoch 314/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:36,901 - LGCN2 model - [INFO]: Epoch 315/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:37,551 - LGCN2 model - [INFO]: Epoch 316/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:38,171 - LGCN2 model - [INFO]: Epoch 317/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:38,787 - LGCN2 model - [INFO]: Epoch 318/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:39,419 - LGCN2 model - [INFO]: Epoch 319/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:40,070 - LGCN2 model - [INFO]: Epoch 320/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:40,714 - LGCN2 model - [INFO]: Epoch 321/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:41,370 - LGCN2 model - [INFO]: Epoch 322/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:41,986 - LGCN2 model - [INFO]: Epoch 323/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:42,602 - LGCN2 model - [INFO]: Epoch 324/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:43,220 - LGCN2 model - [INFO]: Epoch 325/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:43,919 - LGCN2 model - [INFO]: Epoch 326/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:44,553 - LGCN2 model - [INFO]: Epoch 327/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:45,220 - LGCN2 model - [INFO]: Epoch 328/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:45,869 - LGCN2 model - [INFO]: Epoch 329/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:46,519 - LGCN2 model - [INFO]: Epoch 330/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:47,169 - LGCN2 model - [INFO]: Epoch 331/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:47,803 - LGCN2 model - [INFO]: Epoch 332/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:48,447 - LGCN2 model - [INFO]: Epoch 333/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:49,169 - LGCN2 model - [INFO]: Epoch 334/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:49,786 - LGCN2 model - [INFO]: Epoch 335/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:50,437 - LGCN2 model - [INFO]: Epoch 336/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:51,086 - LGCN2 model - [INFO]: Epoch 337/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:51,787 - LGCN2 model - [INFO]: Epoch 338/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:52,420 - LGCN2 model - [INFO]: Epoch 339/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:53,069 - LGCN2 model - [INFO]: Epoch 340/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:53,737 - LGCN2 model - [INFO]: Epoch 341/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:54,386 - LGCN2 model - [INFO]: Epoch 342/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:55,036 - LGCN2 model - [INFO]: Epoch 343/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:55,702 - LGCN2 model - [INFO]: Epoch 344/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:56,336 - LGCN2 model - [INFO]: Epoch 345/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:56,969 - LGCN2 model - [INFO]: Epoch 346/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:57,620 - LGCN2 model - [INFO]: Epoch 347/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:58,253 - LGCN2 model - [INFO]: Epoch 348/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:58,936 - LGCN2 model - [INFO]: Epoch 349/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:45:59,603 - LGCN2 model - [INFO]: Epoch 350/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:00,270 - LGCN2 model - [INFO]: Epoch 351/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:00,953 - LGCN2 model - [INFO]: Epoch 352/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:01,619 - LGCN2 model - [INFO]: Epoch 353/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:02,269 - LGCN2 model - [INFO]: Epoch 354/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:02,936 - LGCN2 model - [INFO]: Epoch 355/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:03,605 - LGCN2 model - [INFO]: Epoch 356/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:04,270 - LGCN2 model - [INFO]: Epoch 357/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:04,953 - LGCN2 model - [INFO]: Epoch 358/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:05,586 - LGCN2 model - [INFO]: Epoch 359/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:06,233 - LGCN2 model - [INFO]: Epoch 360/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:06,869 - LGCN2 model - [INFO]: Epoch 361/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:07,503 - LGCN2 model - [INFO]: Epoch 362/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:08,136 - LGCN2 model - [INFO]: Epoch 363/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:08,785 - LGCN2 model - [INFO]: Epoch 364/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:09,436 - LGCN2 model - [INFO]: Epoch 365/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:10,085 - LGCN2 model - [INFO]: Epoch 366/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:10,788 - LGCN2 model - [INFO]: Epoch 367/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:11,420 - LGCN2 model - [INFO]: Epoch 368/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:12,053 - LGCN2 model - [INFO]: Epoch 369/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:12,687 - LGCN2 model - [INFO]: Epoch 370/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:13,309 - LGCN2 model - [INFO]: Epoch 371/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:13,895 - LGCN2 model - [INFO]: Epoch 372/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:14,586 - LGCN2 model - [INFO]: Epoch 373/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:15,252 - LGCN2 model - [INFO]: Epoch 374/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:15,910 - LGCN2 model - [INFO]: Epoch 375/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:16,603 - LGCN2 model - [INFO]: Epoch 376/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:17,269 - LGCN2 model - [INFO]: Epoch 377/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:17,905 - LGCN2 model - [INFO]: Epoch 378/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:18,152 - LGCN2 model - [INFO]: Epoch 379/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:18,369 - LGCN2 model - [INFO]: Epoch 380/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:18,600 - LGCN2 model - [INFO]: Epoch 381/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:18,818 - LGCN2 model - [INFO]: Epoch 382/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:19,053 - LGCN2 model - [INFO]: Epoch 383/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:19,285 - LGCN2 model - [INFO]: Epoch 384/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:19,502 - LGCN2 model - [INFO]: Epoch 385/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:19,734 - LGCN2 model - [INFO]: Epoch 386/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:19,968 - LGCN2 model - [INFO]: Epoch 387/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:20,219 - LGCN2 model - [INFO]: Epoch 388/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:20,452 - LGCN2 model - [INFO]: Epoch 389/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:20,752 - LGCN2 model - [INFO]: Epoch 390/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:20,985 - LGCN2 model - [INFO]: Epoch 391/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:21,252 - LGCN2 model - [INFO]: Epoch 392/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:21,502 - LGCN2 model - [INFO]: Epoch 393/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:21,735 - LGCN2 model - [INFO]: Epoch 394/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:21,986 - LGCN2 model - [INFO]: Epoch 395/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:22,252 - LGCN2 model - [INFO]: Epoch 396/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:22,502 - LGCN2 model - [INFO]: Epoch 397/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:22,768 - LGCN2 model - [INFO]: Epoch 398/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:23,002 - LGCN2 model - [INFO]: Epoch 399/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:23,285 - LGCN2 model - [INFO]: Epoch 400/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:23,504 - LGCN2 model - [INFO]: Epoch 401/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:23,752 - LGCN2 model - [INFO]: Epoch 402/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:24,004 - LGCN2 model - [INFO]: Epoch 403/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:24,260 - LGCN2 model - [INFO]: Epoch 404/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:24,468 - LGCN2 model - [INFO]: Epoch 405/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:24,752 - LGCN2 model - [INFO]: Epoch 406/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:24,987 - LGCN2 model - [INFO]: Epoch 407/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:25,257 - LGCN2 model - [INFO]: Epoch 408/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:25,519 - LGCN2 model - [INFO]: Epoch 409/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:25,791 - LGCN2 model - [INFO]: Epoch 410/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:26,052 - LGCN2 model - [INFO]: Epoch 411/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:26,319 - LGCN2 model - [INFO]: Epoch 412/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:26,585 - LGCN2 model - [INFO]: Epoch 413/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:26,856 - LGCN2 model - [INFO]: Epoch 414/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:27,118 - LGCN2 model - [INFO]: Epoch 415/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:27,352 - LGCN2 model - [INFO]: Epoch 416/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:27,618 - LGCN2 model - [INFO]: Epoch 417/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:27,852 - LGCN2 model - [INFO]: Epoch 418/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:28,085 - LGCN2 model - [INFO]: Epoch 419/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:28,348 - LGCN2 model - [INFO]: Epoch 420/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:28,585 - LGCN2 model - [INFO]: Epoch 421/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:28,819 - LGCN2 model - [INFO]: Epoch 422/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:29,090 - LGCN2 model - [INFO]: Epoch 423/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:29,349 - LGCN2 model - [INFO]: Epoch 424/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:29,611 - LGCN2 model - [INFO]: Epoch 425/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:29,852 - LGCN2 model - [INFO]: Epoch 426/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:30,138 - LGCN2 model - [INFO]: Epoch 427/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:30,404 - LGCN2 model - [INFO]: Epoch 428/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:30,669 - LGCN2 model - [INFO]: Epoch 429/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:30,934 - LGCN2 model - [INFO]: Epoch 430/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:31,205 - LGCN2 model - [INFO]: Epoch 431/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:31,472 - LGCN2 model - [INFO]: Epoch 432/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:31,735 - LGCN2 model - [INFO]: Epoch 433/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:32,002 - LGCN2 model - [INFO]: Epoch 434/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:32,235 - LGCN2 model - [INFO]: Epoch 435/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:32,502 - LGCN2 model - [INFO]: Epoch 436/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:32,756 - LGCN2 model - [INFO]: Epoch 437/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:33,018 - LGCN2 model - [INFO]: Epoch 438/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:33,285 - LGCN2 model - [INFO]: Epoch 439/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:33,518 - LGCN2 model - [INFO]: Epoch 440/1000, BPR Loss: 5.1658\n",
      "2024-11-20 17:46:33,785 - LGCN2 model - [INFO]: Epoch 441/1000, BPR Loss: 5.1659\n",
      "2024-11-20 17:46:34,018 - LGCN2 model - [INFO]: Epoch 442/1000, BPR Loss: 5.1659\n",
      "2024-11-20 17:46:34,268 - LGCN2 model - [INFO]: Epoch 443/1000, BPR Loss: 5.1659\n",
      "2024-11-20 17:46:34,518 - LGCN2 model - [INFO]: Epoch 444/1000, BPR Loss: 5.1659\n",
      "2024-11-20 17:46:34,768 - LGCN2 model - [INFO]: Epoch 445/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:46:35,002 - LGCN2 model - [INFO]: Epoch 446/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:46:35,258 - LGCN2 model - [INFO]: Epoch 447/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:46:35,502 - LGCN2 model - [INFO]: Epoch 448/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:46:35,768 - LGCN2 model - [INFO]: Epoch 449/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:46:36,058 - LGCN2 model - [INFO]: Epoch 450/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:46:36,318 - LGCN2 model - [INFO]: Epoch 451/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:46:36,552 - LGCN2 model - [INFO]: Epoch 452/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:46:36,788 - LGCN2 model - [INFO]: Epoch 453/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:46:37,067 - LGCN2 model - [INFO]: Epoch 454/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:46:37,303 - LGCN2 model - [INFO]: Epoch 455/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:46:37,568 - LGCN2 model - [INFO]: Epoch 456/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:46:37,835 - LGCN2 model - [INFO]: Epoch 457/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:46:38,106 - LGCN2 model - [INFO]: Epoch 458/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:46:38,388 - LGCN2 model - [INFO]: Epoch 459/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:46:38,635 - LGCN2 model - [INFO]: Epoch 460/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:46:38,885 - LGCN2 model - [INFO]: Epoch 461/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:46:39,162 - LGCN2 model - [INFO]: Epoch 462/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:46:39,445 - LGCN2 model - [INFO]: Epoch 463/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:46:39,742 - LGCN2 model - [INFO]: Epoch 464/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:46:40,035 - LGCN2 model - [INFO]: Epoch 465/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:46:40,336 - LGCN2 model - [INFO]: Epoch 466/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:46:40,615 - LGCN2 model - [INFO]: Epoch 467/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:46:40,887 - LGCN2 model - [INFO]: Epoch 468/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:46:41,236 - LGCN2 model - [INFO]: Epoch 469/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:46:41,537 - LGCN2 model - [INFO]: Epoch 470/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:46:41,805 - LGCN2 model - [INFO]: Epoch 471/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:46:42,102 - LGCN2 model - [INFO]: Epoch 472/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:46:42,424 - LGCN2 model - [INFO]: Epoch 473/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:46:42,718 - LGCN2 model - [INFO]: Epoch 474/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:46:43,002 - LGCN2 model - [INFO]: Epoch 475/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:46:43,302 - LGCN2 model - [INFO]: Epoch 476/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:46:43,629 - LGCN2 model - [INFO]: Epoch 477/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:46:43,921 - LGCN2 model - [INFO]: Epoch 478/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:46:44,204 - LGCN2 model - [INFO]: Epoch 479/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:46:44,505 - LGCN2 model - [INFO]: Epoch 480/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:46:44,840 - LGCN2 model - [INFO]: Epoch 481/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:46:45,135 - LGCN2 model - [INFO]: Epoch 482/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:46:45,420 - LGCN2 model - [INFO]: Epoch 483/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:46:45,721 - LGCN2 model - [INFO]: Epoch 484/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:46:46,038 - LGCN2 model - [INFO]: Epoch 485/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:46:46,340 - LGCN2 model - [INFO]: Epoch 486/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:46:46,616 - LGCN2 model - [INFO]: Epoch 487/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:46:46,918 - LGCN2 model - [INFO]: Epoch 488/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:46:47,235 - LGCN2 model - [INFO]: Epoch 489/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:46:47,520 - LGCN2 model - [INFO]: Epoch 490/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:46:47,804 - LGCN2 model - [INFO]: Epoch 491/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:46:48,102 - LGCN2 model - [INFO]: Epoch 492/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:46:48,417 - LGCN2 model - [INFO]: Epoch 493/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:46:48,752 - LGCN2 model - [INFO]: Epoch 494/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:46:49,068 - LGCN2 model - [INFO]: Epoch 495/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:46:49,352 - LGCN2 model - [INFO]: Epoch 496/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:46:49,633 - LGCN2 model - [INFO]: Epoch 497/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:46:49,885 - LGCN2 model - [INFO]: Epoch 498/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:46:50,168 - LGCN2 model - [INFO]: Epoch 499/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:46:50,454 - LGCN2 model - [INFO]: Epoch 500/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:46:50,762 - LGCN2 model - [INFO]: Epoch 501/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:46:51,102 - LGCN2 model - [INFO]: Epoch 502/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:46:51,418 - LGCN2 model - [INFO]: Epoch 503/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:46:51,735 - LGCN2 model - [INFO]: Epoch 504/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:46:52,018 - LGCN2 model - [INFO]: Epoch 505/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:46:52,301 - LGCN2 model - [INFO]: Epoch 506/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:46:52,588 - LGCN2 model - [INFO]: Epoch 507/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:46:52,887 - LGCN2 model - [INFO]: Epoch 508/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:46:53,152 - LGCN2 model - [INFO]: Epoch 509/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:46:53,434 - LGCN2 model - [INFO]: Epoch 510/1000, BPR Loss: 5.1659\n",
      "2024-11-20 17:46:53,704 - LGCN2 model - [INFO]: Epoch 511/1000, BPR Loss: 5.1659\n",
      "2024-11-20 17:46:54,002 - LGCN2 model - [INFO]: Epoch 512/1000, BPR Loss: 5.1659\n",
      "2024-11-20 17:46:54,335 - LGCN2 model - [INFO]: Epoch 513/1000, BPR Loss: 5.1659\n",
      "2024-11-20 17:46:54,685 - LGCN2 model - [INFO]: Epoch 514/1000, BPR Loss: 5.1659\n",
      "2024-11-20 17:46:55,002 - LGCN2 model - [INFO]: Epoch 515/1000, BPR Loss: 5.1659\n",
      "2024-11-20 17:46:55,302 - LGCN2 model - [INFO]: Epoch 516/1000, BPR Loss: 5.1659\n",
      "2024-11-20 17:46:55,570 - LGCN2 model - [INFO]: Epoch 517/1000, BPR Loss: 5.1659\n",
      "2024-11-20 17:46:55,840 - LGCN2 model - [INFO]: Epoch 518/1000, BPR Loss: 5.1659\n",
      "2024-11-20 17:46:56,111 - LGCN2 model - [INFO]: Epoch 519/1000, BPR Loss: 5.1659\n",
      "2024-11-20 17:46:56,387 - LGCN2 model - [INFO]: Epoch 520/1000, BPR Loss: 5.1659\n",
      "2024-11-20 17:46:56,718 - LGCN2 model - [INFO]: Epoch 521/1000, BPR Loss: 5.1659\n",
      "2024-11-20 17:46:57,002 - LGCN2 model - [INFO]: Epoch 522/1000, BPR Loss: 5.1659\n",
      "2024-11-20 17:46:57,302 - LGCN2 model - [INFO]: Epoch 523/1000, BPR Loss: 5.1659\n",
      "2024-11-20 17:46:57,606 - LGCN2 model - [INFO]: Epoch 524/1000, BPR Loss: 5.1659\n",
      "2024-11-20 17:46:57,935 - LGCN2 model - [INFO]: Epoch 525/1000, BPR Loss: 5.1659\n",
      "2024-11-20 17:46:58,221 - LGCN2 model - [INFO]: Epoch 526/1000, BPR Loss: 5.1659\n",
      "2024-11-20 17:46:58,568 - LGCN2 model - [INFO]: Epoch 527/1000, BPR Loss: 5.1659\n",
      "2024-11-20 17:46:58,886 - LGCN2 model - [INFO]: Epoch 528/1000, BPR Loss: 5.1659\n",
      "2024-11-20 17:46:59,202 - LGCN2 model - [INFO]: Epoch 529/1000, BPR Loss: 5.1659\n",
      "2024-11-20 17:46:59,503 - LGCN2 model - [INFO]: Epoch 530/1000, BPR Loss: 5.1659\n",
      "2024-11-20 17:46:59,785 - LGCN2 model - [INFO]: Epoch 531/1000, BPR Loss: 5.1659\n",
      "2024-11-20 17:47:00,052 - LGCN2 model - [INFO]: Epoch 532/1000, BPR Loss: 5.1659\n",
      "2024-11-20 17:47:00,320 - LGCN2 model - [INFO]: Epoch 533/1000, BPR Loss: 5.1659\n",
      "2024-11-20 17:47:00,621 - LGCN2 model - [INFO]: Epoch 534/1000, BPR Loss: 5.1659\n",
      "2024-11-20 17:47:00,868 - LGCN2 model - [INFO]: Epoch 535/1000, BPR Loss: 5.1659\n",
      "2024-11-20 17:47:01,168 - LGCN2 model - [INFO]: Epoch 536/1000, BPR Loss: 5.1659\n",
      "2024-11-20 17:47:01,486 - LGCN2 model - [INFO]: Epoch 537/1000, BPR Loss: 5.1659\n",
      "2024-11-20 17:47:01,751 - LGCN2 model - [INFO]: Epoch 538/1000, BPR Loss: 5.1659\n",
      "2024-11-20 17:47:01,986 - LGCN2 model - [INFO]: Epoch 539/1000, BPR Loss: 5.1659\n",
      "2024-11-20 17:47:02,251 - LGCN2 model - [INFO]: Epoch 540/1000, BPR Loss: 5.1659\n",
      "2024-11-20 17:47:02,487 - LGCN2 model - [INFO]: Epoch 541/1000, BPR Loss: 5.1659\n",
      "2024-11-20 17:47:02,735 - LGCN2 model - [INFO]: Epoch 542/1000, BPR Loss: 5.1659\n",
      "2024-11-20 17:47:02,989 - LGCN2 model - [INFO]: Epoch 543/1000, BPR Loss: 5.1659\n",
      "2024-11-20 17:47:03,254 - LGCN2 model - [INFO]: Epoch 544/1000, BPR Loss: 5.1659\n",
      "2024-11-20 17:47:03,535 - LGCN2 model - [INFO]: Epoch 545/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:03,802 - LGCN2 model - [INFO]: Epoch 546/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:04,071 - LGCN2 model - [INFO]: Epoch 547/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:04,318 - LGCN2 model - [INFO]: Epoch 548/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:04,570 - LGCN2 model - [INFO]: Epoch 549/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:04,852 - LGCN2 model - [INFO]: Epoch 550/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:05,102 - LGCN2 model - [INFO]: Epoch 551/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:05,368 - LGCN2 model - [INFO]: Epoch 552/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:05,636 - LGCN2 model - [INFO]: Epoch 553/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:05,919 - LGCN2 model - [INFO]: Epoch 554/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:06,187 - LGCN2 model - [INFO]: Epoch 555/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:06,451 - LGCN2 model - [INFO]: Epoch 556/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:06,739 - LGCN2 model - [INFO]: Epoch 557/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:07,001 - LGCN2 model - [INFO]: Epoch 558/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:07,285 - LGCN2 model - [INFO]: Epoch 559/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:07,618 - LGCN2 model - [INFO]: Epoch 560/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:07,968 - LGCN2 model - [INFO]: Epoch 561/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:08,318 - LGCN2 model - [INFO]: Epoch 562/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:08,703 - LGCN2 model - [INFO]: Epoch 563/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:09,056 - LGCN2 model - [INFO]: Epoch 564/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:09,395 - LGCN2 model - [INFO]: Epoch 565/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:09,721 - LGCN2 model - [INFO]: Epoch 566/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:10,085 - LGCN2 model - [INFO]: Epoch 567/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:10,418 - LGCN2 model - [INFO]: Epoch 568/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:10,768 - LGCN2 model - [INFO]: Epoch 569/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:11,152 - LGCN2 model - [INFO]: Epoch 570/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:11,568 - LGCN2 model - [INFO]: Epoch 571/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:11,888 - LGCN2 model - [INFO]: Epoch 572/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:12,204 - LGCN2 model - [INFO]: Epoch 573/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:12,537 - LGCN2 model - [INFO]: Epoch 574/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:12,902 - LGCN2 model - [INFO]: Epoch 575/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:13,268 - LGCN2 model - [INFO]: Epoch 576/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:13,635 - LGCN2 model - [INFO]: Epoch 577/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:13,985 - LGCN2 model - [INFO]: Epoch 578/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:14,335 - LGCN2 model - [INFO]: Epoch 579/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:14,635 - LGCN2 model - [INFO]: Epoch 580/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:14,935 - LGCN2 model - [INFO]: Epoch 581/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:15,268 - LGCN2 model - [INFO]: Epoch 582/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:15,601 - LGCN2 model - [INFO]: Epoch 583/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:15,951 - LGCN2 model - [INFO]: Epoch 584/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:16,285 - LGCN2 model - [INFO]: Epoch 585/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:16,588 - LGCN2 model - [INFO]: Epoch 586/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:16,854 - LGCN2 model - [INFO]: Epoch 587/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:17,122 - LGCN2 model - [INFO]: Epoch 588/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:17,437 - LGCN2 model - [INFO]: Epoch 589/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:17,706 - LGCN2 model - [INFO]: Epoch 590/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:17,968 - LGCN2 model - [INFO]: Epoch 591/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:18,249 - LGCN2 model - [INFO]: Epoch 592/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:18,507 - LGCN2 model - [INFO]: Epoch 593/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:18,785 - LGCN2 model - [INFO]: Epoch 594/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:19,051 - LGCN2 model - [INFO]: Epoch 595/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:19,321 - LGCN2 model - [INFO]: Epoch 596/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:19,589 - LGCN2 model - [INFO]: Epoch 597/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:19,887 - LGCN2 model - [INFO]: Epoch 598/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:20,200 - LGCN2 model - [INFO]: Epoch 599/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:20,474 - LGCN2 model - [INFO]: Epoch 600/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:20,783 - LGCN2 model - [INFO]: Epoch 601/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:21,068 - LGCN2 model - [INFO]: Epoch 602/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:21,351 - LGCN2 model - [INFO]: Epoch 603/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:21,618 - LGCN2 model - [INFO]: Epoch 604/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:21,884 - LGCN2 model - [INFO]: Epoch 605/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:22,168 - LGCN2 model - [INFO]: Epoch 606/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:22,435 - LGCN2 model - [INFO]: Epoch 607/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:22,711 - LGCN2 model - [INFO]: Epoch 608/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:22,969 - LGCN2 model - [INFO]: Epoch 609/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:23,249 - LGCN2 model - [INFO]: Epoch 610/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:23,519 - LGCN2 model - [INFO]: Epoch 611/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:23,785 - LGCN2 model - [INFO]: Epoch 612/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:24,068 - LGCN2 model - [INFO]: Epoch 613/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:24,358 - LGCN2 model - [INFO]: Epoch 614/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:24,634 - LGCN2 model - [INFO]: Epoch 615/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:24,913 - LGCN2 model - [INFO]: Epoch 616/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:25,185 - LGCN2 model - [INFO]: Epoch 617/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:25,434 - LGCN2 model - [INFO]: Epoch 618/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:25,701 - LGCN2 model - [INFO]: Epoch 619/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:25,970 - LGCN2 model - [INFO]: Epoch 620/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:26,251 - LGCN2 model - [INFO]: Epoch 621/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:26,534 - LGCN2 model - [INFO]: Epoch 622/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:26,788 - LGCN2 model - [INFO]: Epoch 623/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:27,051 - LGCN2 model - [INFO]: Epoch 624/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:27,321 - LGCN2 model - [INFO]: Epoch 625/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:27,569 - LGCN2 model - [INFO]: Epoch 626/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:27,837 - LGCN2 model - [INFO]: Epoch 627/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:28,104 - LGCN2 model - [INFO]: Epoch 628/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:28,371 - LGCN2 model - [INFO]: Epoch 629/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:28,668 - LGCN2 model - [INFO]: Epoch 630/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:28,942 - LGCN2 model - [INFO]: Epoch 631/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:29,201 - LGCN2 model - [INFO]: Epoch 632/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:29,485 - LGCN2 model - [INFO]: Epoch 633/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:29,751 - LGCN2 model - [INFO]: Epoch 634/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:30,018 - LGCN2 model - [INFO]: Epoch 635/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:30,287 - LGCN2 model - [INFO]: Epoch 636/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:30,551 - LGCN2 model - [INFO]: Epoch 637/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:30,822 - LGCN2 model - [INFO]: Epoch 638/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:31,087 - LGCN2 model - [INFO]: Epoch 639/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:31,334 - LGCN2 model - [INFO]: Epoch 640/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:31,601 - LGCN2 model - [INFO]: Epoch 641/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:31,884 - LGCN2 model - [INFO]: Epoch 642/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:32,168 - LGCN2 model - [INFO]: Epoch 643/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:32,451 - LGCN2 model - [INFO]: Epoch 644/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:32,824 - LGCN2 model - [INFO]: Epoch 645/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:33,171 - LGCN2 model - [INFO]: Epoch 646/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:33,502 - LGCN2 model - [INFO]: Epoch 647/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:33,889 - LGCN2 model - [INFO]: Epoch 648/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:34,217 - LGCN2 model - [INFO]: Epoch 649/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:34,571 - LGCN2 model - [INFO]: Epoch 650/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:34,922 - LGCN2 model - [INFO]: Epoch 651/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:35,255 - LGCN2 model - [INFO]: Epoch 652/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:35,587 - LGCN2 model - [INFO]: Epoch 653/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:35,952 - LGCN2 model - [INFO]: Epoch 654/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:36,351 - LGCN2 model - [INFO]: Epoch 655/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:36,668 - LGCN2 model - [INFO]: Epoch 656/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:36,995 - LGCN2 model - [INFO]: Epoch 657/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:37,303 - LGCN2 model - [INFO]: Epoch 658/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:37,619 - LGCN2 model - [INFO]: Epoch 659/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:37,935 - LGCN2 model - [INFO]: Epoch 660/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:38,361 - LGCN2 model - [INFO]: Epoch 661/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:38,857 - LGCN2 model - [INFO]: Epoch 662/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:39,345 - LGCN2 model - [INFO]: Epoch 663/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:39,841 - LGCN2 model - [INFO]: Epoch 664/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:40,358 - LGCN2 model - [INFO]: Epoch 665/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:40,757 - LGCN2 model - [INFO]: Epoch 666/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:41,237 - LGCN2 model - [INFO]: Epoch 667/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:41,848 - LGCN2 model - [INFO]: Epoch 668/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:42,438 - LGCN2 model - [INFO]: Epoch 669/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:43,000 - LGCN2 model - [INFO]: Epoch 670/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:43,563 - LGCN2 model - [INFO]: Epoch 671/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:44,163 - LGCN2 model - [INFO]: Epoch 672/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:44,785 - LGCN2 model - [INFO]: Epoch 673/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:45,442 - LGCN2 model - [INFO]: Epoch 674/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:46,018 - LGCN2 model - [INFO]: Epoch 675/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:46,579 - LGCN2 model - [INFO]: Epoch 676/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:47,219 - LGCN2 model - [INFO]: Epoch 677/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:47,887 - LGCN2 model - [INFO]: Epoch 678/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:48,486 - LGCN2 model - [INFO]: Epoch 679/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:49,164 - LGCN2 model - [INFO]: Epoch 680/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:49,803 - LGCN2 model - [INFO]: Epoch 681/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:50,402 - LGCN2 model - [INFO]: Epoch 682/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:51,001 - LGCN2 model - [INFO]: Epoch 683/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:51,603 - LGCN2 model - [INFO]: Epoch 684/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:52,199 - LGCN2 model - [INFO]: Epoch 685/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:52,818 - LGCN2 model - [INFO]: Epoch 686/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:53,470 - LGCN2 model - [INFO]: Epoch 687/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:54,069 - LGCN2 model - [INFO]: Epoch 688/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:54,719 - LGCN2 model - [INFO]: Epoch 689/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:55,317 - LGCN2 model - [INFO]: Epoch 690/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:55,895 - LGCN2 model - [INFO]: Epoch 691/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:56,460 - LGCN2 model - [INFO]: Epoch 692/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:57,096 - LGCN2 model - [INFO]: Epoch 693/1000, BPR Loss: 5.1660\n",
      "2024-11-20 17:47:57,812 - LGCN2 model - [INFO]: Epoch 694/1000, BPR Loss: 5.1659\n",
      "2024-11-20 17:47:58,507 - LGCN2 model - [INFO]: Epoch 695/1000, BPR Loss: 5.1708\n",
      "2024-11-20 17:47:59,115 - LGCN2 model - [INFO]: Epoch 696/1000, BPR Loss: 5.1714\n",
      "2024-11-20 17:47:59,806 - LGCN2 model - [INFO]: Epoch 697/1000, BPR Loss: 5.1727\n",
      "2024-11-20 17:48:00,494 - LGCN2 model - [INFO]: Epoch 698/1000, BPR Loss: 5.1725\n",
      "2024-11-20 17:48:01,085 - LGCN2 model - [INFO]: Epoch 699/1000, BPR Loss: 5.1721\n",
      "2024-11-20 17:48:01,642 - LGCN2 model - [INFO]: Epoch 700/1000, BPR Loss: 5.1709\n",
      "2024-11-20 17:48:02,229 - LGCN2 model - [INFO]: Epoch 701/1000, BPR Loss: 5.1708\n",
      "2024-11-20 17:48:02,856 - LGCN2 model - [INFO]: Epoch 702/1000, BPR Loss: 5.1709\n",
      "2024-11-20 17:48:03,428 - LGCN2 model - [INFO]: Epoch 703/1000, BPR Loss: 5.1706\n",
      "2024-11-20 17:48:04,022 - LGCN2 model - [INFO]: Epoch 704/1000, BPR Loss: 5.1701\n",
      "2024-11-20 17:48:04,626 - LGCN2 model - [INFO]: Epoch 705/1000, BPR Loss: 5.1699\n",
      "2024-11-20 17:48:05,219 - LGCN2 model - [INFO]: Epoch 706/1000, BPR Loss: 5.1702\n",
      "2024-11-20 17:48:05,862 - LGCN2 model - [INFO]: Epoch 707/1000, BPR Loss: 5.1700\n",
      "2024-11-20 17:48:06,543 - LGCN2 model - [INFO]: Epoch 708/1000, BPR Loss: 5.1699\n",
      "2024-11-20 17:48:07,235 - LGCN2 model - [INFO]: Epoch 709/1000, BPR Loss: 5.1696\n",
      "2024-11-20 17:48:07,900 - LGCN2 model - [INFO]: Epoch 710/1000, BPR Loss: 5.1696\n",
      "2024-11-20 17:48:08,523 - LGCN2 model - [INFO]: Epoch 711/1000, BPR Loss: 5.1698\n",
      "2024-11-20 17:48:09,127 - LGCN2 model - [INFO]: Epoch 712/1000, BPR Loss: 5.1695\n",
      "2024-11-20 17:48:09,724 - LGCN2 model - [INFO]: Epoch 713/1000, BPR Loss: 5.1693\n",
      "2024-11-20 17:48:10,301 - LGCN2 model - [INFO]: Epoch 714/1000, BPR Loss: 5.1693\n",
      "2024-11-20 17:48:10,881 - LGCN2 model - [INFO]: Epoch 715/1000, BPR Loss: 5.1692\n",
      "2024-11-20 17:48:11,509 - LGCN2 model - [INFO]: Epoch 716/1000, BPR Loss: 5.1692\n",
      "2024-11-20 17:48:12,153 - LGCN2 model - [INFO]: Epoch 717/1000, BPR Loss: 5.1692\n",
      "2024-11-20 17:48:12,780 - LGCN2 model - [INFO]: Epoch 718/1000, BPR Loss: 5.1693\n",
      "2024-11-20 17:48:13,362 - LGCN2 model - [INFO]: Epoch 719/1000, BPR Loss: 5.1693\n",
      "2024-11-20 17:48:14,048 - LGCN2 model - [INFO]: Epoch 720/1000, BPR Loss: 5.1692\n",
      "2024-11-20 17:48:14,668 - LGCN2 model - [INFO]: Epoch 721/1000, BPR Loss: 5.1691\n",
      "2024-11-20 17:48:15,351 - LGCN2 model - [INFO]: Epoch 722/1000, BPR Loss: 5.1689\n",
      "2024-11-20 17:48:16,017 - LGCN2 model - [INFO]: Epoch 723/1000, BPR Loss: 5.1688\n",
      "2024-11-20 17:48:16,652 - LGCN2 model - [INFO]: Epoch 724/1000, BPR Loss: 5.1688\n",
      "2024-11-20 17:48:17,367 - LGCN2 model - [INFO]: Epoch 725/1000, BPR Loss: 5.1687\n",
      "2024-11-20 17:48:18,002 - LGCN2 model - [INFO]: Epoch 726/1000, BPR Loss: 5.1686\n",
      "2024-11-20 17:48:18,637 - LGCN2 model - [INFO]: Epoch 727/1000, BPR Loss: 5.1685\n",
      "2024-11-20 17:48:19,385 - LGCN2 model - [INFO]: Epoch 728/1000, BPR Loss: 5.1685\n",
      "2024-11-20 17:48:20,034 - LGCN2 model - [INFO]: Epoch 729/1000, BPR Loss: 5.1685\n",
      "2024-11-20 17:48:20,701 - LGCN2 model - [INFO]: Epoch 730/1000, BPR Loss: 5.1685\n",
      "2024-11-20 17:48:21,385 - LGCN2 model - [INFO]: Epoch 731/1000, BPR Loss: 5.1685\n",
      "2024-11-20 17:48:22,034 - LGCN2 model - [INFO]: Epoch 732/1000, BPR Loss: 5.1685\n",
      "2024-11-20 17:48:22,701 - LGCN2 model - [INFO]: Epoch 733/1000, BPR Loss: 5.1684\n",
      "2024-11-20 17:48:23,418 - LGCN2 model - [INFO]: Epoch 734/1000, BPR Loss: 5.1684\n",
      "2024-11-20 17:48:24,050 - LGCN2 model - [INFO]: Epoch 735/1000, BPR Loss: 5.1683\n",
      "2024-11-20 17:48:24,734 - LGCN2 model - [INFO]: Epoch 736/1000, BPR Loss: 5.1683\n",
      "2024-11-20 17:48:25,401 - LGCN2 model - [INFO]: Epoch 737/1000, BPR Loss: 5.1683\n",
      "2024-11-20 17:48:26,068 - LGCN2 model - [INFO]: Epoch 738/1000, BPR Loss: 5.1682\n",
      "2024-11-20 17:48:26,767 - LGCN2 model - [INFO]: Epoch 739/1000, BPR Loss: 5.1682\n",
      "2024-11-20 17:48:27,468 - LGCN2 model - [INFO]: Epoch 740/1000, BPR Loss: 5.1682\n",
      "2024-11-20 17:48:28,118 - LGCN2 model - [INFO]: Epoch 741/1000, BPR Loss: 5.1682\n",
      "2024-11-20 17:48:28,817 - LGCN2 model - [INFO]: Epoch 742/1000, BPR Loss: 5.1682\n",
      "2024-11-20 17:48:29,518 - LGCN2 model - [INFO]: Epoch 743/1000, BPR Loss: 5.1682\n",
      "2024-11-20 17:48:30,133 - LGCN2 model - [INFO]: Epoch 744/1000, BPR Loss: 5.1682\n",
      "2024-11-20 17:48:30,817 - LGCN2 model - [INFO]: Epoch 745/1000, BPR Loss: 5.1682\n",
      "2024-11-20 17:48:31,434 - LGCN2 model - [INFO]: Epoch 746/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:48:32,068 - LGCN2 model - [INFO]: Epoch 747/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:48:32,717 - LGCN2 model - [INFO]: Epoch 748/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:48:33,335 - LGCN2 model - [INFO]: Epoch 749/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:48:33,934 - LGCN2 model - [INFO]: Epoch 750/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:48:34,560 - LGCN2 model - [INFO]: Epoch 751/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:48:35,202 - LGCN2 model - [INFO]: Epoch 752/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:48:35,834 - LGCN2 model - [INFO]: Epoch 753/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:48:36,485 - LGCN2 model - [INFO]: Epoch 754/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:48:37,150 - LGCN2 model - [INFO]: Epoch 755/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:48:37,767 - LGCN2 model - [INFO]: Epoch 756/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:48:38,418 - LGCN2 model - [INFO]: Epoch 757/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:48:39,050 - LGCN2 model - [INFO]: Epoch 758/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:48:39,684 - LGCN2 model - [INFO]: Epoch 759/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:48:40,300 - LGCN2 model - [INFO]: Epoch 760/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:48:40,916 - LGCN2 model - [INFO]: Epoch 761/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:48:41,568 - LGCN2 model - [INFO]: Epoch 762/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:48:42,218 - LGCN2 model - [INFO]: Epoch 763/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:48:42,850 - LGCN2 model - [INFO]: Epoch 764/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:48:43,467 - LGCN2 model - [INFO]: Epoch 765/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:48:44,118 - LGCN2 model - [INFO]: Epoch 766/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:48:44,766 - LGCN2 model - [INFO]: Epoch 767/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:48:45,367 - LGCN2 model - [INFO]: Epoch 768/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:48:46,051 - LGCN2 model - [INFO]: Epoch 769/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:48:46,766 - LGCN2 model - [INFO]: Epoch 770/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:48:47,386 - LGCN2 model - [INFO]: Epoch 771/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:48:48,034 - LGCN2 model - [INFO]: Epoch 772/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:48:48,667 - LGCN2 model - [INFO]: Epoch 773/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:48:49,284 - LGCN2 model - [INFO]: Epoch 774/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:48:49,917 - LGCN2 model - [INFO]: Epoch 775/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:48:50,635 - LGCN2 model - [INFO]: Epoch 776/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:48:51,317 - LGCN2 model - [INFO]: Epoch 777/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:48:51,934 - LGCN2 model - [INFO]: Epoch 778/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:48:52,588 - LGCN2 model - [INFO]: Epoch 779/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:48:53,184 - LGCN2 model - [INFO]: Epoch 780/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:48:53,800 - LGCN2 model - [INFO]: Epoch 781/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:48:54,400 - LGCN2 model - [INFO]: Epoch 782/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:48:55,018 - LGCN2 model - [INFO]: Epoch 783/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:48:55,718 - LGCN2 model - [INFO]: Epoch 784/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:48:56,401 - LGCN2 model - [INFO]: Epoch 785/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:48:57,034 - LGCN2 model - [INFO]: Epoch 786/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:48:57,701 - LGCN2 model - [INFO]: Epoch 787/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:48:58,351 - LGCN2 model - [INFO]: Epoch 788/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:48:58,984 - LGCN2 model - [INFO]: Epoch 789/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:48:59,599 - LGCN2 model - [INFO]: Epoch 790/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:00,235 - LGCN2 model - [INFO]: Epoch 791/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:00,917 - LGCN2 model - [INFO]: Epoch 792/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:01,550 - LGCN2 model - [INFO]: Epoch 793/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:02,201 - LGCN2 model - [INFO]: Epoch 794/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:02,833 - LGCN2 model - [INFO]: Epoch 795/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:03,517 - LGCN2 model - [INFO]: Epoch 796/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:04,151 - LGCN2 model - [INFO]: Epoch 797/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:04,784 - LGCN2 model - [INFO]: Epoch 798/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:05,401 - LGCN2 model - [INFO]: Epoch 799/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:06,034 - LGCN2 model - [INFO]: Epoch 800/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:06,625 - LGCN2 model - [INFO]: Epoch 801/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:07,267 - LGCN2 model - [INFO]: Epoch 802/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:07,884 - LGCN2 model - [INFO]: Epoch 803/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:08,473 - LGCN2 model - [INFO]: Epoch 804/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:09,149 - LGCN2 model - [INFO]: Epoch 805/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:09,751 - LGCN2 model - [INFO]: Epoch 806/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:10,350 - LGCN2 model - [INFO]: Epoch 807/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:10,950 - LGCN2 model - [INFO]: Epoch 808/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:11,652 - LGCN2 model - [INFO]: Epoch 809/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:12,283 - LGCN2 model - [INFO]: Epoch 810/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:12,935 - LGCN2 model - [INFO]: Epoch 811/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:13,550 - LGCN2 model - [INFO]: Epoch 812/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:14,167 - LGCN2 model - [INFO]: Epoch 813/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:14,783 - LGCN2 model - [INFO]: Epoch 814/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:15,417 - LGCN2 model - [INFO]: Epoch 815/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:16,041 - LGCN2 model - [INFO]: Epoch 816/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:16,684 - LGCN2 model - [INFO]: Epoch 817/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:17,343 - LGCN2 model - [INFO]: Epoch 818/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:17,984 - LGCN2 model - [INFO]: Epoch 819/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:18,617 - LGCN2 model - [INFO]: Epoch 820/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:19,250 - LGCN2 model - [INFO]: Epoch 821/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:19,901 - LGCN2 model - [INFO]: Epoch 822/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:20,584 - LGCN2 model - [INFO]: Epoch 823/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:21,217 - LGCN2 model - [INFO]: Epoch 824/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:21,883 - LGCN2 model - [INFO]: Epoch 825/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:22,523 - LGCN2 model - [INFO]: Epoch 826/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:23,167 - LGCN2 model - [INFO]: Epoch 827/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:23,800 - LGCN2 model - [INFO]: Epoch 828/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:24,467 - LGCN2 model - [INFO]: Epoch 829/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:25,133 - LGCN2 model - [INFO]: Epoch 830/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:25,783 - LGCN2 model - [INFO]: Epoch 831/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:26,408 - LGCN2 model - [INFO]: Epoch 832/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:27,040 - LGCN2 model - [INFO]: Epoch 833/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:27,662 - LGCN2 model - [INFO]: Epoch 834/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:28,282 - LGCN2 model - [INFO]: Epoch 835/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:28,866 - LGCN2 model - [INFO]: Epoch 836/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:29,484 - LGCN2 model - [INFO]: Epoch 837/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:30,101 - LGCN2 model - [INFO]: Epoch 838/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:30,725 - LGCN2 model - [INFO]: Epoch 839/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:31,341 - LGCN2 model - [INFO]: Epoch 840/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:31,935 - LGCN2 model - [INFO]: Epoch 841/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:32,534 - LGCN2 model - [INFO]: Epoch 842/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:33,189 - LGCN2 model - [INFO]: Epoch 843/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:33,817 - LGCN2 model - [INFO]: Epoch 844/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:34,433 - LGCN2 model - [INFO]: Epoch 845/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:35,034 - LGCN2 model - [INFO]: Epoch 846/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:35,666 - LGCN2 model - [INFO]: Epoch 847/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:36,284 - LGCN2 model - [INFO]: Epoch 848/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:36,900 - LGCN2 model - [INFO]: Epoch 849/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:37,538 - LGCN2 model - [INFO]: Epoch 850/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:38,192 - LGCN2 model - [INFO]: Epoch 851/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:38,766 - LGCN2 model - [INFO]: Epoch 852/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:39,400 - LGCN2 model - [INFO]: Epoch 853/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:40,017 - LGCN2 model - [INFO]: Epoch 854/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:40,634 - LGCN2 model - [INFO]: Epoch 855/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:41,250 - LGCN2 model - [INFO]: Epoch 856/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:41,867 - LGCN2 model - [INFO]: Epoch 857/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:42,566 - LGCN2 model - [INFO]: Epoch 858/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:43,267 - LGCN2 model - [INFO]: Epoch 859/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:43,916 - LGCN2 model - [INFO]: Epoch 860/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:44,533 - LGCN2 model - [INFO]: Epoch 861/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:45,217 - LGCN2 model - [INFO]: Epoch 862/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:45,851 - LGCN2 model - [INFO]: Epoch 863/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:46,517 - LGCN2 model - [INFO]: Epoch 864/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:47,134 - LGCN2 model - [INFO]: Epoch 865/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:47,783 - LGCN2 model - [INFO]: Epoch 866/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:48,418 - LGCN2 model - [INFO]: Epoch 867/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:49,115 - LGCN2 model - [INFO]: Epoch 868/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:49,733 - LGCN2 model - [INFO]: Epoch 869/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:50,399 - LGCN2 model - [INFO]: Epoch 870/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:51,017 - LGCN2 model - [INFO]: Epoch 871/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:51,651 - LGCN2 model - [INFO]: Epoch 872/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:52,283 - LGCN2 model - [INFO]: Epoch 873/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:52,967 - LGCN2 model - [INFO]: Epoch 874/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:53,568 - LGCN2 model - [INFO]: Epoch 875/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:54,185 - LGCN2 model - [INFO]: Epoch 876/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:54,817 - LGCN2 model - [INFO]: Epoch 877/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:55,434 - LGCN2 model - [INFO]: Epoch 878/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:56,158 - LGCN2 model - [INFO]: Epoch 879/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:56,800 - LGCN2 model - [INFO]: Epoch 880/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:57,416 - LGCN2 model - [INFO]: Epoch 881/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:58,135 - LGCN2 model - [INFO]: Epoch 882/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:58,767 - LGCN2 model - [INFO]: Epoch 883/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:49:59,432 - LGCN2 model - [INFO]: Epoch 884/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:00,034 - LGCN2 model - [INFO]: Epoch 885/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:00,683 - LGCN2 model - [INFO]: Epoch 886/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:01,334 - LGCN2 model - [INFO]: Epoch 887/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:02,034 - LGCN2 model - [INFO]: Epoch 888/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:02,668 - LGCN2 model - [INFO]: Epoch 889/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:03,334 - LGCN2 model - [INFO]: Epoch 890/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:03,967 - LGCN2 model - [INFO]: Epoch 891/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:04,617 - LGCN2 model - [INFO]: Epoch 892/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:05,251 - LGCN2 model - [INFO]: Epoch 893/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:05,868 - LGCN2 model - [INFO]: Epoch 894/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:06,534 - LGCN2 model - [INFO]: Epoch 895/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:07,167 - LGCN2 model - [INFO]: Epoch 896/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:07,817 - LGCN2 model - [INFO]: Epoch 897/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:08,471 - LGCN2 model - [INFO]: Epoch 898/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:09,102 - LGCN2 model - [INFO]: Epoch 899/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:09,733 - LGCN2 model - [INFO]: Epoch 900/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:10,371 - LGCN2 model - [INFO]: Epoch 901/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:10,984 - LGCN2 model - [INFO]: Epoch 902/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:11,682 - LGCN2 model - [INFO]: Epoch 903/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:12,316 - LGCN2 model - [INFO]: Epoch 904/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:12,975 - LGCN2 model - [INFO]: Epoch 905/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:13,617 - LGCN2 model - [INFO]: Epoch 906/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:14,267 - LGCN2 model - [INFO]: Epoch 907/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:14,886 - LGCN2 model - [INFO]: Epoch 908/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:15,487 - LGCN2 model - [INFO]: Epoch 909/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:16,100 - LGCN2 model - [INFO]: Epoch 910/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:16,783 - LGCN2 model - [INFO]: Epoch 911/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:17,435 - LGCN2 model - [INFO]: Epoch 912/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:18,100 - LGCN2 model - [INFO]: Epoch 913/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:18,735 - LGCN2 model - [INFO]: Epoch 914/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:19,350 - LGCN2 model - [INFO]: Epoch 915/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:20,017 - LGCN2 model - [INFO]: Epoch 916/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:20,667 - LGCN2 model - [INFO]: Epoch 917/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:21,299 - LGCN2 model - [INFO]: Epoch 918/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:21,952 - LGCN2 model - [INFO]: Epoch 919/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:22,635 - LGCN2 model - [INFO]: Epoch 920/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:23,250 - LGCN2 model - [INFO]: Epoch 921/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:23,937 - LGCN2 model - [INFO]: Epoch 922/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:24,634 - LGCN2 model - [INFO]: Epoch 923/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:25,250 - LGCN2 model - [INFO]: Epoch 924/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:25,923 - LGCN2 model - [INFO]: Epoch 925/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:26,617 - LGCN2 model - [INFO]: Epoch 926/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:27,268 - LGCN2 model - [INFO]: Epoch 927/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:27,919 - LGCN2 model - [INFO]: Epoch 928/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:28,535 - LGCN2 model - [INFO]: Epoch 929/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:29,186 - LGCN2 model - [INFO]: Epoch 930/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:29,892 - LGCN2 model - [INFO]: Epoch 931/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:30,484 - LGCN2 model - [INFO]: Epoch 932/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:31,083 - LGCN2 model - [INFO]: Epoch 933/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:31,767 - LGCN2 model - [INFO]: Epoch 934/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:32,289 - LGCN2 model - [INFO]: Epoch 935/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:32,824 - LGCN2 model - [INFO]: Epoch 936/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:33,418 - LGCN2 model - [INFO]: Epoch 937/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:34,119 - LGCN2 model - [INFO]: Epoch 938/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:34,733 - LGCN2 model - [INFO]: Epoch 939/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:35,400 - LGCN2 model - [INFO]: Epoch 940/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:36,134 - LGCN2 model - [INFO]: Epoch 941/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:36,715 - LGCN2 model - [INFO]: Epoch 942/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:37,315 - LGCN2 model - [INFO]: Epoch 943/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:37,917 - LGCN2 model - [INFO]: Epoch 944/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:38,533 - LGCN2 model - [INFO]: Epoch 945/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:39,135 - LGCN2 model - [INFO]: Epoch 946/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:39,799 - LGCN2 model - [INFO]: Epoch 947/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:40,432 - LGCN2 model - [INFO]: Epoch 948/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:41,066 - LGCN2 model - [INFO]: Epoch 949/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:41,686 - LGCN2 model - [INFO]: Epoch 950/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:42,265 - LGCN2 model - [INFO]: Epoch 951/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:42,883 - LGCN2 model - [INFO]: Epoch 952/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:43,533 - LGCN2 model - [INFO]: Epoch 953/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:44,166 - LGCN2 model - [INFO]: Epoch 954/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:44,788 - LGCN2 model - [INFO]: Epoch 955/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:45,365 - LGCN2 model - [INFO]: Epoch 956/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:45,967 - LGCN2 model - [INFO]: Epoch 957/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:46,633 - LGCN2 model - [INFO]: Epoch 958/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:47,249 - LGCN2 model - [INFO]: Epoch 959/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:47,872 - LGCN2 model - [INFO]: Epoch 960/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:48,499 - LGCN2 model - [INFO]: Epoch 961/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:49,040 - LGCN2 model - [INFO]: Epoch 962/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:49,657 - LGCN2 model - [INFO]: Epoch 963/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:50,233 - LGCN2 model - [INFO]: Epoch 964/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:50,884 - LGCN2 model - [INFO]: Epoch 965/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:51,482 - LGCN2 model - [INFO]: Epoch 966/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:52,074 - LGCN2 model - [INFO]: Epoch 967/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:52,654 - LGCN2 model - [INFO]: Epoch 968/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:53,223 - LGCN2 model - [INFO]: Epoch 969/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:53,805 - LGCN2 model - [INFO]: Epoch 970/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:54,401 - LGCN2 model - [INFO]: Epoch 971/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:55,069 - LGCN2 model - [INFO]: Epoch 972/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:55,656 - LGCN2 model - [INFO]: Epoch 973/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:56,306 - LGCN2 model - [INFO]: Epoch 974/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:57,011 - LGCN2 model - [INFO]: Epoch 975/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:57,632 - LGCN2 model - [INFO]: Epoch 976/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:58,288 - LGCN2 model - [INFO]: Epoch 977/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:58,915 - LGCN2 model - [INFO]: Epoch 978/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:50:59,525 - LGCN2 model - [INFO]: Epoch 979/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:51:00,251 - LGCN2 model - [INFO]: Epoch 980/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:51:00,853 - LGCN2 model - [INFO]: Epoch 981/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:51:01,429 - LGCN2 model - [INFO]: Epoch 982/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:51:02,051 - LGCN2 model - [INFO]: Epoch 983/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:51:02,616 - LGCN2 model - [INFO]: Epoch 984/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:51:03,217 - LGCN2 model - [INFO]: Epoch 985/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:51:03,850 - LGCN2 model - [INFO]: Epoch 986/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:51:04,475 - LGCN2 model - [INFO]: Epoch 987/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:51:05,064 - LGCN2 model - [INFO]: Epoch 988/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:51:05,724 - LGCN2 model - [INFO]: Epoch 989/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:51:06,332 - LGCN2 model - [INFO]: Epoch 990/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:51:07,015 - LGCN2 model - [INFO]: Epoch 991/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:51:07,666 - LGCN2 model - [INFO]: Epoch 992/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:51:08,273 - LGCN2 model - [INFO]: Epoch 993/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:51:08,863 - LGCN2 model - [INFO]: Epoch 994/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:51:09,453 - LGCN2 model - [INFO]: Epoch 995/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:51:10,067 - LGCN2 model - [INFO]: Epoch 996/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:51:10,747 - LGCN2 model - [INFO]: Epoch 997/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:51:11,466 - LGCN2 model - [INFO]: Epoch 998/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:51:12,055 - LGCN2 model - [INFO]: Epoch 999/1000, BPR Loss: 5.1681\n",
      "2024-11-20 17:51:12,666 - LGCN2 model - [INFO]: Epoch 1000/1000, BPR Loss: 5.1681\n"
     ]
    }
   ],
   "source": [
    "from utils.metrics import calculate_ranking_metrics, calculate_rating_metrics\n",
    "\n",
    "lgcn_model2.train()\n",
    "predictions = lgcn_model2.predict()\n",
    "\n",
    "ratings = calculate_rating_metrics(lgcn_model2.test_pre, predictions)\n",
    "\n",
    "top_k_scores = lgcn_model2.recommend_k()\n",
    "rankings = calculate_ranking_metrics(lgcn_model2.test_pre, top_k_scores, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MAP': 0.05243953065703742,\n",
       " 'nDCG@k': 0.22266731254299707,\n",
       " 'Precision@k': 0.1897136797454931,\n",
       " 'Recall@k': 0.0939751882608445}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 16:53:02,339 - LGCN model - [INFO]: Using CPU\n",
      "2024-11-20 16:53:02,339 - LGCN model - [INFO]: Preparing data...\n",
      "2024-11-20 16:53:02,592 - LGCN model - [INFO]: Data Loaded.\n",
      "2024-11-20 16:53:03,106 - LGCN model - [INFO]: Generating negative samples...\n",
      "2024-11-20 16:53:03,609 - LGCN model - [INFO]: Generating negative samples...\n",
      "2024-11-20 16:53:03,822 - LGCN model - [INFO]: building graph...\n",
      "2024-11-20 16:53:03,839 - LGCN model - [INFO]: initilizing model...\n",
      "2024-11-20 16:53:04,156 - LGCN model - [INFO]: Epoch 1/1000, BPR Loss: 0.6931\n",
      "2024-11-20 16:53:04,376 - LGCN model - [INFO]: Epoch 2/1000, BPR Loss: 0.6924\n",
      "2024-11-20 16:53:04,608 - LGCN model - [INFO]: Epoch 3/1000, BPR Loss: 0.6900\n",
      "2024-11-20 16:53:04,823 - LGCN model - [INFO]: Epoch 4/1000, BPR Loss: 0.6852\n",
      "2024-11-20 16:53:05,091 - LGCN model - [INFO]: Epoch 5/1000, BPR Loss: 0.6775\n",
      "2024-11-20 16:53:05,306 - LGCN model - [INFO]: Epoch 6/1000, BPR Loss: 0.6668\n",
      "2024-11-20 16:53:05,542 - LGCN model - [INFO]: Epoch 7/1000, BPR Loss: 0.6530\n",
      "2024-11-20 16:53:05,758 - LGCN model - [INFO]: Epoch 8/1000, BPR Loss: 0.6364\n",
      "2024-11-20 16:53:05,989 - LGCN model - [INFO]: Epoch 9/1000, BPR Loss: 0.6170\n",
      "2024-11-20 16:53:06,241 - LGCN model - [INFO]: Epoch 10/1000, BPR Loss: 0.5954\n",
      "2024-11-20 16:53:06,539 - LGCN model - [INFO]: Epoch 11/1000, BPR Loss: 0.5720\n",
      "2024-11-20 16:53:06,806 - LGCN model - [INFO]: Epoch 12/1000, BPR Loss: 0.5475\n",
      "2024-11-20 16:53:07,025 - LGCN model - [INFO]: Epoch 13/1000, BPR Loss: 0.5226\n",
      "2024-11-20 16:53:07,289 - LGCN model - [INFO]: Epoch 14/1000, BPR Loss: 0.4983\n",
      "2024-11-20 16:53:07,522 - LGCN model - [INFO]: Epoch 15/1000, BPR Loss: 0.4752\n",
      "2024-11-20 16:53:07,764 - LGCN model - [INFO]: Epoch 16/1000, BPR Loss: 0.4541\n",
      "2024-11-20 16:53:07,989 - LGCN model - [INFO]: Epoch 17/1000, BPR Loss: 0.4354\n",
      "2024-11-20 16:53:08,239 - LGCN model - [INFO]: Epoch 18/1000, BPR Loss: 0.4195\n",
      "2024-11-20 16:53:08,510 - LGCN model - [INFO]: Epoch 19/1000, BPR Loss: 0.4065\n",
      "2024-11-20 16:53:08,748 - LGCN model - [INFO]: Epoch 20/1000, BPR Loss: 0.3962\n",
      "2024-11-20 16:53:08,987 - LGCN model - [INFO]: Epoch 21/1000, BPR Loss: 0.3885\n",
      "2024-11-20 16:53:09,242 - LGCN model - [INFO]: Epoch 22/1000, BPR Loss: 0.3828\n",
      "2024-11-20 16:53:09,473 - LGCN model - [INFO]: Epoch 23/1000, BPR Loss: 0.3789\n",
      "2024-11-20 16:53:09,775 - LGCN model - [INFO]: Epoch 24/1000, BPR Loss: 0.3761\n",
      "2024-11-20 16:53:10,062 - LGCN model - [INFO]: Epoch 25/1000, BPR Loss: 0.3743\n",
      "2024-11-20 16:53:10,342 - LGCN model - [INFO]: Epoch 26/1000, BPR Loss: 0.3729\n",
      "2024-11-20 16:53:10,590 - LGCN model - [INFO]: Epoch 27/1000, BPR Loss: 0.3717\n",
      "2024-11-20 16:53:10,838 - LGCN model - [INFO]: Epoch 28/1000, BPR Loss: 0.3706\n",
      "2024-11-20 16:53:11,156 - LGCN model - [INFO]: Epoch 29/1000, BPR Loss: 0.3694\n",
      "2024-11-20 16:53:11,491 - LGCN model - [INFO]: Epoch 30/1000, BPR Loss: 0.3680\n",
      "2024-11-20 16:53:11,807 - LGCN model - [INFO]: Epoch 31/1000, BPR Loss: 0.3664\n",
      "2024-11-20 16:53:12,109 - LGCN model - [INFO]: Epoch 32/1000, BPR Loss: 0.3646\n",
      "2024-11-20 16:53:12,443 - LGCN model - [INFO]: Epoch 33/1000, BPR Loss: 0.3627\n",
      "2024-11-20 16:53:12,810 - LGCN model - [INFO]: Epoch 34/1000, BPR Loss: 0.3607\n",
      "2024-11-20 16:53:13,138 - LGCN model - [INFO]: Epoch 35/1000, BPR Loss: 0.3586\n",
      "2024-11-20 16:53:13,427 - LGCN model - [INFO]: Epoch 36/1000, BPR Loss: 0.3565\n",
      "2024-11-20 16:53:13,761 - LGCN model - [INFO]: Epoch 37/1000, BPR Loss: 0.3545\n",
      "2024-11-20 16:53:14,123 - LGCN model - [INFO]: Epoch 38/1000, BPR Loss: 0.3527\n",
      "2024-11-20 16:53:14,472 - LGCN model - [INFO]: Epoch 39/1000, BPR Loss: 0.3509\n",
      "2024-11-20 16:53:14,776 - LGCN model - [INFO]: Epoch 40/1000, BPR Loss: 0.3494\n",
      "2024-11-20 16:53:15,089 - LGCN model - [INFO]: Epoch 41/1000, BPR Loss: 0.3480\n",
      "2024-11-20 16:53:15,405 - LGCN model - [INFO]: Epoch 42/1000, BPR Loss: 0.3469\n",
      "2024-11-20 16:53:15,722 - LGCN model - [INFO]: Epoch 43/1000, BPR Loss: 0.3458\n",
      "2024-11-20 16:53:16,010 - LGCN model - [INFO]: Epoch 44/1000, BPR Loss: 0.3449\n",
      "2024-11-20 16:53:16,289 - LGCN model - [INFO]: Epoch 45/1000, BPR Loss: 0.3441\n",
      "2024-11-20 16:53:16,591 - LGCN model - [INFO]: Epoch 46/1000, BPR Loss: 0.3433\n",
      "2024-11-20 16:53:16,890 - LGCN model - [INFO]: Epoch 47/1000, BPR Loss: 0.3425\n",
      "2024-11-20 16:53:17,196 - LGCN model - [INFO]: Epoch 48/1000, BPR Loss: 0.3417\n",
      "2024-11-20 16:53:17,550 - LGCN model - [INFO]: Epoch 49/1000, BPR Loss: 0.3407\n",
      "2024-11-20 16:53:17,840 - LGCN model - [INFO]: Epoch 50/1000, BPR Loss: 0.3397\n",
      "2024-11-20 16:53:18,173 - LGCN model - [INFO]: Epoch 51/1000, BPR Loss: 0.3385\n",
      "2024-11-20 16:53:18,495 - LGCN model - [INFO]: Epoch 52/1000, BPR Loss: 0.3372\n",
      "2024-11-20 16:53:18,807 - LGCN model - [INFO]: Epoch 53/1000, BPR Loss: 0.3357\n",
      "2024-11-20 16:53:19,107 - LGCN model - [INFO]: Epoch 54/1000, BPR Loss: 0.3341\n",
      "2024-11-20 16:53:19,422 - LGCN model - [INFO]: Epoch 55/1000, BPR Loss: 0.3324\n",
      "2024-11-20 16:53:19,723 - LGCN model - [INFO]: Epoch 56/1000, BPR Loss: 0.3306\n",
      "2024-11-20 16:53:20,039 - LGCN model - [INFO]: Epoch 57/1000, BPR Loss: 0.3288\n",
      "2024-11-20 16:53:20,306 - LGCN model - [INFO]: Epoch 58/1000, BPR Loss: 0.3269\n",
      "2024-11-20 16:53:20,605 - LGCN model - [INFO]: Epoch 59/1000, BPR Loss: 0.3250\n",
      "2024-11-20 16:53:20,905 - LGCN model - [INFO]: Epoch 60/1000, BPR Loss: 0.3231\n",
      "2024-11-20 16:53:21,257 - LGCN model - [INFO]: Epoch 61/1000, BPR Loss: 0.3212\n",
      "2024-11-20 16:53:21,522 - LGCN model - [INFO]: Epoch 62/1000, BPR Loss: 0.3193\n",
      "2024-11-20 16:53:21,825 - LGCN model - [INFO]: Epoch 63/1000, BPR Loss: 0.3175\n",
      "2024-11-20 16:53:22,125 - LGCN model - [INFO]: Epoch 64/1000, BPR Loss: 0.3158\n",
      "2024-11-20 16:53:22,411 - LGCN model - [INFO]: Epoch 65/1000, BPR Loss: 0.3141\n",
      "2024-11-20 16:53:22,706 - LGCN model - [INFO]: Epoch 66/1000, BPR Loss: 0.3125\n",
      "2024-11-20 16:53:23,046 - LGCN model - [INFO]: Epoch 67/1000, BPR Loss: 0.3109\n",
      "2024-11-20 16:53:23,294 - LGCN model - [INFO]: Epoch 68/1000, BPR Loss: 0.3094\n",
      "2024-11-20 16:53:23,658 - LGCN model - [INFO]: Epoch 69/1000, BPR Loss: 0.3080\n",
      "2024-11-20 16:53:23,973 - LGCN model - [INFO]: Epoch 70/1000, BPR Loss: 0.3066\n",
      "2024-11-20 16:53:24,292 - LGCN model - [INFO]: Epoch 71/1000, BPR Loss: 0.3053\n",
      "2024-11-20 16:53:24,555 - LGCN model - [INFO]: Epoch 72/1000, BPR Loss: 0.3041\n",
      "2024-11-20 16:53:24,880 - LGCN model - [INFO]: Epoch 73/1000, BPR Loss: 0.3030\n",
      "2024-11-20 16:53:25,175 - LGCN model - [INFO]: Epoch 74/1000, BPR Loss: 0.3019\n",
      "2024-11-20 16:53:25,459 - LGCN model - [INFO]: Epoch 75/1000, BPR Loss: 0.3009\n",
      "2024-11-20 16:53:25,755 - LGCN model - [INFO]: Epoch 76/1000, BPR Loss: 0.2999\n",
      "2024-11-20 16:53:26,039 - LGCN model - [INFO]: Epoch 77/1000, BPR Loss: 0.2990\n",
      "2024-11-20 16:53:26,345 - LGCN model - [INFO]: Epoch 78/1000, BPR Loss: 0.2982\n",
      "2024-11-20 16:53:26,609 - LGCN model - [INFO]: Epoch 79/1000, BPR Loss: 0.2975\n",
      "2024-11-20 16:53:26,889 - LGCN model - [INFO]: Epoch 80/1000, BPR Loss: 0.2968\n",
      "2024-11-20 16:53:27,372 - LGCN model - [INFO]: Epoch 81/1000, BPR Loss: 0.2961\n",
      "2024-11-20 16:53:27,798 - LGCN model - [INFO]: Epoch 82/1000, BPR Loss: 0.2955\n",
      "2024-11-20 16:53:28,142 - LGCN model - [INFO]: Epoch 83/1000, BPR Loss: 0.2949\n",
      "2024-11-20 16:53:28,579 - LGCN model - [INFO]: Epoch 84/1000, BPR Loss: 0.2944\n",
      "2024-11-20 16:53:28,979 - LGCN model - [INFO]: Epoch 85/1000, BPR Loss: 0.2939\n",
      "2024-11-20 16:53:29,344 - LGCN model - [INFO]: Epoch 86/1000, BPR Loss: 0.2934\n",
      "2024-11-20 16:53:29,780 - LGCN model - [INFO]: Epoch 87/1000, BPR Loss: 0.2930\n",
      "2024-11-20 16:53:30,244 - LGCN model - [INFO]: Epoch 88/1000, BPR Loss: 0.2926\n",
      "2024-11-20 16:53:30,625 - LGCN model - [INFO]: Epoch 89/1000, BPR Loss: 0.2922\n",
      "2024-11-20 16:53:31,089 - LGCN model - [INFO]: Epoch 90/1000, BPR Loss: 0.2918\n",
      "2024-11-20 16:53:31,517 - LGCN model - [INFO]: Epoch 91/1000, BPR Loss: 0.2914\n",
      "2024-11-20 16:53:31,889 - LGCN model - [INFO]: Epoch 92/1000, BPR Loss: 0.2910\n",
      "2024-11-20 16:53:32,306 - LGCN model - [INFO]: Epoch 93/1000, BPR Loss: 0.2907\n",
      "2024-11-20 16:53:32,626 - LGCN model - [INFO]: Epoch 94/1000, BPR Loss: 0.2903\n",
      "2024-11-20 16:53:32,906 - LGCN model - [INFO]: Epoch 95/1000, BPR Loss: 0.2900\n",
      "2024-11-20 16:53:33,205 - LGCN model - [INFO]: Epoch 96/1000, BPR Loss: 0.2897\n",
      "2024-11-20 16:53:33,597 - LGCN model - [INFO]: Epoch 97/1000, BPR Loss: 0.2894\n",
      "2024-11-20 16:53:34,028 - LGCN model - [INFO]: Epoch 98/1000, BPR Loss: 0.2891\n",
      "2024-11-20 16:53:34,405 - LGCN model - [INFO]: Epoch 99/1000, BPR Loss: 0.2888\n",
      "2024-11-20 16:53:34,855 - LGCN model - [INFO]: Epoch 100/1000, BPR Loss: 0.2885\n",
      "2024-11-20 16:53:35,313 - LGCN model - [INFO]: Epoch 101/1000, BPR Loss: 0.2882\n",
      "2024-11-20 16:53:35,799 - LGCN model - [INFO]: Epoch 102/1000, BPR Loss: 0.2879\n",
      "2024-11-20 16:53:36,301 - LGCN model - [INFO]: Epoch 103/1000, BPR Loss: 0.2876\n",
      "2024-11-20 16:53:36,822 - LGCN model - [INFO]: Epoch 104/1000, BPR Loss: 0.2874\n",
      "2024-11-20 16:53:37,343 - LGCN model - [INFO]: Epoch 105/1000, BPR Loss: 0.2871\n",
      "2024-11-20 16:53:37,790 - LGCN model - [INFO]: Epoch 106/1000, BPR Loss: 0.2868\n",
      "2024-11-20 16:53:38,222 - LGCN model - [INFO]: Epoch 107/1000, BPR Loss: 0.2866\n",
      "2024-11-20 16:53:38,639 - LGCN model - [INFO]: Epoch 108/1000, BPR Loss: 0.2863\n",
      "2024-11-20 16:53:39,107 - LGCN model - [INFO]: Epoch 109/1000, BPR Loss: 0.2860\n",
      "2024-11-20 16:53:39,522 - LGCN model - [INFO]: Epoch 110/1000, BPR Loss: 0.2858\n",
      "2024-11-20 16:53:39,839 - LGCN model - [INFO]: Epoch 111/1000, BPR Loss: 0.2855\n",
      "2024-11-20 16:53:40,158 - LGCN model - [INFO]: Epoch 112/1000, BPR Loss: 0.2852\n",
      "2024-11-20 16:53:40,564 - LGCN model - [INFO]: Epoch 113/1000, BPR Loss: 0.2849\n",
      "2024-11-20 16:53:41,041 - LGCN model - [INFO]: Epoch 114/1000, BPR Loss: 0.2846\n",
      "2024-11-20 16:53:41,539 - LGCN model - [INFO]: Epoch 115/1000, BPR Loss: 0.2843\n",
      "2024-11-20 16:53:42,060 - LGCN model - [INFO]: Epoch 116/1000, BPR Loss: 0.2840\n",
      "2024-11-20 16:53:42,539 - LGCN model - [INFO]: Epoch 117/1000, BPR Loss: 0.2837\n",
      "2024-11-20 16:53:43,058 - LGCN model - [INFO]: Epoch 118/1000, BPR Loss: 0.2833\n",
      "2024-11-20 16:53:43,560 - LGCN model - [INFO]: Epoch 119/1000, BPR Loss: 0.2830\n",
      "2024-11-20 16:53:43,995 - LGCN model - [INFO]: Epoch 120/1000, BPR Loss: 0.2826\n",
      "2024-11-20 16:53:44,475 - LGCN model - [INFO]: Epoch 121/1000, BPR Loss: 0.2823\n",
      "2024-11-20 16:53:44,948 - LGCN model - [INFO]: Epoch 122/1000, BPR Loss: 0.2819\n",
      "2024-11-20 16:53:45,425 - LGCN model - [INFO]: Epoch 123/1000, BPR Loss: 0.2814\n",
      "2024-11-20 16:53:45,725 - LGCN model - [INFO]: Epoch 124/1000, BPR Loss: 0.2810\n",
      "2024-11-20 16:53:46,022 - LGCN model - [INFO]: Epoch 125/1000, BPR Loss: 0.2806\n",
      "2024-11-20 16:53:46,432 - LGCN model - [INFO]: Epoch 126/1000, BPR Loss: 0.2801\n",
      "2024-11-20 16:53:46,875 - LGCN model - [INFO]: Epoch 127/1000, BPR Loss: 0.2797\n",
      "2024-11-20 16:53:47,266 - LGCN model - [INFO]: Epoch 128/1000, BPR Loss: 0.2792\n",
      "2024-11-20 16:53:47,742 - LGCN model - [INFO]: Epoch 129/1000, BPR Loss: 0.2787\n",
      "2024-11-20 16:53:48,189 - LGCN model - [INFO]: Epoch 130/1000, BPR Loss: 0.2782\n",
      "2024-11-20 16:53:48,605 - LGCN model - [INFO]: Epoch 131/1000, BPR Loss: 0.2776\n",
      "2024-11-20 16:53:49,040 - LGCN model - [INFO]: Epoch 132/1000, BPR Loss: 0.2771\n",
      "2024-11-20 16:53:49,537 - LGCN model - [INFO]: Epoch 133/1000, BPR Loss: 0.2765\n",
      "2024-11-20 16:53:49,942 - LGCN model - [INFO]: Epoch 134/1000, BPR Loss: 0.2760\n",
      "2024-11-20 16:53:50,454 - LGCN model - [INFO]: Epoch 135/1000, BPR Loss: 0.2754\n",
      "2024-11-20 16:53:50,892 - LGCN model - [INFO]: Epoch 136/1000, BPR Loss: 0.2748\n",
      "2024-11-20 16:53:51,305 - LGCN model - [INFO]: Epoch 137/1000, BPR Loss: 0.2742\n",
      "2024-11-20 16:53:51,840 - LGCN model - [INFO]: Epoch 138/1000, BPR Loss: 0.2736\n",
      "2024-11-20 16:53:52,272 - LGCN model - [INFO]: Epoch 139/1000, BPR Loss: 0.2730\n",
      "2024-11-20 16:53:52,724 - LGCN model - [INFO]: Epoch 140/1000, BPR Loss: 0.2724\n",
      "2024-11-20 16:53:53,192 - LGCN model - [INFO]: Epoch 141/1000, BPR Loss: 0.2718\n",
      "2024-11-20 16:53:53,559 - LGCN model - [INFO]: Epoch 142/1000, BPR Loss: 0.2711\n",
      "2024-11-20 16:53:53,921 - LGCN model - [INFO]: Epoch 143/1000, BPR Loss: 0.2705\n",
      "2024-11-20 16:53:54,227 - LGCN model - [INFO]: Epoch 144/1000, BPR Loss: 0.2699\n",
      "2024-11-20 16:53:54,606 - LGCN model - [INFO]: Epoch 145/1000, BPR Loss: 0.2692\n",
      "2024-11-20 16:53:55,060 - LGCN model - [INFO]: Epoch 146/1000, BPR Loss: 0.2686\n",
      "2024-11-20 16:53:55,576 - LGCN model - [INFO]: Epoch 147/1000, BPR Loss: 0.2680\n",
      "2024-11-20 16:53:55,972 - LGCN model - [INFO]: Epoch 148/1000, BPR Loss: 0.2674\n",
      "2024-11-20 16:53:56,424 - LGCN model - [INFO]: Epoch 149/1000, BPR Loss: 0.2667\n",
      "2024-11-20 16:53:56,890 - LGCN model - [INFO]: Epoch 150/1000, BPR Loss: 0.2661\n",
      "2024-11-20 16:53:57,322 - LGCN model - [INFO]: Epoch 151/1000, BPR Loss: 0.2655\n",
      "2024-11-20 16:53:57,848 - LGCN model - [INFO]: Epoch 152/1000, BPR Loss: 0.2649\n",
      "2024-11-20 16:53:58,254 - LGCN model - [INFO]: Epoch 153/1000, BPR Loss: 0.2643\n",
      "2024-11-20 16:53:58,707 - LGCN model - [INFO]: Epoch 154/1000, BPR Loss: 0.2637\n",
      "2024-11-20 16:53:59,224 - LGCN model - [INFO]: Epoch 155/1000, BPR Loss: 0.2632\n",
      "2024-11-20 16:53:59,625 - LGCN model - [INFO]: Epoch 156/1000, BPR Loss: 0.2626\n",
      "2024-11-20 16:54:00,059 - LGCN model - [INFO]: Epoch 157/1000, BPR Loss: 0.2620\n",
      "2024-11-20 16:54:00,538 - LGCN model - [INFO]: Epoch 158/1000, BPR Loss: 0.2615\n",
      "2024-11-20 16:54:00,989 - LGCN model - [INFO]: Epoch 159/1000, BPR Loss: 0.2610\n",
      "2024-11-20 16:54:01,355 - LGCN model - [INFO]: Epoch 160/1000, BPR Loss: 0.2605\n",
      "2024-11-20 16:54:01,656 - LGCN model - [INFO]: Epoch 161/1000, BPR Loss: 0.2600\n",
      "2024-11-20 16:54:01,973 - LGCN model - [INFO]: Epoch 162/1000, BPR Loss: 0.2595\n",
      "2024-11-20 16:54:02,378 - LGCN model - [INFO]: Epoch 163/1000, BPR Loss: 0.2590\n",
      "2024-11-20 16:54:02,822 - LGCN model - [INFO]: Epoch 164/1000, BPR Loss: 0.2586\n",
      "2024-11-20 16:54:03,294 - LGCN model - [INFO]: Epoch 165/1000, BPR Loss: 0.2581\n",
      "2024-11-20 16:54:03,697 - LGCN model - [INFO]: Epoch 166/1000, BPR Loss: 0.2577\n",
      "2024-11-20 16:54:04,172 - LGCN model - [INFO]: Epoch 167/1000, BPR Loss: 0.2573\n",
      "2024-11-20 16:54:04,611 - LGCN model - [INFO]: Epoch 168/1000, BPR Loss: 0.2569\n",
      "2024-11-20 16:54:05,169 - LGCN model - [INFO]: Epoch 169/1000, BPR Loss: 0.2566\n",
      "2024-11-20 16:54:05,597 - LGCN model - [INFO]: Epoch 170/1000, BPR Loss: 0.2562\n",
      "2024-11-20 16:54:06,175 - LGCN model - [INFO]: Epoch 171/1000, BPR Loss: 0.2558\n",
      "2024-11-20 16:54:06,734 - LGCN model - [INFO]: Epoch 172/1000, BPR Loss: 0.2555\n",
      "2024-11-20 16:54:07,226 - LGCN model - [INFO]: Epoch 173/1000, BPR Loss: 0.2552\n",
      "2024-11-20 16:54:07,689 - LGCN model - [INFO]: Epoch 174/1000, BPR Loss: 0.2549\n",
      "2024-11-20 16:54:08,117 - LGCN model - [INFO]: Epoch 175/1000, BPR Loss: 0.2546\n",
      "2024-11-20 16:54:08,507 - LGCN model - [INFO]: Epoch 176/1000, BPR Loss: 0.2543\n",
      "2024-11-20 16:54:08,827 - LGCN model - [INFO]: Epoch 177/1000, BPR Loss: 0.2540\n",
      "2024-11-20 16:54:09,139 - LGCN model - [INFO]: Epoch 178/1000, BPR Loss: 0.2538\n",
      "2024-11-20 16:54:09,572 - LGCN model - [INFO]: Epoch 179/1000, BPR Loss: 0.2535\n",
      "2024-11-20 16:54:09,972 - LGCN model - [INFO]: Epoch 180/1000, BPR Loss: 0.2533\n",
      "2024-11-20 16:54:10,422 - LGCN model - [INFO]: Epoch 181/1000, BPR Loss: 0.2531\n",
      "2024-11-20 16:54:10,892 - LGCN model - [INFO]: Epoch 182/1000, BPR Loss: 0.2528\n",
      "2024-11-20 16:54:11,438 - LGCN model - [INFO]: Epoch 183/1000, BPR Loss: 0.2526\n",
      "2024-11-20 16:54:11,993 - LGCN model - [INFO]: Epoch 184/1000, BPR Loss: 0.2524\n",
      "2024-11-20 16:54:12,546 - LGCN model - [INFO]: Epoch 185/1000, BPR Loss: 0.2522\n",
      "2024-11-20 16:54:13,082 - LGCN model - [INFO]: Epoch 186/1000, BPR Loss: 0.2520\n",
      "2024-11-20 16:54:13,617 - LGCN model - [INFO]: Epoch 187/1000, BPR Loss: 0.2518\n",
      "2024-11-20 16:54:14,088 - LGCN model - [INFO]: Epoch 188/1000, BPR Loss: 0.2517\n",
      "2024-11-20 16:54:14,482 - LGCN model - [INFO]: Epoch 189/1000, BPR Loss: 0.2515\n",
      "2024-11-20 16:54:14,958 - LGCN model - [INFO]: Epoch 190/1000, BPR Loss: 0.2513\n",
      "2024-11-20 16:54:15,442 - LGCN model - [INFO]: Epoch 191/1000, BPR Loss: 0.2511\n",
      "2024-11-20 16:54:15,822 - LGCN model - [INFO]: Epoch 192/1000, BPR Loss: 0.2510\n",
      "2024-11-20 16:54:16,190 - LGCN model - [INFO]: Epoch 193/1000, BPR Loss: 0.2508\n",
      "2024-11-20 16:54:16,509 - LGCN model - [INFO]: Epoch 194/1000, BPR Loss: 0.2507\n",
      "2024-11-20 16:54:16,901 - LGCN model - [INFO]: Epoch 195/1000, BPR Loss: 0.2505\n",
      "2024-11-20 16:54:17,338 - LGCN model - [INFO]: Epoch 196/1000, BPR Loss: 0.2504\n",
      "2024-11-20 16:54:17,839 - LGCN model - [INFO]: Epoch 197/1000, BPR Loss: 0.2502\n",
      "2024-11-20 16:54:18,258 - LGCN model - [INFO]: Epoch 198/1000, BPR Loss: 0.2501\n",
      "2024-11-20 16:54:18,705 - LGCN model - [INFO]: Epoch 199/1000, BPR Loss: 0.2500\n",
      "2024-11-20 16:54:19,173 - LGCN model - [INFO]: Epoch 200/1000, BPR Loss: 0.2498\n",
      "2024-11-20 16:54:19,611 - LGCN model - [INFO]: Epoch 201/1000, BPR Loss: 0.2497\n",
      "2024-11-20 16:54:20,071 - LGCN model - [INFO]: Epoch 202/1000, BPR Loss: 0.2495\n",
      "2024-11-20 16:54:20,551 - LGCN model - [INFO]: Epoch 203/1000, BPR Loss: 0.2494\n",
      "2024-11-20 16:54:20,982 - LGCN model - [INFO]: Epoch 204/1000, BPR Loss: 0.2493\n",
      "2024-11-20 16:54:21,489 - LGCN model - [INFO]: Epoch 205/1000, BPR Loss: 0.2492\n",
      "2024-11-20 16:54:21,939 - LGCN model - [INFO]: Epoch 206/1000, BPR Loss: 0.2490\n",
      "2024-11-20 16:54:22,372 - LGCN model - [INFO]: Epoch 207/1000, BPR Loss: 0.2489\n",
      "2024-11-20 16:54:22,858 - LGCN model - [INFO]: Epoch 208/1000, BPR Loss: 0.2488\n",
      "2024-11-20 16:54:23,229 - LGCN model - [INFO]: Epoch 209/1000, BPR Loss: 0.2487\n",
      "2024-11-20 16:54:23,755 - LGCN model - [INFO]: Epoch 210/1000, BPR Loss: 0.2485\n",
      "2024-11-20 16:54:24,127 - LGCN model - [INFO]: Epoch 211/1000, BPR Loss: 0.2484\n",
      "2024-11-20 16:54:24,405 - LGCN model - [INFO]: Epoch 212/1000, BPR Loss: 0.2483\n",
      "2024-11-20 16:54:24,708 - LGCN model - [INFO]: Epoch 213/1000, BPR Loss: 0.2482\n",
      "2024-11-20 16:54:25,039 - LGCN model - [INFO]: Epoch 214/1000, BPR Loss: 0.2480\n",
      "2024-11-20 16:54:25,496 - LGCN model - [INFO]: Epoch 215/1000, BPR Loss: 0.2479\n",
      "2024-11-20 16:54:25,955 - LGCN model - [INFO]: Epoch 216/1000, BPR Loss: 0.2478\n",
      "2024-11-20 16:54:26,424 - LGCN model - [INFO]: Epoch 217/1000, BPR Loss: 0.2477\n",
      "2024-11-20 16:54:26,838 - LGCN model - [INFO]: Epoch 218/1000, BPR Loss: 0.2476\n",
      "2024-11-20 16:54:27,346 - LGCN model - [INFO]: Epoch 219/1000, BPR Loss: 0.2474\n",
      "2024-11-20 16:54:27,774 - LGCN model - [INFO]: Epoch 220/1000, BPR Loss: 0.2473\n",
      "2024-11-20 16:54:28,258 - LGCN model - [INFO]: Epoch 221/1000, BPR Loss: 0.2472\n",
      "2024-11-20 16:54:28,732 - LGCN model - [INFO]: Epoch 222/1000, BPR Loss: 0.2471\n",
      "2024-11-20 16:54:29,200 - LGCN model - [INFO]: Epoch 223/1000, BPR Loss: 0.2470\n",
      "2024-11-20 16:54:29,621 - LGCN model - [INFO]: Epoch 224/1000, BPR Loss: 0.2468\n",
      "2024-11-20 16:54:30,107 - LGCN model - [INFO]: Epoch 225/1000, BPR Loss: 0.2467\n",
      "2024-11-20 16:54:30,558 - LGCN model - [INFO]: Epoch 226/1000, BPR Loss: 0.2466\n",
      "2024-11-20 16:54:30,955 - LGCN model - [INFO]: Epoch 227/1000, BPR Loss: 0.2465\n",
      "2024-11-20 16:54:31,488 - LGCN model - [INFO]: Epoch 228/1000, BPR Loss: 0.2464\n",
      "2024-11-20 16:54:31,871 - LGCN model - [INFO]: Epoch 229/1000, BPR Loss: 0.2463\n",
      "2024-11-20 16:54:32,175 - LGCN model - [INFO]: Epoch 230/1000, BPR Loss: 0.2461\n",
      "2024-11-20 16:54:32,505 - LGCN model - [INFO]: Epoch 231/1000, BPR Loss: 0.2460\n",
      "2024-11-20 16:54:32,910 - LGCN model - [INFO]: Epoch 232/1000, BPR Loss: 0.2459\n",
      "2024-11-20 16:54:33,338 - LGCN model - [INFO]: Epoch 233/1000, BPR Loss: 0.2458\n",
      "2024-11-20 16:54:33,788 - LGCN model - [INFO]: Epoch 234/1000, BPR Loss: 0.2457\n",
      "2024-11-20 16:54:34,175 - LGCN model - [INFO]: Epoch 235/1000, BPR Loss: 0.2456\n",
      "2024-11-20 16:54:34,672 - LGCN model - [INFO]: Epoch 236/1000, BPR Loss: 0.2455\n",
      "2024-11-20 16:54:35,123 - LGCN model - [INFO]: Epoch 237/1000, BPR Loss: 0.2453\n",
      "2024-11-20 16:54:35,556 - LGCN model - [INFO]: Epoch 238/1000, BPR Loss: 0.2452\n",
      "2024-11-20 16:54:36,021 - LGCN model - [INFO]: Epoch 239/1000, BPR Loss: 0.2451\n",
      "2024-11-20 16:54:36,522 - LGCN model - [INFO]: Epoch 240/1000, BPR Loss: 0.2450\n",
      "2024-11-20 16:54:37,039 - LGCN model - [INFO]: Epoch 241/1000, BPR Loss: 0.2449\n",
      "2024-11-20 16:54:37,505 - LGCN model - [INFO]: Epoch 242/1000, BPR Loss: 0.2448\n",
      "2024-11-20 16:54:37,977 - LGCN model - [INFO]: Epoch 243/1000, BPR Loss: 0.2447\n",
      "2024-11-20 16:54:38,474 - LGCN model - [INFO]: Epoch 244/1000, BPR Loss: 0.2446\n",
      "2024-11-20 16:54:38,822 - LGCN model - [INFO]: Epoch 245/1000, BPR Loss: 0.2445\n",
      "2024-11-20 16:54:39,141 - LGCN model - [INFO]: Epoch 246/1000, BPR Loss: 0.2444\n",
      "2024-11-20 16:54:39,508 - LGCN model - [INFO]: Epoch 247/1000, BPR Loss: 0.2443\n",
      "2024-11-20 16:54:39,986 - LGCN model - [INFO]: Epoch 248/1000, BPR Loss: 0.2442\n",
      "2024-11-20 16:54:40,471 - LGCN model - [INFO]: Epoch 249/1000, BPR Loss: 0.2441\n",
      "2024-11-20 16:54:40,956 - LGCN model - [INFO]: Epoch 250/1000, BPR Loss: 0.2440\n",
      "2024-11-20 16:54:41,421 - LGCN model - [INFO]: Epoch 251/1000, BPR Loss: 0.2439\n",
      "2024-11-20 16:54:41,890 - LGCN model - [INFO]: Epoch 252/1000, BPR Loss: 0.2438\n",
      "2024-11-20 16:54:42,341 - LGCN model - [INFO]: Epoch 253/1000, BPR Loss: 0.2437\n",
      "2024-11-20 16:54:42,805 - LGCN model - [INFO]: Epoch 254/1000, BPR Loss: 0.2436\n",
      "2024-11-20 16:54:43,273 - LGCN model - [INFO]: Epoch 255/1000, BPR Loss: 0.2435\n",
      "2024-11-20 16:54:43,754 - LGCN model - [INFO]: Epoch 256/1000, BPR Loss: 0.2434\n",
      "2024-11-20 16:54:44,243 - LGCN model - [INFO]: Epoch 257/1000, BPR Loss: 0.2433\n",
      "2024-11-20 16:54:44,621 - LGCN model - [INFO]: Epoch 258/1000, BPR Loss: 0.2432\n",
      "2024-11-20 16:54:44,951 - LGCN model - [INFO]: Epoch 259/1000, BPR Loss: 0.2431\n",
      "2024-11-20 16:54:45,252 - LGCN model - [INFO]: Epoch 260/1000, BPR Loss: 0.2430\n",
      "2024-11-20 16:54:45,721 - LGCN model - [INFO]: Epoch 261/1000, BPR Loss: 0.2429\n",
      "2024-11-20 16:54:46,156 - LGCN model - [INFO]: Epoch 262/1000, BPR Loss: 0.2428\n",
      "2024-11-20 16:54:46,608 - LGCN model - [INFO]: Epoch 263/1000, BPR Loss: 0.2427\n",
      "2024-11-20 16:54:47,091 - LGCN model - [INFO]: Epoch 264/1000, BPR Loss: 0.2426\n",
      "2024-11-20 16:54:47,505 - LGCN model - [INFO]: Epoch 265/1000, BPR Loss: 0.2425\n",
      "2024-11-20 16:54:47,942 - LGCN model - [INFO]: Epoch 266/1000, BPR Loss: 0.2424\n",
      "2024-11-20 16:54:48,380 - LGCN model - [INFO]: Epoch 267/1000, BPR Loss: 0.2424\n",
      "2024-11-20 16:54:48,788 - LGCN model - [INFO]: Epoch 268/1000, BPR Loss: 0.2423\n",
      "2024-11-20 16:54:49,288 - LGCN model - [INFO]: Epoch 269/1000, BPR Loss: 0.2422\n",
      "2024-11-20 16:54:49,692 - LGCN model - [INFO]: Epoch 270/1000, BPR Loss: 0.2421\n",
      "2024-11-20 16:54:50,156 - LGCN model - [INFO]: Epoch 271/1000, BPR Loss: 0.2420\n",
      "2024-11-20 16:54:50,505 - LGCN model - [INFO]: Epoch 272/1000, BPR Loss: 0.2419\n",
      "2024-11-20 16:54:51,005 - LGCN model - [INFO]: Epoch 273/1000, BPR Loss: 0.2419\n",
      "2024-11-20 16:54:51,456 - LGCN model - [INFO]: Epoch 274/1000, BPR Loss: 0.2418\n",
      "2024-11-20 16:54:51,892 - LGCN model - [INFO]: Epoch 275/1000, BPR Loss: 0.2417\n",
      "2024-11-20 16:54:52,321 - LGCN model - [INFO]: Epoch 276/1000, BPR Loss: 0.2416\n",
      "2024-11-20 16:54:52,725 - LGCN model - [INFO]: Epoch 277/1000, BPR Loss: 0.2415\n",
      "2024-11-20 16:54:53,025 - LGCN model - [INFO]: Epoch 278/1000, BPR Loss: 0.2415\n",
      "2024-11-20 16:54:53,341 - LGCN model - [INFO]: Epoch 279/1000, BPR Loss: 0.2414\n",
      "2024-11-20 16:54:53,682 - LGCN model - [INFO]: Epoch 280/1000, BPR Loss: 0.2413\n",
      "2024-11-20 16:54:54,156 - LGCN model - [INFO]: Epoch 281/1000, BPR Loss: 0.2412\n",
      "2024-11-20 16:54:54,656 - LGCN model - [INFO]: Epoch 282/1000, BPR Loss: 0.2412\n",
      "2024-11-20 16:54:55,124 - LGCN model - [INFO]: Epoch 283/1000, BPR Loss: 0.2411\n",
      "2024-11-20 16:54:55,580 - LGCN model - [INFO]: Epoch 284/1000, BPR Loss: 0.2410\n",
      "2024-11-20 16:54:56,005 - LGCN model - [INFO]: Epoch 285/1000, BPR Loss: 0.2409\n",
      "2024-11-20 16:54:56,457 - LGCN model - [INFO]: Epoch 286/1000, BPR Loss: 0.2409\n",
      "2024-11-20 16:54:56,944 - LGCN model - [INFO]: Epoch 287/1000, BPR Loss: 0.2408\n",
      "2024-11-20 16:54:57,338 - LGCN model - [INFO]: Epoch 288/1000, BPR Loss: 0.2407\n",
      "2024-11-20 16:54:57,804 - LGCN model - [INFO]: Epoch 289/1000, BPR Loss: 0.2407\n",
      "2024-11-20 16:54:58,245 - LGCN model - [INFO]: Epoch 290/1000, BPR Loss: 0.2406\n",
      "2024-11-20 16:54:58,704 - LGCN model - [INFO]: Epoch 291/1000, BPR Loss: 0.2405\n",
      "2024-11-20 16:54:59,172 - LGCN model - [INFO]: Epoch 292/1000, BPR Loss: 0.2405\n",
      "2024-11-20 16:54:59,544 - LGCN model - [INFO]: Epoch 293/1000, BPR Loss: 0.2404\n",
      "2024-11-20 16:55:00,039 - LGCN model - [INFO]: Epoch 294/1000, BPR Loss: 0.2403\n",
      "2024-11-20 16:55:00,490 - LGCN model - [INFO]: Epoch 295/1000, BPR Loss: 0.2403\n",
      "2024-11-20 16:55:00,921 - LGCN model - [INFO]: Epoch 296/1000, BPR Loss: 0.2402\n",
      "2024-11-20 16:55:01,288 - LGCN model - [INFO]: Epoch 297/1000, BPR Loss: 0.2401\n",
      "2024-11-20 16:55:01,591 - LGCN model - [INFO]: Epoch 298/1000, BPR Loss: 0.2401\n",
      "2024-11-20 16:55:01,904 - LGCN model - [INFO]: Epoch 299/1000, BPR Loss: 0.2400\n",
      "2024-11-20 16:55:02,302 - LGCN model - [INFO]: Epoch 300/1000, BPR Loss: 0.2400\n",
      "2024-11-20 16:55:02,765 - LGCN model - [INFO]: Epoch 301/1000, BPR Loss: 0.2399\n",
      "2024-11-20 16:55:03,341 - LGCN model - [INFO]: Epoch 302/1000, BPR Loss: 0.2398\n",
      "2024-11-20 16:55:03,866 - LGCN model - [INFO]: Epoch 303/1000, BPR Loss: 0.2398\n",
      "2024-11-20 16:55:04,405 - LGCN model - [INFO]: Epoch 304/1000, BPR Loss: 0.2397\n",
      "2024-11-20 16:55:04,888 - LGCN model - [INFO]: Epoch 305/1000, BPR Loss: 0.2397\n",
      "2024-11-20 16:55:05,354 - LGCN model - [INFO]: Epoch 306/1000, BPR Loss: 0.2396\n",
      "2024-11-20 16:55:05,904 - LGCN model - [INFO]: Epoch 307/1000, BPR Loss: 0.2396\n",
      "2024-11-20 16:55:06,376 - LGCN model - [INFO]: Epoch 308/1000, BPR Loss: 0.2395\n",
      "2024-11-20 16:55:06,936 - LGCN model - [INFO]: Epoch 309/1000, BPR Loss: 0.2394\n",
      "2024-11-20 16:55:07,407 - LGCN model - [INFO]: Epoch 310/1000, BPR Loss: 0.2394\n",
      "2024-11-20 16:55:07,869 - LGCN model - [INFO]: Epoch 311/1000, BPR Loss: 0.2393\n",
      "2024-11-20 16:55:08,324 - LGCN model - [INFO]: Epoch 312/1000, BPR Loss: 0.2393\n",
      "2024-11-20 16:55:08,742 - LGCN model - [INFO]: Epoch 313/1000, BPR Loss: 0.2392\n",
      "2024-11-20 16:55:09,238 - LGCN model - [INFO]: Epoch 314/1000, BPR Loss: 0.2392\n",
      "2024-11-20 16:55:09,692 - LGCN model - [INFO]: Epoch 315/1000, BPR Loss: 0.2391\n",
      "2024-11-20 16:55:10,171 - LGCN model - [INFO]: Epoch 316/1000, BPR Loss: 0.2391\n",
      "2024-11-20 16:55:10,604 - LGCN model - [INFO]: Epoch 317/1000, BPR Loss: 0.2390\n",
      "2024-11-20 16:55:10,939 - LGCN model - [INFO]: Epoch 318/1000, BPR Loss: 0.2390\n",
      "2024-11-20 16:55:11,255 - LGCN model - [INFO]: Epoch 319/1000, BPR Loss: 0.2389\n",
      "2024-11-20 16:55:11,571 - LGCN model - [INFO]: Epoch 320/1000, BPR Loss: 0.2389\n",
      "2024-11-20 16:55:12,072 - LGCN model - [INFO]: Epoch 321/1000, BPR Loss: 0.2388\n",
      "2024-11-20 16:55:12,474 - LGCN model - [INFO]: Epoch 322/1000, BPR Loss: 0.2388\n",
      "2024-11-20 16:55:13,059 - LGCN model - [INFO]: Epoch 323/1000, BPR Loss: 0.2387\n",
      "2024-11-20 16:55:13,443 - LGCN model - [INFO]: Epoch 324/1000, BPR Loss: 0.2387\n",
      "2024-11-20 16:55:13,940 - LGCN model - [INFO]: Epoch 325/1000, BPR Loss: 0.2386\n",
      "2024-11-20 16:55:14,369 - LGCN model - [INFO]: Epoch 326/1000, BPR Loss: 0.2386\n",
      "2024-11-20 16:55:14,854 - LGCN model - [INFO]: Epoch 327/1000, BPR Loss: 0.2385\n",
      "2024-11-20 16:55:15,338 - LGCN model - [INFO]: Epoch 328/1000, BPR Loss: 0.2385\n",
      "2024-11-20 16:55:15,774 - LGCN model - [INFO]: Epoch 329/1000, BPR Loss: 0.2385\n",
      "2024-11-20 16:55:16,304 - LGCN model - [INFO]: Epoch 330/1000, BPR Loss: 0.2384\n",
      "2024-11-20 16:55:16,755 - LGCN model - [INFO]: Epoch 331/1000, BPR Loss: 0.2384\n",
      "2024-11-20 16:55:17,205 - LGCN model - [INFO]: Epoch 332/1000, BPR Loss: 0.2383\n",
      "2024-11-20 16:55:17,673 - LGCN model - [INFO]: Epoch 333/1000, BPR Loss: 0.2383\n",
      "2024-11-20 16:55:18,155 - LGCN model - [INFO]: Epoch 334/1000, BPR Loss: 0.2382\n",
      "2024-11-20 16:55:18,602 - LGCN model - [INFO]: Epoch 335/1000, BPR Loss: 0.2382\n",
      "2024-11-20 16:55:19,039 - LGCN model - [INFO]: Epoch 336/1000, BPR Loss: 0.2382\n",
      "2024-11-20 16:55:19,571 - LGCN model - [INFO]: Epoch 337/1000, BPR Loss: 0.2381\n",
      "2024-11-20 16:55:19,956 - LGCN model - [INFO]: Epoch 338/1000, BPR Loss: 0.2381\n",
      "2024-11-20 16:55:20,339 - LGCN model - [INFO]: Epoch 339/1000, BPR Loss: 0.2380\n",
      "2024-11-20 16:55:20,688 - LGCN model - [INFO]: Epoch 340/1000, BPR Loss: 0.2380\n",
      "2024-11-20 16:55:21,155 - LGCN model - [INFO]: Epoch 341/1000, BPR Loss: 0.2379\n",
      "2024-11-20 16:55:21,686 - LGCN model - [INFO]: Epoch 342/1000, BPR Loss: 0.2379\n",
      "2024-11-20 16:55:22,205 - LGCN model - [INFO]: Epoch 343/1000, BPR Loss: 0.2379\n",
      "2024-11-20 16:55:22,690 - LGCN model - [INFO]: Epoch 344/1000, BPR Loss: 0.2378\n",
      "2024-11-20 16:55:23,203 - LGCN model - [INFO]: Epoch 345/1000, BPR Loss: 0.2378\n",
      "2024-11-20 16:55:23,738 - LGCN model - [INFO]: Epoch 346/1000, BPR Loss: 0.2377\n",
      "2024-11-20 16:55:24,240 - LGCN model - [INFO]: Epoch 347/1000, BPR Loss: 0.2377\n",
      "2024-11-20 16:55:24,705 - LGCN model - [INFO]: Epoch 348/1000, BPR Loss: 0.2377\n",
      "2024-11-20 16:55:25,176 - LGCN model - [INFO]: Epoch 349/1000, BPR Loss: 0.2376\n",
      "2024-11-20 16:55:25,654 - LGCN model - [INFO]: Epoch 350/1000, BPR Loss: 0.2376\n",
      "2024-11-20 16:55:26,154 - LGCN model - [INFO]: Epoch 351/1000, BPR Loss: 0.2375\n",
      "2024-11-20 16:55:26,556 - LGCN model - [INFO]: Epoch 352/1000, BPR Loss: 0.2375\n",
      "2024-11-20 16:55:26,973 - LGCN model - [INFO]: Epoch 353/1000, BPR Loss: 0.2375\n",
      "2024-11-20 16:55:27,287 - LGCN model - [INFO]: Epoch 354/1000, BPR Loss: 0.2374\n",
      "2024-11-20 16:55:27,605 - LGCN model - [INFO]: Epoch 355/1000, BPR Loss: 0.2374\n",
      "2024-11-20 16:55:27,908 - LGCN model - [INFO]: Epoch 356/1000, BPR Loss: 0.2373\n",
      "2024-11-20 16:55:28,388 - LGCN model - [INFO]: Epoch 357/1000, BPR Loss: 0.2373\n",
      "2024-11-20 16:55:28,858 - LGCN model - [INFO]: Epoch 358/1000, BPR Loss: 0.2373\n",
      "2024-11-20 16:55:29,255 - LGCN model - [INFO]: Epoch 359/1000, BPR Loss: 0.2372\n",
      "2024-11-20 16:55:29,740 - LGCN model - [INFO]: Epoch 360/1000, BPR Loss: 0.2372\n",
      "2024-11-20 16:55:30,187 - LGCN model - [INFO]: Epoch 361/1000, BPR Loss: 0.2372\n",
      "2024-11-20 16:55:30,588 - LGCN model - [INFO]: Epoch 362/1000, BPR Loss: 0.2371\n",
      "2024-11-20 16:55:31,040 - LGCN model - [INFO]: Epoch 363/1000, BPR Loss: 0.2371\n",
      "2024-11-20 16:55:31,426 - LGCN model - [INFO]: Epoch 364/1000, BPR Loss: 0.2370\n",
      "2024-11-20 16:55:31,888 - LGCN model - [INFO]: Epoch 365/1000, BPR Loss: 0.2370\n",
      "2024-11-20 16:55:32,317 - LGCN model - [INFO]: Epoch 366/1000, BPR Loss: 0.2370\n",
      "2024-11-20 16:55:32,737 - LGCN model - [INFO]: Epoch 367/1000, BPR Loss: 0.2369\n",
      "2024-11-20 16:55:33,142 - LGCN model - [INFO]: Epoch 368/1000, BPR Loss: 0.2369\n",
      "2024-11-20 16:55:33,554 - LGCN model - [INFO]: Epoch 369/1000, BPR Loss: 0.2369\n",
      "2024-11-20 16:55:34,013 - LGCN model - [INFO]: Epoch 370/1000, BPR Loss: 0.2368\n",
      "2024-11-20 16:55:34,455 - LGCN model - [INFO]: Epoch 371/1000, BPR Loss: 0.2368\n",
      "2024-11-20 16:55:34,924 - LGCN model - [INFO]: Epoch 372/1000, BPR Loss: 0.2368\n",
      "2024-11-20 16:55:35,304 - LGCN model - [INFO]: Epoch 373/1000, BPR Loss: 0.2367\n",
      "2024-11-20 16:55:35,621 - LGCN model - [INFO]: Epoch 374/1000, BPR Loss: 0.2367\n",
      "2024-11-20 16:55:35,938 - LGCN model - [INFO]: Epoch 375/1000, BPR Loss: 0.2366\n",
      "2024-11-20 16:55:36,385 - LGCN model - [INFO]: Epoch 376/1000, BPR Loss: 0.2366\n",
      "2024-11-20 16:55:36,880 - LGCN model - [INFO]: Epoch 377/1000, BPR Loss: 0.2366\n",
      "2024-11-20 16:55:37,388 - LGCN model - [INFO]: Epoch 378/1000, BPR Loss: 0.2365\n",
      "2024-11-20 16:55:37,800 - LGCN model - [INFO]: Epoch 379/1000, BPR Loss: 0.2365\n",
      "2024-11-20 16:55:38,374 - LGCN model - [INFO]: Epoch 380/1000, BPR Loss: 0.2365\n",
      "2024-11-20 16:55:38,737 - LGCN model - [INFO]: Epoch 381/1000, BPR Loss: 0.2364\n",
      "2024-11-20 16:55:39,271 - LGCN model - [INFO]: Epoch 382/1000, BPR Loss: 0.2364\n",
      "2024-11-20 16:55:39,626 - LGCN model - [INFO]: Epoch 383/1000, BPR Loss: 0.2364\n",
      "2024-11-20 16:55:40,121 - LGCN model - [INFO]: Epoch 384/1000, BPR Loss: 0.2363\n",
      "2024-11-20 16:55:40,612 - LGCN model - [INFO]: Epoch 385/1000, BPR Loss: 0.2363\n",
      "2024-11-20 16:55:41,056 - LGCN model - [INFO]: Epoch 386/1000, BPR Loss: 0.2362\n",
      "2024-11-20 16:55:41,493 - LGCN model - [INFO]: Epoch 387/1000, BPR Loss: 0.2362\n",
      "2024-11-20 16:55:41,954 - LGCN model - [INFO]: Epoch 388/1000, BPR Loss: 0.2362\n",
      "2024-11-20 16:55:42,455 - LGCN model - [INFO]: Epoch 389/1000, BPR Loss: 0.2361\n",
      "2024-11-20 16:55:42,869 - LGCN model - [INFO]: Epoch 390/1000, BPR Loss: 0.2361\n",
      "2024-11-20 16:55:43,405 - LGCN model - [INFO]: Epoch 391/1000, BPR Loss: 0.2361\n",
      "2024-11-20 16:55:43,807 - LGCN model - [INFO]: Epoch 392/1000, BPR Loss: 0.2360\n",
      "2024-11-20 16:55:44,260 - LGCN model - [INFO]: Epoch 393/1000, BPR Loss: 0.2360\n",
      "2024-11-20 16:55:44,674 - LGCN model - [INFO]: Epoch 394/1000, BPR Loss: 0.2360\n",
      "2024-11-20 16:55:44,991 - LGCN model - [INFO]: Epoch 395/1000, BPR Loss: 0.2359\n",
      "2024-11-20 16:55:45,296 - LGCN model - [INFO]: Epoch 396/1000, BPR Loss: 0.2359\n",
      "2024-11-20 16:55:45,659 - LGCN model - [INFO]: Epoch 397/1000, BPR Loss: 0.2358\n",
      "2024-11-20 16:55:46,137 - LGCN model - [INFO]: Epoch 398/1000, BPR Loss: 0.2358\n",
      "2024-11-20 16:55:46,639 - LGCN model - [INFO]: Epoch 399/1000, BPR Loss: 0.2358\n",
      "2024-11-20 16:55:47,187 - LGCN model - [INFO]: Epoch 400/1000, BPR Loss: 0.2357\n",
      "2024-11-20 16:55:47,687 - LGCN model - [INFO]: Epoch 401/1000, BPR Loss: 0.2357\n",
      "2024-11-20 16:55:48,206 - LGCN model - [INFO]: Epoch 402/1000, BPR Loss: 0.2357\n",
      "2024-11-20 16:55:48,709 - LGCN model - [INFO]: Epoch 403/1000, BPR Loss: 0.2356\n",
      "2024-11-20 16:55:49,225 - LGCN model - [INFO]: Epoch 404/1000, BPR Loss: 0.2356\n",
      "2024-11-20 16:55:49,760 - LGCN model - [INFO]: Epoch 405/1000, BPR Loss: 0.2356\n",
      "2024-11-20 16:55:50,257 - LGCN model - [INFO]: Epoch 406/1000, BPR Loss: 0.2355\n",
      "2024-11-20 16:55:50,760 - LGCN model - [INFO]: Epoch 407/1000, BPR Loss: 0.2355\n",
      "2024-11-20 16:55:51,271 - LGCN model - [INFO]: Epoch 408/1000, BPR Loss: 0.2355\n",
      "2024-11-20 16:55:51,825 - LGCN model - [INFO]: Epoch 409/1000, BPR Loss: 0.2354\n",
      "2024-11-20 16:55:52,254 - LGCN model - [INFO]: Epoch 410/1000, BPR Loss: 0.2354\n",
      "2024-11-20 16:55:52,624 - LGCN model - [INFO]: Epoch 411/1000, BPR Loss: 0.2353\n",
      "2024-11-20 16:55:53,026 - LGCN model - [INFO]: Epoch 412/1000, BPR Loss: 0.2353\n",
      "2024-11-20 16:55:53,624 - LGCN model - [INFO]: Epoch 413/1000, BPR Loss: 0.2353\n",
      "2024-11-20 16:55:54,156 - LGCN model - [INFO]: Epoch 414/1000, BPR Loss: 0.2352\n",
      "2024-11-20 16:55:54,659 - LGCN model - [INFO]: Epoch 415/1000, BPR Loss: 0.2352\n",
      "2024-11-20 16:55:55,126 - LGCN model - [INFO]: Epoch 416/1000, BPR Loss: 0.2352\n",
      "2024-11-20 16:55:55,654 - LGCN model - [INFO]: Epoch 417/1000, BPR Loss: 0.2351\n",
      "2024-11-20 16:55:56,174 - LGCN model - [INFO]: Epoch 418/1000, BPR Loss: 0.2351\n",
      "2024-11-20 16:55:56,673 - LGCN model - [INFO]: Epoch 419/1000, BPR Loss: 0.2350\n",
      "2024-11-20 16:55:57,122 - LGCN model - [INFO]: Epoch 420/1000, BPR Loss: 0.2350\n",
      "2024-11-20 16:55:57,658 - LGCN model - [INFO]: Epoch 421/1000, BPR Loss: 0.2350\n",
      "2024-11-20 16:55:58,113 - LGCN model - [INFO]: Epoch 422/1000, BPR Loss: 0.2349\n",
      "2024-11-20 16:55:58,671 - LGCN model - [INFO]: Epoch 423/1000, BPR Loss: 0.2349\n",
      "2024-11-20 16:55:59,143 - LGCN model - [INFO]: Epoch 424/1000, BPR Loss: 0.2349\n",
      "2024-11-20 16:55:59,614 - LGCN model - [INFO]: Epoch 425/1000, BPR Loss: 0.2348\n",
      "2024-11-20 16:56:00,107 - LGCN model - [INFO]: Epoch 426/1000, BPR Loss: 0.2348\n",
      "2024-11-20 16:56:00,560 - LGCN model - [INFO]: Epoch 427/1000, BPR Loss: 0.2348\n",
      "2024-11-20 16:56:00,904 - LGCN model - [INFO]: Epoch 428/1000, BPR Loss: 0.2347\n",
      "2024-11-20 16:56:01,226 - LGCN model - [INFO]: Epoch 429/1000, BPR Loss: 0.2347\n",
      "2024-11-20 16:56:01,557 - LGCN model - [INFO]: Epoch 430/1000, BPR Loss: 0.2346\n",
      "2024-11-20 16:56:02,082 - LGCN model - [INFO]: Epoch 431/1000, BPR Loss: 0.2346\n",
      "2024-11-20 16:56:02,474 - LGCN model - [INFO]: Epoch 432/1000, BPR Loss: 0.2346\n",
      "2024-11-20 16:56:03,075 - LGCN model - [INFO]: Epoch 433/1000, BPR Loss: 0.2345\n",
      "2024-11-20 16:56:03,543 - LGCN model - [INFO]: Epoch 434/1000, BPR Loss: 0.2345\n",
      "2024-11-20 16:56:04,070 - LGCN model - [INFO]: Epoch 435/1000, BPR Loss: 0.2345\n",
      "2024-11-20 16:56:04,570 - LGCN model - [INFO]: Epoch 436/1000, BPR Loss: 0.2344\n",
      "2024-11-20 16:56:04,977 - LGCN model - [INFO]: Epoch 437/1000, BPR Loss: 0.2344\n",
      "2024-11-20 16:56:05,487 - LGCN model - [INFO]: Epoch 438/1000, BPR Loss: 0.2343\n",
      "2024-11-20 16:56:05,956 - LGCN model - [INFO]: Epoch 439/1000, BPR Loss: 0.2343\n",
      "2024-11-20 16:56:06,439 - LGCN model - [INFO]: Epoch 440/1000, BPR Loss: 0.2343\n",
      "2024-11-20 16:56:06,922 - LGCN model - [INFO]: Epoch 441/1000, BPR Loss: 0.2342\n",
      "2024-11-20 16:56:07,305 - LGCN model - [INFO]: Epoch 442/1000, BPR Loss: 0.2342\n",
      "2024-11-20 16:56:07,873 - LGCN model - [INFO]: Epoch 443/1000, BPR Loss: 0.2342\n",
      "2024-11-20 16:56:08,254 - LGCN model - [INFO]: Epoch 444/1000, BPR Loss: 0.2341\n",
      "2024-11-20 16:56:08,770 - LGCN model - [INFO]: Epoch 445/1000, BPR Loss: 0.2341\n",
      "2024-11-20 16:56:09,271 - LGCN model - [INFO]: Epoch 446/1000, BPR Loss: 0.2341\n",
      "2024-11-20 16:56:09,637 - LGCN model - [INFO]: Epoch 447/1000, BPR Loss: 0.2340\n",
      "2024-11-20 16:56:10,024 - LGCN model - [INFO]: Epoch 448/1000, BPR Loss: 0.2340\n",
      "2024-11-20 16:56:10,375 - LGCN model - [INFO]: Epoch 449/1000, BPR Loss: 0.2339\n",
      "2024-11-20 16:56:10,790 - LGCN model - [INFO]: Epoch 450/1000, BPR Loss: 0.2339\n",
      "2024-11-20 16:56:11,255 - LGCN model - [INFO]: Epoch 451/1000, BPR Loss: 0.2339\n",
      "2024-11-20 16:56:11,725 - LGCN model - [INFO]: Epoch 452/1000, BPR Loss: 0.2338\n",
      "2024-11-20 16:56:12,204 - LGCN model - [INFO]: Epoch 453/1000, BPR Loss: 0.2338\n",
      "2024-11-20 16:56:12,704 - LGCN model - [INFO]: Epoch 454/1000, BPR Loss: 0.2338\n",
      "2024-11-20 16:56:13,158 - LGCN model - [INFO]: Epoch 455/1000, BPR Loss: 0.2337\n",
      "2024-11-20 16:56:13,626 - LGCN model - [INFO]: Epoch 456/1000, BPR Loss: 0.2337\n",
      "2024-11-20 16:56:14,203 - LGCN model - [INFO]: Epoch 457/1000, BPR Loss: 0.2337\n",
      "2024-11-20 16:56:14,654 - LGCN model - [INFO]: Epoch 458/1000, BPR Loss: 0.2336\n",
      "2024-11-20 16:56:15,106 - LGCN model - [INFO]: Epoch 459/1000, BPR Loss: 0.2336\n",
      "2024-11-20 16:56:15,571 - LGCN model - [INFO]: Epoch 460/1000, BPR Loss: 0.2336\n",
      "2024-11-20 16:56:16,048 - LGCN model - [INFO]: Epoch 461/1000, BPR Loss: 0.2335\n",
      "2024-11-20 16:56:16,458 - LGCN model - [INFO]: Epoch 462/1000, BPR Loss: 0.2335\n",
      "2024-11-20 16:56:16,986 - LGCN model - [INFO]: Epoch 463/1000, BPR Loss: 0.2335\n",
      "2024-11-20 16:56:17,457 - LGCN model - [INFO]: Epoch 464/1000, BPR Loss: 0.2334\n",
      "2024-11-20 16:56:17,977 - LGCN model - [INFO]: Epoch 465/1000, BPR Loss: 0.2334\n",
      "2024-11-20 16:56:18,454 - LGCN model - [INFO]: Epoch 466/1000, BPR Loss: 0.2334\n",
      "2024-11-20 16:56:18,957 - LGCN model - [INFO]: Epoch 467/1000, BPR Loss: 0.2333\n",
      "2024-11-20 16:56:19,357 - LGCN model - [INFO]: Epoch 468/1000, BPR Loss: 0.2333\n",
      "2024-11-20 16:56:19,673 - LGCN model - [INFO]: Epoch 469/1000, BPR Loss: 0.2333\n",
      "2024-11-20 16:56:19,970 - LGCN model - [INFO]: Epoch 470/1000, BPR Loss: 0.2332\n",
      "2024-11-20 16:56:20,437 - LGCN model - [INFO]: Epoch 471/1000, BPR Loss: 0.2332\n",
      "2024-11-20 16:56:20,856 - LGCN model - [INFO]: Epoch 472/1000, BPR Loss: 0.2332\n",
      "2024-11-20 16:56:21,388 - LGCN model - [INFO]: Epoch 473/1000, BPR Loss: 0.2331\n",
      "2024-11-20 16:56:21,820 - LGCN model - [INFO]: Epoch 474/1000, BPR Loss: 0.2331\n",
      "2024-11-20 16:56:22,341 - LGCN model - [INFO]: Epoch 475/1000, BPR Loss: 0.2331\n",
      "2024-11-20 16:56:22,804 - LGCN model - [INFO]: Epoch 476/1000, BPR Loss: 0.2330\n",
      "2024-11-20 16:56:23,237 - LGCN model - [INFO]: Epoch 477/1000, BPR Loss: 0.2330\n",
      "2024-11-20 16:56:23,690 - LGCN model - [INFO]: Epoch 478/1000, BPR Loss: 0.2330\n",
      "2024-11-20 16:56:24,100 - LGCN model - [INFO]: Epoch 479/1000, BPR Loss: 0.2329\n",
      "2024-11-20 16:56:24,624 - LGCN model - [INFO]: Epoch 480/1000, BPR Loss: 0.2329\n",
      "2024-11-20 16:56:25,024 - LGCN model - [INFO]: Epoch 481/1000, BPR Loss: 0.2329\n",
      "2024-11-20 16:56:25,488 - LGCN model - [INFO]: Epoch 482/1000, BPR Loss: 0.2328\n",
      "2024-11-20 16:56:25,960 - LGCN model - [INFO]: Epoch 483/1000, BPR Loss: 0.2328\n",
      "2024-11-20 16:56:26,403 - LGCN model - [INFO]: Epoch 484/1000, BPR Loss: 0.2328\n",
      "2024-11-20 16:56:26,860 - LGCN model - [INFO]: Epoch 485/1000, BPR Loss: 0.2327\n",
      "2024-11-20 16:56:27,320 - LGCN model - [INFO]: Epoch 486/1000, BPR Loss: 0.2327\n",
      "2024-11-20 16:56:27,841 - LGCN model - [INFO]: Epoch 487/1000, BPR Loss: 0.2327\n",
      "2024-11-20 16:56:28,241 - LGCN model - [INFO]: Epoch 488/1000, BPR Loss: 0.2327\n",
      "2024-11-20 16:56:28,585 - LGCN model - [INFO]: Epoch 489/1000, BPR Loss: 0.2326\n",
      "2024-11-20 16:56:28,954 - LGCN model - [INFO]: Epoch 490/1000, BPR Loss: 0.2326\n",
      "2024-11-20 16:56:29,299 - LGCN model - [INFO]: Epoch 491/1000, BPR Loss: 0.2326\n",
      "2024-11-20 16:56:29,746 - LGCN model - [INFO]: Epoch 492/1000, BPR Loss: 0.2325\n",
      "2024-11-20 16:56:30,159 - LGCN model - [INFO]: Epoch 493/1000, BPR Loss: 0.2325\n",
      "2024-11-20 16:56:30,640 - LGCN model - [INFO]: Epoch 494/1000, BPR Loss: 0.2325\n",
      "2024-11-20 16:56:31,070 - LGCN model - [INFO]: Epoch 495/1000, BPR Loss: 0.2325\n",
      "2024-11-20 16:56:31,570 - LGCN model - [INFO]: Epoch 496/1000, BPR Loss: 0.2324\n",
      "2024-11-20 16:56:31,996 - LGCN model - [INFO]: Epoch 497/1000, BPR Loss: 0.2324\n",
      "2024-11-20 16:56:32,420 - LGCN model - [INFO]: Epoch 498/1000, BPR Loss: 0.2324\n",
      "2024-11-20 16:56:32,822 - LGCN model - [INFO]: Epoch 499/1000, BPR Loss: 0.2323\n",
      "2024-11-20 16:56:33,270 - LGCN model - [INFO]: Epoch 500/1000, BPR Loss: 0.2323\n",
      "2024-11-20 16:56:33,704 - LGCN model - [INFO]: Epoch 501/1000, BPR Loss: 0.2323\n",
      "2024-11-20 16:56:34,156 - LGCN model - [INFO]: Epoch 502/1000, BPR Loss: 0.2323\n",
      "2024-11-20 16:56:34,654 - LGCN model - [INFO]: Epoch 503/1000, BPR Loss: 0.2322\n",
      "2024-11-20 16:56:35,043 - LGCN model - [INFO]: Epoch 504/1000, BPR Loss: 0.2322\n",
      "2024-11-20 16:56:35,543 - LGCN model - [INFO]: Epoch 505/1000, BPR Loss: 0.2322\n",
      "2024-11-20 16:56:35,976 - LGCN model - [INFO]: Epoch 506/1000, BPR Loss: 0.2322\n",
      "2024-11-20 16:56:36,409 - LGCN model - [INFO]: Epoch 507/1000, BPR Loss: 0.2321\n",
      "2024-11-20 16:56:36,920 - LGCN model - [INFO]: Epoch 508/1000, BPR Loss: 0.2321\n",
      "2024-11-20 16:56:37,320 - LGCN model - [INFO]: Epoch 509/1000, BPR Loss: 0.2321\n",
      "2024-11-20 16:56:37,723 - LGCN model - [INFO]: Epoch 510/1000, BPR Loss: 0.2321\n",
      "2024-11-20 16:56:38,057 - LGCN model - [INFO]: Epoch 511/1000, BPR Loss: 0.2320\n",
      "2024-11-20 16:56:38,353 - LGCN model - [INFO]: Epoch 512/1000, BPR Loss: 0.2320\n",
      "2024-11-20 16:56:38,758 - LGCN model - [INFO]: Epoch 513/1000, BPR Loss: 0.2320\n",
      "2024-11-20 16:56:39,144 - LGCN model - [INFO]: Epoch 514/1000, BPR Loss: 0.2320\n",
      "2024-11-20 16:56:39,637 - LGCN model - [INFO]: Epoch 515/1000, BPR Loss: 0.2319\n",
      "2024-11-20 16:56:40,056 - LGCN model - [INFO]: Epoch 516/1000, BPR Loss: 0.2319\n",
      "2024-11-20 16:56:40,590 - LGCN model - [INFO]: Epoch 517/1000, BPR Loss: 0.2319\n",
      "2024-11-20 16:56:41,020 - LGCN model - [INFO]: Epoch 518/1000, BPR Loss: 0.2319\n",
      "2024-11-20 16:56:41,500 - LGCN model - [INFO]: Epoch 519/1000, BPR Loss: 0.2319\n",
      "2024-11-20 16:56:41,970 - LGCN model - [INFO]: Epoch 520/1000, BPR Loss: 0.2318\n",
      "2024-11-20 16:56:42,347 - LGCN model - [INFO]: Epoch 521/1000, BPR Loss: 0.2318\n",
      "2024-11-20 16:56:42,806 - LGCN model - [INFO]: Epoch 522/1000, BPR Loss: 0.2318\n",
      "2024-11-20 16:56:43,226 - LGCN model - [INFO]: Epoch 523/1000, BPR Loss: 0.2318\n",
      "2024-11-20 16:56:43,637 - LGCN model - [INFO]: Epoch 524/1000, BPR Loss: 0.2317\n",
      "2024-11-20 16:56:44,049 - LGCN model - [INFO]: Epoch 525/1000, BPR Loss: 0.2317\n",
      "2024-11-20 16:56:44,522 - LGCN model - [INFO]: Epoch 526/1000, BPR Loss: 0.2317\n",
      "2024-11-20 16:56:44,970 - LGCN model - [INFO]: Epoch 527/1000, BPR Loss: 0.2317\n",
      "2024-11-20 16:56:45,442 - LGCN model - [INFO]: Epoch 528/1000, BPR Loss: 0.2317\n",
      "2024-11-20 16:56:45,904 - LGCN model - [INFO]: Epoch 529/1000, BPR Loss: 0.2316\n",
      "2024-11-20 16:56:46,354 - LGCN model - [INFO]: Epoch 530/1000, BPR Loss: 0.2316\n",
      "2024-11-20 16:56:46,772 - LGCN model - [INFO]: Epoch 531/1000, BPR Loss: 0.2316\n",
      "2024-11-20 16:56:47,154 - LGCN model - [INFO]: Epoch 532/1000, BPR Loss: 0.2316\n",
      "2024-11-20 16:56:47,493 - LGCN model - [INFO]: Epoch 533/1000, BPR Loss: 0.2316\n",
      "2024-11-20 16:56:47,841 - LGCN model - [INFO]: Epoch 534/1000, BPR Loss: 0.2316\n",
      "2024-11-20 16:56:48,290 - LGCN model - [INFO]: Epoch 535/1000, BPR Loss: 0.2315\n",
      "2024-11-20 16:56:48,724 - LGCN model - [INFO]: Epoch 536/1000, BPR Loss: 0.2315\n",
      "2024-11-20 16:56:49,161 - LGCN model - [INFO]: Epoch 537/1000, BPR Loss: 0.2315\n",
      "2024-11-20 16:56:49,637 - LGCN model - [INFO]: Epoch 538/1000, BPR Loss: 0.2315\n",
      "2024-11-20 16:56:50,057 - LGCN model - [INFO]: Epoch 539/1000, BPR Loss: 0.2315\n",
      "2024-11-20 16:56:50,519 - LGCN model - [INFO]: Epoch 540/1000, BPR Loss: 0.2315\n",
      "2024-11-20 16:56:50,971 - LGCN model - [INFO]: Epoch 541/1000, BPR Loss: 0.2314\n",
      "2024-11-20 16:56:51,471 - LGCN model - [INFO]: Epoch 542/1000, BPR Loss: 0.2314\n",
      "2024-11-20 16:56:51,920 - LGCN model - [INFO]: Epoch 543/1000, BPR Loss: 0.2314\n",
      "2024-11-20 16:56:52,403 - LGCN model - [INFO]: Epoch 544/1000, BPR Loss: 0.2314\n",
      "2024-11-20 16:56:52,872 - LGCN model - [INFO]: Epoch 545/1000, BPR Loss: 0.2314\n",
      "2024-11-20 16:56:53,274 - LGCN model - [INFO]: Epoch 546/1000, BPR Loss: 0.2314\n",
      "2024-11-20 16:56:53,820 - LGCN model - [INFO]: Epoch 547/1000, BPR Loss: 0.2313\n",
      "2024-11-20 16:56:54,253 - LGCN model - [INFO]: Epoch 548/1000, BPR Loss: 0.2313\n",
      "2024-11-20 16:56:54,705 - LGCN model - [INFO]: Epoch 549/1000, BPR Loss: 0.2313\n",
      "2024-11-20 16:56:55,136 - LGCN model - [INFO]: Epoch 550/1000, BPR Loss: 0.2313\n",
      "2024-11-20 16:56:55,441 - LGCN model - [INFO]: Epoch 551/1000, BPR Loss: 0.2313\n",
      "2024-11-20 16:56:55,737 - LGCN model - [INFO]: Epoch 552/1000, BPR Loss: 0.2313\n",
      "2024-11-20 16:56:56,132 - LGCN model - [INFO]: Epoch 553/1000, BPR Loss: 0.2312\n",
      "2024-11-20 16:56:56,653 - LGCN model - [INFO]: Epoch 554/1000, BPR Loss: 0.2312\n",
      "2024-11-20 16:56:57,138 - LGCN model - [INFO]: Epoch 555/1000, BPR Loss: 0.2312\n",
      "2024-11-20 16:56:57,698 - LGCN model - [INFO]: Epoch 556/1000, BPR Loss: 0.2312\n",
      "2024-11-20 16:56:58,148 - LGCN model - [INFO]: Epoch 557/1000, BPR Loss: 0.2312\n",
      "2024-11-20 16:56:58,603 - LGCN model - [INFO]: Epoch 558/1000, BPR Loss: 0.2312\n",
      "2024-11-20 16:56:59,099 - LGCN model - [INFO]: Epoch 559/1000, BPR Loss: 0.2312\n",
      "2024-11-20 16:56:59,457 - LGCN model - [INFO]: Epoch 560/1000, BPR Loss: 0.2312\n",
      "2024-11-20 16:57:00,020 - LGCN model - [INFO]: Epoch 561/1000, BPR Loss: 0.2311\n",
      "2024-11-20 16:57:00,470 - LGCN model - [INFO]: Epoch 562/1000, BPR Loss: 0.2311\n",
      "2024-11-20 16:57:00,870 - LGCN model - [INFO]: Epoch 563/1000, BPR Loss: 0.2311\n",
      "2024-11-20 16:57:01,403 - LGCN model - [INFO]: Epoch 564/1000, BPR Loss: 0.2311\n",
      "2024-11-20 16:57:01,855 - LGCN model - [INFO]: Epoch 565/1000, BPR Loss: 0.2311\n",
      "2024-11-20 16:57:02,323 - LGCN model - [INFO]: Epoch 566/1000, BPR Loss: 0.2311\n",
      "2024-11-20 16:57:02,805 - LGCN model - [INFO]: Epoch 567/1000, BPR Loss: 0.2311\n",
      "2024-11-20 16:57:03,174 - LGCN model - [INFO]: Epoch 568/1000, BPR Loss: 0.2311\n",
      "2024-11-20 16:57:03,551 - LGCN model - [INFO]: Epoch 569/1000, BPR Loss: 0.2310\n",
      "2024-11-20 16:57:03,904 - LGCN model - [INFO]: Epoch 570/1000, BPR Loss: 0.2310\n",
      "2024-11-20 16:57:04,241 - LGCN model - [INFO]: Epoch 571/1000, BPR Loss: 0.2310\n",
      "2024-11-20 16:57:04,753 - LGCN model - [INFO]: Epoch 572/1000, BPR Loss: 0.2310\n",
      "2024-11-20 16:57:05,275 - LGCN model - [INFO]: Epoch 573/1000, BPR Loss: 0.2310\n",
      "2024-11-20 16:57:05,723 - LGCN model - [INFO]: Epoch 574/1000, BPR Loss: 0.2310\n",
      "2024-11-20 16:57:06,136 - LGCN model - [INFO]: Epoch 575/1000, BPR Loss: 0.2310\n",
      "2024-11-20 16:57:06,686 - LGCN model - [INFO]: Epoch 576/1000, BPR Loss: 0.2310\n",
      "2024-11-20 16:57:07,179 - LGCN model - [INFO]: Epoch 577/1000, BPR Loss: 0.2309\n",
      "2024-11-20 16:57:07,625 - LGCN model - [INFO]: Epoch 578/1000, BPR Loss: 0.2309\n",
      "2024-11-20 16:57:08,124 - LGCN model - [INFO]: Epoch 579/1000, BPR Loss: 0.2309\n",
      "2024-11-20 16:57:08,606 - LGCN model - [INFO]: Epoch 580/1000, BPR Loss: 0.2309\n",
      "2024-11-20 16:57:09,157 - LGCN model - [INFO]: Epoch 581/1000, BPR Loss: 0.2309\n",
      "2024-11-20 16:57:09,770 - LGCN model - [INFO]: Epoch 582/1000, BPR Loss: 0.2309\n",
      "2024-11-20 16:57:10,273 - LGCN model - [INFO]: Epoch 583/1000, BPR Loss: 0.2309\n",
      "2024-11-20 16:57:10,601 - LGCN model - [INFO]: Epoch 584/1000, BPR Loss: 0.2309\n",
      "2024-11-20 16:57:11,003 - LGCN model - [INFO]: Epoch 585/1000, BPR Loss: 0.2309\n",
      "2024-11-20 16:57:11,459 - LGCN model - [INFO]: Epoch 586/1000, BPR Loss: 0.2309\n",
      "2024-11-20 16:57:12,006 - LGCN model - [INFO]: Epoch 587/1000, BPR Loss: 0.2308\n",
      "2024-11-20 16:57:12,474 - LGCN model - [INFO]: Epoch 588/1000, BPR Loss: 0.2308\n",
      "2024-11-20 16:57:13,001 - LGCN model - [INFO]: Epoch 589/1000, BPR Loss: 0.2308\n",
      "2024-11-20 16:57:13,514 - LGCN model - [INFO]: Epoch 590/1000, BPR Loss: 0.2308\n",
      "2024-11-20 16:57:14,039 - LGCN model - [INFO]: Epoch 591/1000, BPR Loss: 0.2308\n",
      "2024-11-20 16:57:14,530 - LGCN model - [INFO]: Epoch 592/1000, BPR Loss: 0.2308\n",
      "2024-11-20 16:57:15,019 - LGCN model - [INFO]: Epoch 593/1000, BPR Loss: 0.2308\n",
      "2024-11-20 16:57:15,503 - LGCN model - [INFO]: Epoch 594/1000, BPR Loss: 0.2308\n",
      "2024-11-20 16:57:15,986 - LGCN model - [INFO]: Epoch 595/1000, BPR Loss: 0.2308\n",
      "2024-11-20 16:57:16,437 - LGCN model - [INFO]: Epoch 596/1000, BPR Loss: 0.2308\n",
      "2024-11-20 16:57:16,774 - LGCN model - [INFO]: Epoch 597/1000, BPR Loss: 0.2308\n",
      "2024-11-20 16:57:17,102 - LGCN model - [INFO]: Epoch 598/1000, BPR Loss: 0.2307\n",
      "2024-11-20 16:57:17,489 - LGCN model - [INFO]: Epoch 599/1000, BPR Loss: 0.2307\n",
      "2024-11-20 16:57:17,958 - LGCN model - [INFO]: Epoch 600/1000, BPR Loss: 0.2307\n",
      "2024-11-20 16:57:18,437 - LGCN model - [INFO]: Epoch 601/1000, BPR Loss: 0.2307\n",
      "2024-11-20 16:57:18,904 - LGCN model - [INFO]: Epoch 602/1000, BPR Loss: 0.2307\n",
      "2024-11-20 16:57:19,458 - LGCN model - [INFO]: Epoch 603/1000, BPR Loss: 0.2307\n",
      "2024-11-20 16:57:20,020 - LGCN model - [INFO]: Epoch 604/1000, BPR Loss: 0.2307\n",
      "2024-11-20 16:57:20,522 - LGCN model - [INFO]: Epoch 605/1000, BPR Loss: 0.2307\n",
      "2024-11-20 16:57:21,003 - LGCN model - [INFO]: Epoch 606/1000, BPR Loss: 0.2307\n",
      "2024-11-20 16:57:21,503 - LGCN model - [INFO]: Epoch 607/1000, BPR Loss: 0.2307\n",
      "2024-11-20 16:57:21,971 - LGCN model - [INFO]: Epoch 608/1000, BPR Loss: 0.2307\n",
      "2024-11-20 16:57:22,453 - LGCN model - [INFO]: Epoch 609/1000, BPR Loss: 0.2307\n",
      "2024-11-20 16:57:22,820 - LGCN model - [INFO]: Epoch 610/1000, BPR Loss: 0.2306\n",
      "2024-11-20 16:57:23,153 - LGCN model - [INFO]: Epoch 611/1000, BPR Loss: 0.2306\n",
      "2024-11-20 16:57:23,544 - LGCN model - [INFO]: Epoch 612/1000, BPR Loss: 0.2306\n",
      "2024-11-20 16:57:24,077 - LGCN model - [INFO]: Epoch 613/1000, BPR Loss: 0.2306\n",
      "2024-11-20 16:57:24,602 - LGCN model - [INFO]: Epoch 614/1000, BPR Loss: 0.2306\n",
      "2024-11-20 16:57:25,140 - LGCN model - [INFO]: Epoch 615/1000, BPR Loss: 0.2306\n",
      "2024-11-20 16:57:25,718 - LGCN model - [INFO]: Epoch 616/1000, BPR Loss: 0.2306\n",
      "2024-11-20 16:57:26,253 - LGCN model - [INFO]: Epoch 617/1000, BPR Loss: 0.2306\n",
      "2024-11-20 16:57:26,787 - LGCN model - [INFO]: Epoch 618/1000, BPR Loss: 0.2306\n",
      "2024-11-20 16:57:27,295 - LGCN model - [INFO]: Epoch 619/1000, BPR Loss: 0.2306\n",
      "2024-11-20 16:57:27,803 - LGCN model - [INFO]: Epoch 620/1000, BPR Loss: 0.2306\n",
      "2024-11-20 16:57:28,339 - LGCN model - [INFO]: Epoch 621/1000, BPR Loss: 0.2306\n",
      "2024-11-20 16:57:28,836 - LGCN model - [INFO]: Epoch 622/1000, BPR Loss: 0.2306\n",
      "2024-11-20 16:57:29,193 - LGCN model - [INFO]: Epoch 623/1000, BPR Loss: 0.2306\n",
      "2024-11-20 16:57:29,521 - LGCN model - [INFO]: Epoch 624/1000, BPR Loss: 0.2305\n",
      "2024-11-20 16:57:29,956 - LGCN model - [INFO]: Epoch 625/1000, BPR Loss: 0.2305\n",
      "2024-11-20 16:57:30,470 - LGCN model - [INFO]: Epoch 626/1000, BPR Loss: 0.2305\n",
      "2024-11-20 16:57:31,021 - LGCN model - [INFO]: Epoch 627/1000, BPR Loss: 0.2305\n",
      "2024-11-20 16:57:31,598 - LGCN model - [INFO]: Epoch 628/1000, BPR Loss: 0.2305\n",
      "2024-11-20 16:57:32,172 - LGCN model - [INFO]: Epoch 629/1000, BPR Loss: 0.2305\n",
      "2024-11-20 16:57:32,732 - LGCN model - [INFO]: Epoch 630/1000, BPR Loss: 0.2305\n",
      "2024-11-20 16:57:33,309 - LGCN model - [INFO]: Epoch 631/1000, BPR Loss: 0.2305\n",
      "2024-11-20 16:57:33,822 - LGCN model - [INFO]: Epoch 632/1000, BPR Loss: 0.2305\n",
      "2024-11-20 16:57:34,416 - LGCN model - [INFO]: Epoch 633/1000, BPR Loss: 0.2305\n",
      "2024-11-20 16:57:35,026 - LGCN model - [INFO]: Epoch 634/1000, BPR Loss: 0.2305\n",
      "2024-11-20 16:57:35,527 - LGCN model - [INFO]: Epoch 635/1000, BPR Loss: 0.2305\n",
      "2024-11-20 16:57:35,900 - LGCN model - [INFO]: Epoch 636/1000, BPR Loss: 0.2305\n",
      "2024-11-20 16:57:36,271 - LGCN model - [INFO]: Epoch 637/1000, BPR Loss: 0.2305\n",
      "2024-11-20 16:57:36,690 - LGCN model - [INFO]: Epoch 638/1000, BPR Loss: 0.2305\n",
      "2024-11-20 16:57:37,218 - LGCN model - [INFO]: Epoch 639/1000, BPR Loss: 0.2304\n",
      "2024-11-20 16:57:37,700 - LGCN model - [INFO]: Epoch 640/1000, BPR Loss: 0.2304\n",
      "2024-11-20 16:57:38,071 - LGCN model - [INFO]: Epoch 641/1000, BPR Loss: 0.2304\n",
      "2024-11-20 16:57:38,611 - LGCN model - [INFO]: Epoch 642/1000, BPR Loss: 0.2304\n",
      "2024-11-20 16:57:39,010 - LGCN model - [INFO]: Epoch 643/1000, BPR Loss: 0.2304\n",
      "2024-11-20 16:57:39,528 - LGCN model - [INFO]: Epoch 644/1000, BPR Loss: 0.2304\n",
      "2024-11-20 16:57:40,016 - LGCN model - [INFO]: Epoch 645/1000, BPR Loss: 0.2304\n",
      "2024-11-20 16:57:40,558 - LGCN model - [INFO]: Epoch 646/1000, BPR Loss: 0.2304\n",
      "2024-11-20 16:57:41,050 - LGCN model - [INFO]: Epoch 647/1000, BPR Loss: 0.2304\n",
      "2024-11-20 16:57:41,540 - LGCN model - [INFO]: Epoch 648/1000, BPR Loss: 0.2304\n",
      "2024-11-20 16:57:42,025 - LGCN model - [INFO]: Epoch 649/1000, BPR Loss: 0.2304\n",
      "2024-11-20 16:57:42,474 - LGCN model - [INFO]: Epoch 650/1000, BPR Loss: 0.2304\n",
      "2024-11-20 16:57:42,820 - LGCN model - [INFO]: Epoch 651/1000, BPR Loss: 0.2304\n",
      "2024-11-20 16:57:43,136 - LGCN model - [INFO]: Epoch 652/1000, BPR Loss: 0.2304\n",
      "2024-11-20 16:57:43,513 - LGCN model - [INFO]: Epoch 653/1000, BPR Loss: 0.2304\n",
      "2024-11-20 16:57:44,044 - LGCN model - [INFO]: Epoch 654/1000, BPR Loss: 0.2304\n",
      "2024-11-20 16:57:44,636 - LGCN model - [INFO]: Epoch 655/1000, BPR Loss: 0.2304\n",
      "2024-11-20 16:57:45,142 - LGCN model - [INFO]: Epoch 656/1000, BPR Loss: 0.2303\n",
      "2024-11-20 16:57:45,627 - LGCN model - [INFO]: Epoch 657/1000, BPR Loss: 0.2303\n",
      "2024-11-20 16:57:46,103 - LGCN model - [INFO]: Epoch 658/1000, BPR Loss: 0.2303\n",
      "2024-11-20 16:57:46,603 - LGCN model - [INFO]: Epoch 659/1000, BPR Loss: 0.2303\n",
      "2024-11-20 16:57:47,069 - LGCN model - [INFO]: Epoch 660/1000, BPR Loss: 0.2303\n",
      "2024-11-20 16:57:47,553 - LGCN model - [INFO]: Epoch 661/1000, BPR Loss: 0.2303\n",
      "2024-11-20 16:57:47,953 - LGCN model - [INFO]: Epoch 662/1000, BPR Loss: 0.2303\n",
      "2024-11-20 16:57:48,472 - LGCN model - [INFO]: Epoch 663/1000, BPR Loss: 0.2303\n",
      "2024-11-20 16:57:48,936 - LGCN model - [INFO]: Epoch 664/1000, BPR Loss: 0.2303\n",
      "2024-11-20 16:57:49,393 - LGCN model - [INFO]: Epoch 665/1000, BPR Loss: 0.2303\n",
      "2024-11-20 16:57:49,769 - LGCN model - [INFO]: Epoch 666/1000, BPR Loss: 0.2303\n",
      "2024-11-20 16:57:50,073 - LGCN model - [INFO]: Epoch 667/1000, BPR Loss: 0.2303\n",
      "2024-11-20 16:57:50,419 - LGCN model - [INFO]: Epoch 668/1000, BPR Loss: 0.2303\n",
      "2024-11-20 16:57:50,926 - LGCN model - [INFO]: Epoch 669/1000, BPR Loss: 0.2303\n",
      "2024-11-20 16:57:51,411 - LGCN model - [INFO]: Epoch 670/1000, BPR Loss: 0.2303\n",
      "2024-11-20 16:57:51,899 - LGCN model - [INFO]: Epoch 671/1000, BPR Loss: 0.2303\n",
      "2024-11-20 16:57:52,398 - LGCN model - [INFO]: Epoch 672/1000, BPR Loss: 0.2303\n",
      "2024-11-20 16:57:52,872 - LGCN model - [INFO]: Epoch 673/1000, BPR Loss: 0.2303\n",
      "2024-11-20 16:57:53,354 - LGCN model - [INFO]: Epoch 674/1000, BPR Loss: 0.2302\n",
      "2024-11-20 16:57:53,843 - LGCN model - [INFO]: Epoch 675/1000, BPR Loss: 0.2302\n",
      "2024-11-20 16:57:54,328 - LGCN model - [INFO]: Epoch 676/1000, BPR Loss: 0.2302\n",
      "2024-11-20 16:57:54,823 - LGCN model - [INFO]: Epoch 677/1000, BPR Loss: 0.2302\n",
      "2024-11-20 16:57:55,304 - LGCN model - [INFO]: Epoch 678/1000, BPR Loss: 0.2302\n",
      "2024-11-20 16:57:55,755 - LGCN model - [INFO]: Epoch 679/1000, BPR Loss: 0.2302\n",
      "2024-11-20 16:57:56,303 - LGCN model - [INFO]: Epoch 680/1000, BPR Loss: 0.2302\n",
      "2024-11-20 16:57:56,742 - LGCN model - [INFO]: Epoch 681/1000, BPR Loss: 0.2302\n",
      "2024-11-20 16:57:57,198 - LGCN model - [INFO]: Epoch 682/1000, BPR Loss: 0.2302\n",
      "2024-11-20 16:57:57,590 - LGCN model - [INFO]: Epoch 683/1000, BPR Loss: 0.2302\n",
      "2024-11-20 16:57:57,921 - LGCN model - [INFO]: Epoch 684/1000, BPR Loss: 0.2302\n",
      "2024-11-20 16:57:58,288 - LGCN model - [INFO]: Epoch 685/1000, BPR Loss: 0.2302\n",
      "2024-11-20 16:57:58,769 - LGCN model - [INFO]: Epoch 686/1000, BPR Loss: 0.2302\n",
      "2024-11-20 16:57:59,295 - LGCN model - [INFO]: Epoch 687/1000, BPR Loss: 0.2302\n",
      "2024-11-20 16:57:59,796 - LGCN model - [INFO]: Epoch 688/1000, BPR Loss: 0.2302\n",
      "2024-11-20 16:58:00,337 - LGCN model - [INFO]: Epoch 689/1000, BPR Loss: 0.2302\n",
      "2024-11-20 16:58:00,837 - LGCN model - [INFO]: Epoch 690/1000, BPR Loss: 0.2302\n",
      "2024-11-20 16:58:01,345 - LGCN model - [INFO]: Epoch 691/1000, BPR Loss: 0.2302\n",
      "2024-11-20 16:58:01,853 - LGCN model - [INFO]: Epoch 692/1000, BPR Loss: 0.2302\n",
      "2024-11-20 16:58:02,290 - LGCN model - [INFO]: Epoch 693/1000, BPR Loss: 0.2302\n",
      "2024-11-20 16:58:02,785 - LGCN model - [INFO]: Epoch 694/1000, BPR Loss: 0.2301\n",
      "2024-11-20 16:58:03,292 - LGCN model - [INFO]: Epoch 695/1000, BPR Loss: 0.2301\n",
      "2024-11-20 16:58:03,655 - LGCN model - [INFO]: Epoch 696/1000, BPR Loss: 0.2301\n",
      "2024-11-20 16:58:03,973 - LGCN model - [INFO]: Epoch 697/1000, BPR Loss: 0.2301\n",
      "2024-11-20 16:58:04,393 - LGCN model - [INFO]: Epoch 698/1000, BPR Loss: 0.2301\n",
      "2024-11-20 16:58:04,871 - LGCN model - [INFO]: Epoch 699/1000, BPR Loss: 0.2301\n",
      "2024-11-20 16:58:05,356 - LGCN model - [INFO]: Epoch 700/1000, BPR Loss: 0.2301\n",
      "2024-11-20 16:58:05,827 - LGCN model - [INFO]: Epoch 701/1000, BPR Loss: 0.2301\n",
      "2024-11-20 16:58:06,356 - LGCN model - [INFO]: Epoch 702/1000, BPR Loss: 0.2301\n",
      "2024-11-20 16:58:06,853 - LGCN model - [INFO]: Epoch 703/1000, BPR Loss: 0.2301\n",
      "2024-11-20 16:58:07,319 - LGCN model - [INFO]: Epoch 704/1000, BPR Loss: 0.2301\n",
      "2024-11-20 16:58:07,836 - LGCN model - [INFO]: Epoch 705/1000, BPR Loss: 0.2301\n",
      "2024-11-20 16:58:08,352 - LGCN model - [INFO]: Epoch 706/1000, BPR Loss: 0.2301\n",
      "2024-11-20 16:58:08,907 - LGCN model - [INFO]: Epoch 707/1000, BPR Loss: 0.2301\n",
      "2024-11-20 16:58:09,336 - LGCN model - [INFO]: Epoch 708/1000, BPR Loss: 0.2301\n",
      "2024-11-20 16:58:09,655 - LGCN model - [INFO]: Epoch 709/1000, BPR Loss: 0.2301\n",
      "2024-11-20 16:58:09,955 - LGCN model - [INFO]: Epoch 710/1000, BPR Loss: 0.2301\n",
      "2024-11-20 16:58:10,310 - LGCN model - [INFO]: Epoch 711/1000, BPR Loss: 0.2301\n",
      "2024-11-20 16:58:10,787 - LGCN model - [INFO]: Epoch 712/1000, BPR Loss: 0.2301\n",
      "2024-11-20 16:58:11,237 - LGCN model - [INFO]: Epoch 713/1000, BPR Loss: 0.2301\n",
      "2024-11-20 16:58:11,770 - LGCN model - [INFO]: Epoch 714/1000, BPR Loss: 0.2301\n",
      "2024-11-20 16:58:12,319 - LGCN model - [INFO]: Epoch 715/1000, BPR Loss: 0.2300\n",
      "2024-11-20 16:58:12,803 - LGCN model - [INFO]: Epoch 716/1000, BPR Loss: 0.2300\n",
      "2024-11-20 16:58:13,319 - LGCN model - [INFO]: Epoch 717/1000, BPR Loss: 0.2300\n",
      "2024-11-20 16:58:13,836 - LGCN model - [INFO]: Epoch 718/1000, BPR Loss: 0.2300\n",
      "2024-11-20 16:58:14,360 - LGCN model - [INFO]: Epoch 719/1000, BPR Loss: 0.2300\n",
      "2024-11-20 16:58:14,819 - LGCN model - [INFO]: Epoch 720/1000, BPR Loss: 0.2300\n",
      "2024-11-20 16:58:15,320 - LGCN model - [INFO]: Epoch 721/1000, BPR Loss: 0.2300\n",
      "2024-11-20 16:58:15,695 - LGCN model - [INFO]: Epoch 722/1000, BPR Loss: 0.2300\n",
      "2024-11-20 16:58:16,019 - LGCN model - [INFO]: Epoch 723/1000, BPR Loss: 0.2300\n",
      "2024-11-20 16:58:16,424 - LGCN model - [INFO]: Epoch 724/1000, BPR Loss: 0.2300\n",
      "2024-11-20 16:58:16,932 - LGCN model - [INFO]: Epoch 725/1000, BPR Loss: 0.2300\n",
      "2024-11-20 16:58:17,423 - LGCN model - [INFO]: Epoch 726/1000, BPR Loss: 0.2300\n",
      "2024-11-20 16:58:17,903 - LGCN model - [INFO]: Epoch 727/1000, BPR Loss: 0.2300\n",
      "2024-11-20 16:58:18,416 - LGCN model - [INFO]: Epoch 728/1000, BPR Loss: 0.2300\n",
      "2024-11-20 16:58:18,969 - LGCN model - [INFO]: Epoch 729/1000, BPR Loss: 0.2300\n",
      "2024-11-20 16:58:19,493 - LGCN model - [INFO]: Epoch 730/1000, BPR Loss: 0.2300\n",
      "2024-11-20 16:58:20,106 - LGCN model - [INFO]: Epoch 731/1000, BPR Loss: 0.2300\n",
      "2024-11-20 16:58:20,592 - LGCN model - [INFO]: Epoch 732/1000, BPR Loss: 0.2300\n",
      "2024-11-20 16:58:21,169 - LGCN model - [INFO]: Epoch 733/1000, BPR Loss: 0.2300\n",
      "2024-11-20 16:58:21,655 - LGCN model - [INFO]: Epoch 734/1000, BPR Loss: 0.2300\n",
      "2024-11-20 16:58:22,035 - LGCN model - [INFO]: Epoch 735/1000, BPR Loss: 0.2300\n",
      "2024-11-20 16:58:22,400 - LGCN model - [INFO]: Epoch 736/1000, BPR Loss: 0.2300\n",
      "2024-11-20 16:58:22,925 - LGCN model - [INFO]: Epoch 737/1000, BPR Loss: 0.2300\n",
      "2024-11-20 16:58:23,494 - LGCN model - [INFO]: Epoch 738/1000, BPR Loss: 0.2299\n",
      "2024-11-20 16:58:24,092 - LGCN model - [INFO]: Epoch 739/1000, BPR Loss: 0.2299\n",
      "2024-11-20 16:58:24,623 - LGCN model - [INFO]: Epoch 740/1000, BPR Loss: 0.2299\n",
      "2024-11-20 16:58:25,125 - LGCN model - [INFO]: Epoch 741/1000, BPR Loss: 0.2299\n",
      "2024-11-20 16:58:25,594 - LGCN model - [INFO]: Epoch 742/1000, BPR Loss: 0.2299\n",
      "2024-11-20 16:58:26,138 - LGCN model - [INFO]: Epoch 743/1000, BPR Loss: 0.2299\n",
      "2024-11-20 16:58:26,599 - LGCN model - [INFO]: Epoch 744/1000, BPR Loss: 0.2299\n",
      "2024-11-20 16:58:27,032 - LGCN model - [INFO]: Epoch 745/1000, BPR Loss: 0.2299\n",
      "2024-11-20 16:58:27,502 - LGCN model - [INFO]: Epoch 746/1000, BPR Loss: 0.2299\n",
      "2024-11-20 16:58:27,958 - LGCN model - [INFO]: Epoch 747/1000, BPR Loss: 0.2299\n",
      "2024-11-20 16:58:28,419 - LGCN model - [INFO]: Epoch 748/1000, BPR Loss: 0.2299\n",
      "2024-11-20 16:58:28,877 - LGCN model - [INFO]: Epoch 749/1000, BPR Loss: 0.2299\n",
      "2024-11-20 16:58:29,344 - LGCN model - [INFO]: Epoch 750/1000, BPR Loss: 0.2299\n",
      "2024-11-20 16:58:29,835 - LGCN model - [INFO]: Epoch 751/1000, BPR Loss: 0.2299\n",
      "2024-11-20 16:58:30,202 - LGCN model - [INFO]: Epoch 752/1000, BPR Loss: 0.2299\n",
      "2024-11-20 16:58:30,597 - LGCN model - [INFO]: Epoch 753/1000, BPR Loss: 0.2299\n",
      "2024-11-20 16:58:30,936 - LGCN model - [INFO]: Epoch 754/1000, BPR Loss: 0.2299\n",
      "2024-11-20 16:58:31,255 - LGCN model - [INFO]: Epoch 755/1000, BPR Loss: 0.2299\n",
      "2024-11-20 16:58:31,711 - LGCN model - [INFO]: Epoch 756/1000, BPR Loss: 0.2299\n",
      "2024-11-20 16:58:32,185 - LGCN model - [INFO]: Epoch 757/1000, BPR Loss: 0.2299\n",
      "2024-11-20 16:58:32,652 - LGCN model - [INFO]: Epoch 758/1000, BPR Loss: 0.2299\n",
      "2024-11-20 16:58:33,023 - LGCN model - [INFO]: Epoch 759/1000, BPR Loss: 0.2299\n",
      "2024-11-20 16:58:33,619 - LGCN model - [INFO]: Epoch 760/1000, BPR Loss: 0.2299\n",
      "2024-11-20 16:58:34,069 - LGCN model - [INFO]: Epoch 761/1000, BPR Loss: 0.2299\n",
      "2024-11-20 16:58:34,554 - LGCN model - [INFO]: Epoch 762/1000, BPR Loss: 0.2299\n",
      "2024-11-20 16:58:35,020 - LGCN model - [INFO]: Epoch 763/1000, BPR Loss: 0.2298\n",
      "2024-11-20 16:58:35,385 - LGCN model - [INFO]: Epoch 764/1000, BPR Loss: 0.2298\n",
      "2024-11-20 16:58:35,919 - LGCN model - [INFO]: Epoch 765/1000, BPR Loss: 0.2298\n",
      "2024-11-20 16:58:36,366 - LGCN model - [INFO]: Epoch 766/1000, BPR Loss: 0.2298\n",
      "2024-11-20 16:58:36,824 - LGCN model - [INFO]: Epoch 767/1000, BPR Loss: 0.2298\n",
      "2024-11-20 16:58:37,279 - LGCN model - [INFO]: Epoch 768/1000, BPR Loss: 0.2298\n",
      "2024-11-20 16:58:37,698 - LGCN model - [INFO]: Epoch 769/1000, BPR Loss: 0.2298\n",
      "2024-11-20 16:58:38,202 - LGCN model - [INFO]: Epoch 770/1000, BPR Loss: 0.2298\n",
      "2024-11-20 16:58:38,650 - LGCN model - [INFO]: Epoch 771/1000, BPR Loss: 0.2298\n",
      "2024-11-20 16:58:39,102 - LGCN model - [INFO]: Epoch 772/1000, BPR Loss: 0.2298\n",
      "2024-11-20 16:58:39,458 - LGCN model - [INFO]: Epoch 773/1000, BPR Loss: 0.2298\n",
      "2024-11-20 16:58:39,772 - LGCN model - [INFO]: Epoch 774/1000, BPR Loss: 0.2298\n",
      "2024-11-20 16:58:40,080 - LGCN model - [INFO]: Epoch 775/1000, BPR Loss: 0.2298\n",
      "2024-11-20 16:58:40,481 - LGCN model - [INFO]: Epoch 776/1000, BPR Loss: 0.2298\n",
      "2024-11-20 16:58:40,909 - LGCN model - [INFO]: Epoch 777/1000, BPR Loss: 0.2298\n",
      "2024-11-20 16:58:41,388 - LGCN model - [INFO]: Epoch 778/1000, BPR Loss: 0.2298\n",
      "2024-11-20 16:58:41,838 - LGCN model - [INFO]: Epoch 779/1000, BPR Loss: 0.2298\n",
      "2024-11-20 16:58:42,269 - LGCN model - [INFO]: Epoch 780/1000, BPR Loss: 0.2298\n",
      "2024-11-20 16:58:42,714 - LGCN model - [INFO]: Epoch 781/1000, BPR Loss: 0.2298\n",
      "2024-11-20 16:58:43,088 - LGCN model - [INFO]: Epoch 782/1000, BPR Loss: 0.2298\n",
      "2024-11-20 16:58:43,568 - LGCN model - [INFO]: Epoch 783/1000, BPR Loss: 0.2298\n",
      "2024-11-20 16:58:44,024 - LGCN model - [INFO]: Epoch 784/1000, BPR Loss: 0.2298\n",
      "2024-11-20 16:58:44,449 - LGCN model - [INFO]: Epoch 785/1000, BPR Loss: 0.2298\n",
      "2024-11-20 16:58:44,897 - LGCN model - [INFO]: Epoch 786/1000, BPR Loss: 0.2298\n",
      "2024-11-20 16:58:45,272 - LGCN model - [INFO]: Epoch 787/1000, BPR Loss: 0.2298\n",
      "2024-11-20 16:58:45,761 - LGCN model - [INFO]: Epoch 788/1000, BPR Loss: 0.2298\n",
      "2024-11-20 16:58:46,220 - LGCN model - [INFO]: Epoch 789/1000, BPR Loss: 0.2298\n",
      "2024-11-20 16:58:46,659 - LGCN model - [INFO]: Epoch 790/1000, BPR Loss: 0.2297\n",
      "2024-11-20 16:58:47,202 - LGCN model - [INFO]: Epoch 791/1000, BPR Loss: 0.2297\n",
      "2024-11-20 16:58:47,601 - LGCN model - [INFO]: Epoch 792/1000, BPR Loss: 0.2297\n",
      "2024-11-20 16:58:47,988 - LGCN model - [INFO]: Epoch 793/1000, BPR Loss: 0.2297\n",
      "2024-11-20 16:58:48,349 - LGCN model - [INFO]: Epoch 794/1000, BPR Loss: 0.2297\n",
      "2024-11-20 16:58:48,730 - LGCN model - [INFO]: Epoch 795/1000, BPR Loss: 0.2297\n",
      "2024-11-20 16:58:49,271 - LGCN model - [INFO]: Epoch 796/1000, BPR Loss: 0.2297\n",
      "2024-11-20 16:58:49,784 - LGCN model - [INFO]: Epoch 797/1000, BPR Loss: 0.2297\n",
      "2024-11-20 16:58:50,302 - LGCN model - [INFO]: Epoch 798/1000, BPR Loss: 0.2297\n",
      "2024-11-20 16:58:50,759 - LGCN model - [INFO]: Epoch 799/1000, BPR Loss: 0.2297\n",
      "2024-11-20 16:58:51,240 - LGCN model - [INFO]: Epoch 800/1000, BPR Loss: 0.2297\n",
      "2024-11-20 16:58:51,758 - LGCN model - [INFO]: Epoch 801/1000, BPR Loss: 0.2297\n",
      "2024-11-20 16:58:52,132 - LGCN model - [INFO]: Epoch 802/1000, BPR Loss: 0.2297\n",
      "2024-11-20 16:58:52,619 - LGCN model - [INFO]: Epoch 803/1000, BPR Loss: 0.2297\n",
      "2024-11-20 16:58:53,027 - LGCN model - [INFO]: Epoch 804/1000, BPR Loss: 0.2297\n",
      "2024-11-20 16:58:53,487 - LGCN model - [INFO]: Epoch 805/1000, BPR Loss: 0.2297\n",
      "2024-11-20 16:58:53,885 - LGCN model - [INFO]: Epoch 806/1000, BPR Loss: 0.2297\n",
      "2024-11-20 16:58:54,286 - LGCN model - [INFO]: Epoch 807/1000, BPR Loss: 0.2297\n",
      "2024-11-20 16:58:54,740 - LGCN model - [INFO]: Epoch 808/1000, BPR Loss: 0.2297\n",
      "2024-11-20 16:58:55,162 - LGCN model - [INFO]: Epoch 809/1000, BPR Loss: 0.2297\n",
      "2024-11-20 16:58:55,624 - LGCN model - [INFO]: Epoch 810/1000, BPR Loss: 0.2297\n",
      "2024-11-20 16:58:56,015 - LGCN model - [INFO]: Epoch 811/1000, BPR Loss: 0.2297\n",
      "2024-11-20 16:58:56,437 - LGCN model - [INFO]: Epoch 812/1000, BPR Loss: 0.2297\n",
      "2024-11-20 16:58:56,802 - LGCN model - [INFO]: Epoch 813/1000, BPR Loss: 0.2297\n",
      "2024-11-20 16:58:57,161 - LGCN model - [INFO]: Epoch 814/1000, BPR Loss: 0.2297\n",
      "2024-11-20 16:58:57,485 - LGCN model - [INFO]: Epoch 815/1000, BPR Loss: 0.2297\n",
      "2024-11-20 16:58:57,875 - LGCN model - [INFO]: Epoch 816/1000, BPR Loss: 0.2297\n",
      "2024-11-20 16:58:58,315 - LGCN model - [INFO]: Epoch 817/1000, BPR Loss: 0.2297\n",
      "2024-11-20 16:58:58,758 - LGCN model - [INFO]: Epoch 818/1000, BPR Loss: 0.2297\n",
      "2024-11-20 16:58:59,124 - LGCN model - [INFO]: Epoch 819/1000, BPR Loss: 0.2297\n",
      "2024-11-20 16:58:59,585 - LGCN model - [INFO]: Epoch 820/1000, BPR Loss: 0.2297\n",
      "2024-11-20 16:58:59,985 - LGCN model - [INFO]: Epoch 821/1000, BPR Loss: 0.2297\n",
      "2024-11-20 16:59:00,521 - LGCN model - [INFO]: Epoch 822/1000, BPR Loss: 0.2296\n",
      "2024-11-20 16:59:00,951 - LGCN model - [INFO]: Epoch 823/1000, BPR Loss: 0.2296\n",
      "2024-11-20 16:59:01,323 - LGCN model - [INFO]: Epoch 824/1000, BPR Loss: 0.2296\n",
      "2024-11-20 16:59:01,887 - LGCN model - [INFO]: Epoch 825/1000, BPR Loss: 0.2296\n",
      "2024-11-20 16:59:02,305 - LGCN model - [INFO]: Epoch 826/1000, BPR Loss: 0.2296\n",
      "2024-11-20 16:59:02,852 - LGCN model - [INFO]: Epoch 827/1000, BPR Loss: 0.2296\n",
      "2024-11-20 16:59:03,259 - LGCN model - [INFO]: Epoch 828/1000, BPR Loss: 0.2296\n",
      "2024-11-20 16:59:03,758 - LGCN model - [INFO]: Epoch 829/1000, BPR Loss: 0.2296\n",
      "2024-11-20 16:59:04,235 - LGCN model - [INFO]: Epoch 830/1000, BPR Loss: 0.2296\n",
      "2024-11-20 16:59:04,685 - LGCN model - [INFO]: Epoch 831/1000, BPR Loss: 0.2296\n",
      "2024-11-20 16:59:05,202 - LGCN model - [INFO]: Epoch 832/1000, BPR Loss: 0.2296\n",
      "2024-11-20 16:59:05,668 - LGCN model - [INFO]: Epoch 833/1000, BPR Loss: 0.2296\n",
      "2024-11-20 16:59:06,052 - LGCN model - [INFO]: Epoch 834/1000, BPR Loss: 0.2296\n",
      "2024-11-20 16:59:06,402 - LGCN model - [INFO]: Epoch 835/1000, BPR Loss: 0.2296\n",
      "2024-11-20 16:59:06,835 - LGCN model - [INFO]: Epoch 836/1000, BPR Loss: 0.2296\n",
      "2024-11-20 16:59:07,337 - LGCN model - [INFO]: Epoch 837/1000, BPR Loss: 0.2296\n",
      "2024-11-20 16:59:07,835 - LGCN model - [INFO]: Epoch 838/1000, BPR Loss: 0.2296\n",
      "2024-11-20 16:59:08,315 - LGCN model - [INFO]: Epoch 839/1000, BPR Loss: 0.2296\n",
      "2024-11-20 16:59:08,794 - LGCN model - [INFO]: Epoch 840/1000, BPR Loss: 0.2296\n",
      "2024-11-20 16:59:09,244 - LGCN model - [INFO]: Epoch 841/1000, BPR Loss: 0.2296\n",
      "2024-11-20 16:59:09,752 - LGCN model - [INFO]: Epoch 842/1000, BPR Loss: 0.2296\n",
      "2024-11-20 16:59:10,245 - LGCN model - [INFO]: Epoch 843/1000, BPR Loss: 0.2296\n",
      "2024-11-20 16:59:10,674 - LGCN model - [INFO]: Epoch 844/1000, BPR Loss: 0.2296\n",
      "2024-11-20 16:59:11,244 - LGCN model - [INFO]: Epoch 845/1000, BPR Loss: 0.2296\n",
      "2024-11-20 16:59:11,705 - LGCN model - [INFO]: Epoch 846/1000, BPR Loss: 0.2296\n",
      "2024-11-20 16:59:12,235 - LGCN model - [INFO]: Epoch 847/1000, BPR Loss: 0.2296\n",
      "2024-11-20 16:59:12,692 - LGCN model - [INFO]: Epoch 848/1000, BPR Loss: 0.2296\n",
      "2024-11-20 16:59:13,165 - LGCN model - [INFO]: Epoch 849/1000, BPR Loss: 0.2296\n",
      "2024-11-20 16:59:13,696 - LGCN model - [INFO]: Epoch 850/1000, BPR Loss: 0.2296\n",
      "2024-11-20 16:59:14,146 - LGCN model - [INFO]: Epoch 851/1000, BPR Loss: 0.2296\n",
      "2024-11-20 16:59:14,692 - LGCN model - [INFO]: Epoch 852/1000, BPR Loss: 0.2296\n",
      "2024-11-20 16:59:15,162 - LGCN model - [INFO]: Epoch 853/1000, BPR Loss: 0.2296\n",
      "2024-11-20 16:59:15,518 - LGCN model - [INFO]: Epoch 854/1000, BPR Loss: 0.2296\n",
      "2024-11-20 16:59:15,849 - LGCN model - [INFO]: Epoch 855/1000, BPR Loss: 0.2296\n",
      "2024-11-20 16:59:16,185 - LGCN model - [INFO]: Epoch 856/1000, BPR Loss: 0.2296\n",
      "2024-11-20 16:59:16,757 - LGCN model - [INFO]: Epoch 857/1000, BPR Loss: 0.2296\n",
      "2024-11-20 16:59:17,252 - LGCN model - [INFO]: Epoch 858/1000, BPR Loss: 0.2296\n",
      "2024-11-20 16:59:17,735 - LGCN model - [INFO]: Epoch 859/1000, BPR Loss: 0.2296\n",
      "2024-11-20 16:59:18,278 - LGCN model - [INFO]: Epoch 860/1000, BPR Loss: 0.2295\n",
      "2024-11-20 16:59:18,775 - LGCN model - [INFO]: Epoch 861/1000, BPR Loss: 0.2295\n",
      "2024-11-20 16:59:19,268 - LGCN model - [INFO]: Epoch 862/1000, BPR Loss: 0.2295\n",
      "2024-11-20 16:59:19,802 - LGCN model - [INFO]: Epoch 863/1000, BPR Loss: 0.2295\n",
      "2024-11-20 16:59:20,322 - LGCN model - [INFO]: Epoch 864/1000, BPR Loss: 0.2295\n",
      "2024-11-20 16:59:20,802 - LGCN model - [INFO]: Epoch 865/1000, BPR Loss: 0.2295\n",
      "2024-11-20 16:59:21,325 - LGCN model - [INFO]: Epoch 866/1000, BPR Loss: 0.2295\n",
      "2024-11-20 16:59:21,803 - LGCN model - [INFO]: Epoch 867/1000, BPR Loss: 0.2295\n",
      "2024-11-20 16:59:22,318 - LGCN model - [INFO]: Epoch 868/1000, BPR Loss: 0.2295\n",
      "2024-11-20 16:59:22,818 - LGCN model - [INFO]: Epoch 869/1000, BPR Loss: 0.2295\n",
      "2024-11-20 16:59:23,315 - LGCN model - [INFO]: Epoch 870/1000, BPR Loss: 0.2295\n",
      "2024-11-20 16:59:23,891 - LGCN model - [INFO]: Epoch 871/1000, BPR Loss: 0.2295\n",
      "2024-11-20 16:59:24,323 - LGCN model - [INFO]: Epoch 872/1000, BPR Loss: 0.2295\n",
      "2024-11-20 16:59:24,761 - LGCN model - [INFO]: Epoch 873/1000, BPR Loss: 0.2295\n",
      "2024-11-20 16:59:25,127 - LGCN model - [INFO]: Epoch 874/1000, BPR Loss: 0.2295\n",
      "2024-11-20 16:59:25,501 - LGCN model - [INFO]: Epoch 875/1000, BPR Loss: 0.2295\n",
      "2024-11-20 16:59:26,018 - LGCN model - [INFO]: Epoch 876/1000, BPR Loss: 0.2295\n",
      "2024-11-20 16:59:26,542 - LGCN model - [INFO]: Epoch 877/1000, BPR Loss: 0.2295\n",
      "2024-11-20 16:59:27,024 - LGCN model - [INFO]: Epoch 878/1000, BPR Loss: 0.2295\n",
      "2024-11-20 16:59:27,549 - LGCN model - [INFO]: Epoch 879/1000, BPR Loss: 0.2295\n",
      "2024-11-20 16:59:27,999 - LGCN model - [INFO]: Epoch 880/1000, BPR Loss: 0.2295\n",
      "2024-11-20 16:59:28,574 - LGCN model - [INFO]: Epoch 881/1000, BPR Loss: 0.2295\n",
      "2024-11-20 16:59:29,087 - LGCN model - [INFO]: Epoch 882/1000, BPR Loss: 0.2295\n",
      "2024-11-20 16:59:29,614 - LGCN model - [INFO]: Epoch 883/1000, BPR Loss: 0.2295\n",
      "2024-11-20 16:59:30,133 - LGCN model - [INFO]: Epoch 884/1000, BPR Loss: 0.2295\n",
      "2024-11-20 16:59:30,612 - LGCN model - [INFO]: Epoch 885/1000, BPR Loss: 0.2295\n",
      "2024-11-20 16:59:31,160 - LGCN model - [INFO]: Epoch 886/1000, BPR Loss: 0.2295\n",
      "2024-11-20 16:59:31,693 - LGCN model - [INFO]: Epoch 887/1000, BPR Loss: 0.2295\n",
      "2024-11-20 16:59:32,191 - LGCN model - [INFO]: Epoch 888/1000, BPR Loss: 0.2295\n",
      "2024-11-20 16:59:32,772 - LGCN model - [INFO]: Epoch 889/1000, BPR Loss: 0.2295\n",
      "2024-11-20 16:59:33,271 - LGCN model - [INFO]: Epoch 890/1000, BPR Loss: 0.2295\n",
      "2024-11-20 16:59:33,835 - LGCN model - [INFO]: Epoch 891/1000, BPR Loss: 0.2295\n",
      "2024-11-20 16:59:34,285 - LGCN model - [INFO]: Epoch 892/1000, BPR Loss: 0.2295\n",
      "2024-11-20 16:59:34,667 - LGCN model - [INFO]: Epoch 893/1000, BPR Loss: 0.2295\n",
      "2024-11-20 16:59:35,078 - LGCN model - [INFO]: Epoch 894/1000, BPR Loss: 0.2295\n",
      "2024-11-20 16:59:35,724 - LGCN model - [INFO]: Epoch 895/1000, BPR Loss: 0.2295\n",
      "2024-11-20 16:59:36,256 - LGCN model - [INFO]: Epoch 896/1000, BPR Loss: 0.2295\n",
      "2024-11-20 16:59:36,880 - LGCN model - [INFO]: Epoch 897/1000, BPR Loss: 0.2295\n",
      "2024-11-20 16:59:37,502 - LGCN model - [INFO]: Epoch 898/1000, BPR Loss: 0.2295\n",
      "2024-11-20 16:59:38,143 - LGCN model - [INFO]: Epoch 899/1000, BPR Loss: 0.2295\n",
      "2024-11-20 16:59:38,709 - LGCN model - [INFO]: Epoch 900/1000, BPR Loss: 0.2295\n",
      "2024-11-20 16:59:39,433 - LGCN model - [INFO]: Epoch 901/1000, BPR Loss: 0.2295\n",
      "2024-11-20 16:59:40,011 - LGCN model - [INFO]: Epoch 902/1000, BPR Loss: 0.2295\n",
      "2024-11-20 16:59:40,606 - LGCN model - [INFO]: Epoch 903/1000, BPR Loss: 0.2295\n",
      "2024-11-20 16:59:41,303 - LGCN model - [INFO]: Epoch 904/1000, BPR Loss: 0.2295\n",
      "2024-11-20 16:59:41,954 - LGCN model - [INFO]: Epoch 905/1000, BPR Loss: 0.2295\n",
      "2024-11-20 16:59:42,585 - LGCN model - [INFO]: Epoch 906/1000, BPR Loss: 0.2295\n",
      "2024-11-20 16:59:43,224 - LGCN model - [INFO]: Epoch 907/1000, BPR Loss: 0.2295\n",
      "2024-11-20 16:59:43,961 - LGCN model - [INFO]: Epoch 908/1000, BPR Loss: 0.2295\n",
      "2024-11-20 16:59:44,470 - LGCN model - [INFO]: Epoch 909/1000, BPR Loss: 0.2295\n",
      "2024-11-20 16:59:44,901 - LGCN model - [INFO]: Epoch 910/1000, BPR Loss: 0.2295\n",
      "2024-11-20 16:59:45,322 - LGCN model - [INFO]: Epoch 911/1000, BPR Loss: 0.2295\n",
      "2024-11-20 16:59:45,920 - LGCN model - [INFO]: Epoch 912/1000, BPR Loss: 0.2294\n",
      "2024-11-20 16:59:46,470 - LGCN model - [INFO]: Epoch 913/1000, BPR Loss: 0.2294\n",
      "2024-11-20 16:59:47,038 - LGCN model - [INFO]: Epoch 914/1000, BPR Loss: 0.2294\n",
      "2024-11-20 16:59:47,544 - LGCN model - [INFO]: Epoch 915/1000, BPR Loss: 0.2294\n",
      "2024-11-20 16:59:48,170 - LGCN model - [INFO]: Epoch 916/1000, BPR Loss: 0.2294\n",
      "2024-11-20 16:59:48,746 - LGCN model - [INFO]: Epoch 917/1000, BPR Loss: 0.2294\n",
      "2024-11-20 16:59:49,341 - LGCN model - [INFO]: Epoch 918/1000, BPR Loss: 0.2294\n",
      "2024-11-20 16:59:49,859 - LGCN model - [INFO]: Epoch 919/1000, BPR Loss: 0.2294\n",
      "2024-11-20 16:59:50,474 - LGCN model - [INFO]: Epoch 920/1000, BPR Loss: 0.2294\n",
      "2024-11-20 16:59:51,134 - LGCN model - [INFO]: Epoch 921/1000, BPR Loss: 0.2294\n",
      "2024-11-20 16:59:51,739 - LGCN model - [INFO]: Epoch 922/1000, BPR Loss: 0.2294\n",
      "2024-11-20 16:59:52,408 - LGCN model - [INFO]: Epoch 923/1000, BPR Loss: 0.2294\n",
      "2024-11-20 16:59:52,905 - LGCN model - [INFO]: Epoch 924/1000, BPR Loss: 0.2294\n",
      "2024-11-20 16:59:53,690 - LGCN model - [INFO]: Epoch 925/1000, BPR Loss: 0.2294\n",
      "2024-11-20 16:59:54,228 - LGCN model - [INFO]: Epoch 926/1000, BPR Loss: 0.2294\n",
      "2024-11-20 16:59:54,717 - LGCN model - [INFO]: Epoch 927/1000, BPR Loss: 0.2294\n",
      "2024-11-20 16:59:55,283 - LGCN model - [INFO]: Epoch 928/1000, BPR Loss: 0.2294\n",
      "2024-11-20 16:59:55,774 - LGCN model - [INFO]: Epoch 929/1000, BPR Loss: 0.2294\n",
      "2024-11-20 16:59:56,338 - LGCN model - [INFO]: Epoch 930/1000, BPR Loss: 0.2294\n",
      "2024-11-20 16:59:56,835 - LGCN model - [INFO]: Epoch 931/1000, BPR Loss: 0.2294\n",
      "2024-11-20 16:59:57,342 - LGCN model - [INFO]: Epoch 932/1000, BPR Loss: 0.2294\n",
      "2024-11-20 16:59:57,938 - LGCN model - [INFO]: Epoch 933/1000, BPR Loss: 0.2294\n",
      "2024-11-20 16:59:58,453 - LGCN model - [INFO]: Epoch 934/1000, BPR Loss: 0.2294\n",
      "2024-11-20 16:59:58,981 - LGCN model - [INFO]: Epoch 935/1000, BPR Loss: 0.2294\n",
      "2024-11-20 16:59:59,560 - LGCN model - [INFO]: Epoch 936/1000, BPR Loss: 0.2294\n",
      "2024-11-20 17:00:00,084 - LGCN model - [INFO]: Epoch 937/1000, BPR Loss: 0.2294\n",
      "2024-11-20 17:00:00,576 - LGCN model - [INFO]: Epoch 938/1000, BPR Loss: 0.2294\n",
      "2024-11-20 17:00:01,179 - LGCN model - [INFO]: Epoch 939/1000, BPR Loss: 0.2294\n",
      "2024-11-20 17:00:01,681 - LGCN model - [INFO]: Epoch 940/1000, BPR Loss: 0.2294\n",
      "2024-11-20 17:00:02,087 - LGCN model - [INFO]: Epoch 941/1000, BPR Loss: 0.2294\n",
      "2024-11-20 17:00:02,699 - LGCN model - [INFO]: Epoch 942/1000, BPR Loss: 0.2294\n",
      "2024-11-20 17:00:03,265 - LGCN model - [INFO]: Epoch 943/1000, BPR Loss: 0.2294\n",
      "2024-11-20 17:00:03,827 - LGCN model - [INFO]: Epoch 944/1000, BPR Loss: 0.2294\n",
      "2024-11-20 17:00:04,223 - LGCN model - [INFO]: Epoch 945/1000, BPR Loss: 0.2294\n",
      "2024-11-20 17:00:04,661 - LGCN model - [INFO]: Epoch 946/1000, BPR Loss: 0.2294\n",
      "2024-11-20 17:00:05,086 - LGCN model - [INFO]: Epoch 947/1000, BPR Loss: 0.2294\n",
      "2024-11-20 17:00:05,458 - LGCN model - [INFO]: Epoch 948/1000, BPR Loss: 0.2294\n",
      "2024-11-20 17:00:05,872 - LGCN model - [INFO]: Epoch 949/1000, BPR Loss: 0.2294\n",
      "2024-11-20 17:00:06,218 - LGCN model - [INFO]: Epoch 950/1000, BPR Loss: 0.2294\n",
      "2024-11-20 17:00:06,564 - LGCN model - [INFO]: Epoch 951/1000, BPR Loss: 0.2294\n",
      "2024-11-20 17:00:06,937 - LGCN model - [INFO]: Epoch 952/1000, BPR Loss: 0.2294\n",
      "2024-11-20 17:00:07,361 - LGCN model - [INFO]: Epoch 953/1000, BPR Loss: 0.2294\n",
      "2024-11-20 17:00:07,740 - LGCN model - [INFO]: Epoch 954/1000, BPR Loss: 0.2294\n",
      "2024-11-20 17:00:08,262 - LGCN model - [INFO]: Epoch 955/1000, BPR Loss: 0.2294\n",
      "2024-11-20 17:00:08,609 - LGCN model - [INFO]: Epoch 956/1000, BPR Loss: 0.2294\n",
      "2024-11-20 17:00:08,929 - LGCN model - [INFO]: Epoch 957/1000, BPR Loss: 0.2294\n",
      "2024-11-20 17:00:09,238 - LGCN model - [INFO]: Epoch 958/1000, BPR Loss: 0.2294\n",
      "2024-11-20 17:00:09,533 - LGCN model - [INFO]: Epoch 959/1000, BPR Loss: 0.2294\n",
      "2024-11-20 17:00:09,836 - LGCN model - [INFO]: Epoch 960/1000, BPR Loss: 0.2294\n",
      "2024-11-20 17:00:10,166 - LGCN model - [INFO]: Epoch 961/1000, BPR Loss: 0.2294\n",
      "2024-11-20 17:00:10,460 - LGCN model - [INFO]: Epoch 962/1000, BPR Loss: 0.2294\n",
      "2024-11-20 17:00:10,768 - LGCN model - [INFO]: Epoch 963/1000, BPR Loss: 0.2294\n",
      "2024-11-20 17:00:11,077 - LGCN model - [INFO]: Epoch 964/1000, BPR Loss: 0.2294\n",
      "2024-11-20 17:00:11,367 - LGCN model - [INFO]: Epoch 965/1000, BPR Loss: 0.2294\n",
      "2024-11-20 17:00:11,670 - LGCN model - [INFO]: Epoch 966/1000, BPR Loss: 0.2294\n",
      "2024-11-20 17:00:11,976 - LGCN model - [INFO]: Epoch 967/1000, BPR Loss: 0.2294\n",
      "2024-11-20 17:00:12,267 - LGCN model - [INFO]: Epoch 968/1000, BPR Loss: 0.2294\n",
      "2024-11-20 17:00:12,584 - LGCN model - [INFO]: Epoch 969/1000, BPR Loss: 0.2294\n",
      "2024-11-20 17:00:12,868 - LGCN model - [INFO]: Epoch 970/1000, BPR Loss: 0.2294\n",
      "2024-11-20 17:00:13,170 - LGCN model - [INFO]: Epoch 971/1000, BPR Loss: 0.2294\n",
      "2024-11-20 17:00:13,468 - LGCN model - [INFO]: Epoch 972/1000, BPR Loss: 0.2294\n",
      "2024-11-20 17:00:13,784 - LGCN model - [INFO]: Epoch 973/1000, BPR Loss: 0.2294\n",
      "2024-11-20 17:00:14,090 - LGCN model - [INFO]: Epoch 974/1000, BPR Loss: 0.2294\n",
      "2024-11-20 17:00:14,413 - LGCN model - [INFO]: Epoch 975/1000, BPR Loss: 0.2294\n",
      "2024-11-20 17:00:14,743 - LGCN model - [INFO]: Epoch 976/1000, BPR Loss: 0.2294\n",
      "2024-11-20 17:00:15,069 - LGCN model - [INFO]: Epoch 977/1000, BPR Loss: 0.2294\n",
      "2024-11-20 17:00:15,377 - LGCN model - [INFO]: Epoch 978/1000, BPR Loss: 0.2294\n",
      "2024-11-20 17:00:15,685 - LGCN model - [INFO]: Epoch 979/1000, BPR Loss: 0.2294\n",
      "2024-11-20 17:00:15,968 - LGCN model - [INFO]: Epoch 980/1000, BPR Loss: 0.2294\n",
      "2024-11-20 17:00:16,271 - LGCN model - [INFO]: Epoch 981/1000, BPR Loss: 0.2294\n",
      "2024-11-20 17:00:16,590 - LGCN model - [INFO]: Epoch 982/1000, BPR Loss: 0.2294\n",
      "2024-11-20 17:00:16,922 - LGCN model - [INFO]: Epoch 983/1000, BPR Loss: 0.2294\n",
      "2024-11-20 17:00:17,224 - LGCN model - [INFO]: Epoch 984/1000, BPR Loss: 0.2294\n",
      "2024-11-20 17:00:17,536 - LGCN model - [INFO]: Epoch 985/1000, BPR Loss: 0.2294\n",
      "2024-11-20 17:00:17,836 - LGCN model - [INFO]: Epoch 986/1000, BPR Loss: 0.2294\n",
      "2024-11-20 17:00:18,143 - LGCN model - [INFO]: Epoch 987/1000, BPR Loss: 0.2294\n",
      "2024-11-20 17:00:18,444 - LGCN model - [INFO]: Epoch 988/1000, BPR Loss: 0.2294\n",
      "2024-11-20 17:00:18,729 - LGCN model - [INFO]: Epoch 989/1000, BPR Loss: 0.2294\n",
      "2024-11-20 17:00:19,025 - LGCN model - [INFO]: Epoch 990/1000, BPR Loss: 0.2294\n",
      "2024-11-20 17:00:19,332 - LGCN model - [INFO]: Epoch 991/1000, BPR Loss: 0.2294\n",
      "2024-11-20 17:00:19,619 - LGCN model - [INFO]: Epoch 992/1000, BPR Loss: 0.2294\n",
      "2024-11-20 17:00:19,884 - LGCN model - [INFO]: Epoch 993/1000, BPR Loss: 0.2294\n",
      "2024-11-20 17:00:20,151 - LGCN model - [INFO]: Epoch 994/1000, BPR Loss: 0.2294\n",
      "2024-11-20 17:00:20,451 - LGCN model - [INFO]: Epoch 995/1000, BPR Loss: 0.2294\n",
      "2024-11-20 17:00:20,742 - LGCN model - [INFO]: Epoch 996/1000, BPR Loss: 0.2294\n",
      "2024-11-20 17:00:21,001 - LGCN model - [INFO]: Epoch 997/1000, BPR Loss: 0.2294\n",
      "2024-11-20 17:00:21,271 - LGCN model - [INFO]: Epoch 998/1000, BPR Loss: 0.2294\n",
      "2024-11-20 17:00:21,527 - LGCN model - [INFO]: Epoch 999/1000, BPR Loss: 0.2293\n",
      "2024-11-20 17:00:21,821 - LGCN model - [INFO]: Epoch 1000/1000, BPR Loss: 0.2293\n"
     ]
    }
   ],
   "source": [
    "from models.lightgcn_model import LightGCNModel\n",
    "\n",
    "lgcn_model = LightGCNModel(\n",
    "        size=\"100k\",\n",
    "        num_layers=3,\n",
    "        num_negatives=4,\n",
    "        embedding_dim=64,\n",
    "        learning_rate=0.01,\n",
    "        epochs=1000,\n",
    "        batch_size=1024,\n",
    "        device='cpu'\n",
    "    )\n",
    "\n",
    "lgcn_model.prepare_training_data()\n",
    "\n",
    "lgcn_model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MAP': 0.11230943829402501,\n",
       " 'nDCG@k': 0.3609482703371832,\n",
       " 'Precision@k': 0.3031813361611877,\n",
       " 'Recall@k': 0.19674492206266433}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.metrics import calculate_ranking_metrics, calculate_rating_metrics\n",
    "\n",
    "predictions = lgcn_model.predict()\n",
    "\n",
    "ratings = calculate_rating_metrics(lgcn_model.test_pre, predictions)\n",
    "\n",
    "top_k_scores = lgcn_model.recommend_k()\n",
    "rankings = calculate_ranking_metrics(lgcn_model.test_pre, top_k_scores, 10)\n",
    "\n",
    "rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dinghui101",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
