{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## macos mps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import surprise\n",
    "\n",
    "from recommenders.utils.timer import Timer\n",
    "from recommenders.datasets import movielens\n",
    "from recommenders.datasets.python_splitters import python_random_split\n",
    "from recommenders.evaluation.python_evaluation import (\n",
    "    rmse,\n",
    "    mae,\n",
    "    rsquared,\n",
    "    exp_var,\n",
    "    map_at_k,\n",
    "    ndcg_at_k,\n",
    "    precision_at_k,\n",
    "    recall_at_k,\n",
    "    get_top_k_items,\n",
    ")\n",
    "from recommenders.models.surprise.surprise_utils import (\n",
    "    predict,\n",
    "    compute_ranking_predictions,\n",
    ")\n",
    "from recommenders.utils.notebook_utils import store_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.9.20 (main, Oct  3 2024, 02:24:59) \n",
      "[Clang 14.0.6 ]\n",
      "Surprise version: 1.1.4\n"
     ]
    }
   ],
   "source": [
    "print(f\"System version: {sys.version}\")\n",
    "print(f\"Surprise version: {surprise.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  userId  movieId  rating  timestamp\n",
      "0          43       1      804     4.0  964980499\n",
      "1          73       1     1210     5.0  964980499\n",
      "2         171       1     2628     4.0  964980523\n",
      "3         183       1     2826     4.0  964980523\n",
      "4         120       1     2018     5.0  964980523\n",
      "   Unnamed: 0  userId  movieId  rating  timestamp\n",
      "0          76       1     1219     2.0  964983393\n",
      "1         174       1     2644     4.0  964983393\n",
      "2          91       1     1348     4.0  964983393\n",
      "3         176       1     2654     5.0  964983393\n",
      "4          83       1     1258     3.0  964983414\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.read_csv('datasets/ml-latest-small/train.csv')\n",
    "test_df = pd.read_csv('datasets/ml-latest-small/test.csv')\n",
    "\n",
    "print(train_df.head())\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import Dataset, Reader, SVD, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Reader(rating_scale=(train_df['rating'].min(), train_df['rating'].max()))\n",
    "\n",
    "train_set = Dataset.load_from_df(train_df[['userId', 'movieId', 'rating']], reader).build_full_trainset()\n",
    "test_set = list(zip(test_df['userId'], test_df['movieId'], test_df['rating']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.9038\n",
      "testset RMSE: 0.9037522276885648\n"
     ]
    }
   ],
   "source": [
    "svd = SVD(random_state=0, n_factors=200, n_epochs=30, verbose=False)\n",
    "svd.fit(train_set)\n",
    "\n",
    "test_predictions = svd.test(test_set)\n",
    "\n",
    "test_rmse = accuracy.rmse(test_predictions)\n",
    "print(f\"testset RMSE: {test_rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 0.8894\n",
      "knn baseline RMSE: 0.889403935617733\n",
      "RMSE: 0.9494\n",
      "knn mean RMSE: 0.9494347668407263\n"
     ]
    }
   ],
   "source": [
    "from surprise import KNNBaseline, KNNWithMeans\n",
    "knn_baseline = KNNBaseline(sim_options={'name': 'pearson_baseline','user_based': False })\n",
    "knn_baseline.fit(train_set)\n",
    "\n",
    "knn_mean = KNNWithMeans(sim_options={'name':'cosine', 'user_based': True})\n",
    "knn_mean.fit(train_set)\n",
    "\n",
    "test_pred_knn_bl = knn_baseline.test(test_set)\n",
    "test_rmse_knn_bl = accuracy.rmse(test_pred_knn_bl)\n",
    "print(f\"knn baseline RMSE: {test_rmse_knn_bl}\")\n",
    "\n",
    "test_pred_knn_mean = knn_mean.test(test_set)\n",
    "test_rmse_knn_mean = accuracy.rmse(test_pred_knn_mean)\n",
    "print(f\"knn mean RMSE: {test_rmse_knn_mean}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import BertModel, BertConfig\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize encoders\n",
    "user_encoder = LabelEncoder()\n",
    "item_encoder = LabelEncoder()\n",
    "\n",
    "# Combine and encode data\n",
    "combined_df = pd.concat([train_df, test_df], axis=0)\n",
    "combined_df['user'] = user_encoder.fit_transform(combined_df['userId'])\n",
    "combined_df['item'] = item_encoder.fit_transform(combined_df['movieId'])\n",
    "\n",
    "num_users = combined_df['user'].nunique()\n",
    "num_items = combined_df['item'].nunique()\n",
    "\n",
    "# Split back into train and test\n",
    "train_df = combined_df.iloc[:len(train_df)]\n",
    "test_df = combined_df.iloc[len(train_df):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user                                               item\n",
      "0     0  [632, 910, 1978, 2125, 1492, 2670, 2692, 2798,...\n",
      "1     1  [277, 7355, 8810, 8532, 8045, 8448, 2670, 7750...\n",
      "2     2  [973, 1566, 1053, 696, 961, 1543, 30, 585, 276...\n",
      "3     3  [143, 684, 1001, 361, 1752, 2224, 2035, 135, 5...\n",
      "4     4  [508, 509, 123, 257, 337, 398, 506, 302, 126, ...\n",
      "Training set size: 72257\n",
      "Validation set size: 8029\n"
     ]
    }
   ],
   "source": [
    "# Sort and create sequences\n",
    "train_df = train_df.sort_values(['user', 'timestamp'])\n",
    "user_sequences = train_df.groupby('user')['item'].apply(list).reset_index()\n",
    "print(user_sequences.head())\n",
    "\n",
    "# Create input sequences and targets\n",
    "max_seq_length = 20 \n",
    "input_sequences = []\n",
    "target_items = []\n",
    "\n",
    "for seq in user_sequences['item']:\n",
    "    if len(seq) < 2:\n",
    "        continue\n",
    "    for i in range(1, len(seq)):\n",
    "        start = max(i - (max_seq_length - 1), 0)\n",
    "        input_seq = seq[start:i]\n",
    "        target = seq[i]\n",
    "        input_sequences.append(input_seq)\n",
    "        target_items.append(target)\n",
    "\n",
    "# Split into train and validation sets\n",
    "train_inputs, val_inputs, train_targets, val_targets = train_test_split(\n",
    "    input_sequences, target_items, test_size=0.1, random_state=42\n",
    ")\n",
    "print(f\"Training set size: {len(train_inputs)}\")\n",
    "print(f\"Validation set size: {len(val_inputs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT4Rec(nn.Module):\n",
    "    def __init__(self, num_items, hidden_size=128, num_layers=2, num_heads=4, \n",
    "                 max_position_embeddings=20, dropout=0.1):\n",
    "        super(BERT4Rec, self).__init__()\n",
    "        config = BertConfig(\n",
    "            vocab_size=num_items + 3,  # Including [PAD], [BOS], [EOS]\n",
    "            hidden_size=hidden_size,\n",
    "            num_hidden_layers=num_layers,\n",
    "            num_attention_heads=num_heads,\n",
    "            intermediate_size=hidden_size * 4,\n",
    "            max_position_embeddings=max_position_embeddings,\n",
    "            hidden_dropout_prob=dropout,\n",
    "            attention_probs_dropout_prob=dropout,\n",
    "            type_vocab_size=1,\n",
    "            output_attentions=False,\n",
    "            output_hidden_states=False,\n",
    "        )\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(hidden_size, num_items + 3)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        pooled_output = self.dropout(last_hidden_state)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits\n",
    "\n",
    "    def predict_next_item(self, input_ids, attention_mask):\n",
    "        logits = self.forward(input_ids, attention_mask)\n",
    "        next_logits = logits[:, -1, :]\n",
    "        return next_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ItemTokenizer:\n",
    "    def __init__(self, num_items):\n",
    "        self.num_items = num_items\n",
    "        self.pad_token_id = 0\n",
    "        self.bos_token_id = 1\n",
    "        self.eos_token_id = 2\n",
    "        self.vocab_size = num_items + 3\n",
    "\n",
    "    def encode_plus(self, sequence, add_special_tokens=True, max_length=None, \n",
    "                   padding='max_length', truncation=True, return_attention_mask=True,\n",
    "                   return_tensors='pt'):\n",
    "        if not sequence:\n",
    "            sequence = [self.pad_token_id]\n",
    "            \n",
    "        if add_special_tokens:\n",
    "            tokens = [self.bos_token_id] + sequence + [self.eos_token_id]\n",
    "        else:\n",
    "            tokens = sequence\n",
    "            \n",
    "        if padding == 'max_length' and max_length is not None:\n",
    "            pad_length = max_length - len(tokens)\n",
    "            if pad_length > 0:\n",
    "                tokens = tokens + [self.pad_token_id] * pad_length\n",
    "            elif truncation:\n",
    "                tokens = tokens[:max_length]\n",
    "                \n",
    "        attention_mask = [1] * len(tokens)\n",
    "        if padding == 'max_length' and max_length is not None:\n",
    "            attention_mask = attention_mask + [0] * (max_length - len(attention_mask))\n",
    "            \n",
    "        if return_tensors == 'pt':\n",
    "            tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "            attention_mask = torch.tensor(attention_mask, dtype=torch.long)\n",
    "            \n",
    "        return {\n",
    "            'input_ids': tokens,\n",
    "            'attention_mask': attention_mask if return_attention_mask else None\n",
    "        }\n",
    "\n",
    "class RecDataset(Dataset):\n",
    "    def __init__(self, inputs, targets, tokenizer, max_length):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.inputs[idx]\n",
    "        target = self.targets[idx]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            [item + 1 for item in seq],\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(target, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch in tqdm(data_loader, desc=\"Training\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_logits = logits[:, -1, :]\n",
    "\n",
    "        loss = criterion(last_logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        _, preds = torch.max(last_logits, dim=1)\n",
    "        correct += torch.sum(preds == labels)\n",
    "        total += labels.size(0)\n",
    "\n",
    "    avg_loss = np.mean(losses)\n",
    "    # accuracy = correct.double() / total\n",
    "    accuracy = correct.float() / total\n",
    "    return avg_loss, accuracy.item()\n",
    "\n",
    "def eval_model(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            last_logits = logits[:, -1, :]\n",
    "\n",
    "            loss = criterion(last_logits, labels)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            _, preds = torch.max(last_logits, dim=1)\n",
    "            correct += torch.sum(preds == labels)\n",
    "            total += labels.size(0)\n",
    "\n",
    "    avg_loss = np.mean(losses)\n",
    "    # accuracy = correct.double() / total\n",
    "    accuracy = correct.float() / total\n",
    "    return avg_loss, accuracy.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device mps\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# Initialize tokenizer and datasets\n",
    "tokenizer = ItemTokenizer(num_items)\n",
    "train_dataset = RecDataset(train_inputs, train_targets, tokenizer, max_seq_length)\n",
    "val_dataset = RecDataset(val_inputs, val_targets, tokenizer, max_seq_length)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "\n",
    "# Initialize model\n",
    "model = BERT4Rec(num_items=num_items)\n",
    "model = model.to(device)\n",
    "\n",
    "# Setup training\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "EPOCHS = 5\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/5\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2259/2259 [00:48<00:00, 46.76it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot convert a MPS Tensor to float64 dtype as the MPS framework doesn't support float64. Please use float32 instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Training accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m val_loss, val_acc \u001b[38;5;241m=\u001b[39m eval_model(\n\u001b[1;32m     15\u001b[0m     model,\n\u001b[1;32m     16\u001b[0m     val_loader,\n\u001b[1;32m     17\u001b[0m     criterion,\n\u001b[1;32m     18\u001b[0m     device\n\u001b[1;32m     19\u001b[0m )\n",
      "Cell \u001b[0;32mIn[23], line 26\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, data_loader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     23\u001b[0m     total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     25\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(losses)\n\u001b[0;32m---> 26\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mcorrect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdouble\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m total\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m avg_loss, accuracy\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot convert a MPS Tensor to float64 dtype as the MPS framework doesn't support float64. Please use float32 instead."
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{EPOCHS}\")\n",
    "    print(\"-\" * 10)\n",
    "\n",
    "    train_loss, train_acc = train_epoch(\n",
    "        model,\n",
    "        train_loader,\n",
    "        criterion,\n",
    "        optimizer,\n",
    "        device\n",
    "    )\n",
    "    print(f\"Training loss: {train_loss:.4f}, Training accuracy: {train_acc:.4f}\")\n",
    "\n",
    "    val_loss, val_acc = eval_model(\n",
    "        model,\n",
    "        val_loader,\n",
    "        criterion,\n",
    "        device\n",
    "    )\n",
    "    print(f\"Validation loss: {val_loss:.4f}, Validation accuracy: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topk_accuracy(model, data_loader, device, k=10):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Computing Top-K Accuracy\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            last_logits = logits[:, -1, :]\n",
    "            topk_preds = torch.topk(last_logits, k, dim=1).indices\n",
    "            \n",
    "            labels = labels.view(-1, 1)\n",
    "            correct += torch.sum(topk_preds == labels)\n",
    "            total += labels.size(0)\n",
    "\n",
    "    # topk_acc = correct.double() / total\n",
    "    topk_acc = correct.float() / total\n",
    "    return topk_acc.item()\n",
    "\n",
    "# Compute Top-K accuracy\n",
    "topk = 10\n",
    "topk_acc = get_topk_accuracy(model, val_loader, device, k=topk)\n",
    "print(f\"Validation Top-{topk} accuracy: {topk_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lightGCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MovieLensDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.user_indices = torch.tensor(df['user_idx'].values, dtype=torch.long)\n",
    "        self.item_indices = torch.tensor(df['item_idx'].values, dtype=torch.long)\n",
    "        self.ratings = torch.tensor(df['rating'].values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ratings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.user_indices[idx], self.item_indices[idx], self.ratings[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "train_dataset = MovieLensDataset(train_df)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = MovieLensDataset(test_df)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recommenders.models.graph_based.lightgcn import LightGCN\n",
    "\n",
    "# 定义模型参数\n",
    "embedding_dim = 20  # 嵌入维度\n",
    "num_layers = 3      # 图卷积层数\n",
    "\n",
    "# 实例化 LightGCN 模型\n",
    "model = LightGCN(\n",
    "    n_users=num_users,\n",
    "    n_items=num_items,\n",
    "    embedding_dim=embedding_dim,\n",
    "    num_layers=num_layers,\n",
    "    learning_rate=0.01,\n",
    "    weight_decay=1e-4,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# 训练模型\n",
    "model.fit(\n",
    "    train=train_df,\n",
    "    test=test_df,\n",
    "    epochs=10,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(test_df)\n",
    "print(f\"测试集评估结果：{results}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rec",
   "language": "python",
   "name": "rec_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
