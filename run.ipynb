{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark with Movielens dataset\n",
    "The main purpose of this notebook is not to produce comprehensive benchmarking results on multiple datasets. Rather, it is intended to evaluate different recommender algorithms(SVD, LightGCN, Transformer and our algorithm) in this repository.\n",
    "\n",
    "* Datasets\n",
    "  * [Movielens 100K](https://grouplens.org/datasets/movielens/100k/).\n",
    "  * [Movielens 1M](https://grouplens.org/datasets/movielens/1m/).\n",
    "\n",
    "* Data split\n",
    "  * TODO\n",
    "  \n",
    "\n",
    "* Evaluation metrics\n",
    "  * Ranking metrics:\n",
    "    * Precision@k.\n",
    "    * Recall@k.\n",
    "    * Normalized discounted cumulative gain@k (NDCG@k).\n",
    "    * Mean-average-precision (MAP). \n",
    "  * Rating metrics:\n",
    "    * Root mean squared error (RMSE).\n",
    "    * Mean average error (MAE).\n",
    "    * R squared.\n",
    "    * Explained variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/alanz/miniconda3/envs/rec_env/lib/python3.9/site-packages (2.3.0.post101)\n",
      "Requirement already satisfied: filelock in /Users/alanz/miniconda3/envs/rec_env/lib/python3.9/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/alanz/miniconda3/envs/rec_env/lib/python3.9/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in /Users/alanz/miniconda3/envs/rec_env/lib/python3.9/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: networkx in /Users/alanz/miniconda3/envs/rec_env/lib/python3.9/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/alanz/miniconda3/envs/rec_env/lib/python3.9/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/alanz/miniconda3/envs/rec_env/lib/python3.9/site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/alanz/miniconda3/envs/rec_env/lib/python3.9/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/alanz/miniconda3/envs/rec_env/lib/python3.9/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from train_and_evaluate import svd_model_train_and_evaluate, lgcn_model_train_and_evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(data, algo, k, rating_metrics, ranking_metrics):\n",
    "    summary = {\"Data\": data, \"Algo\": algo, \"K\": k}\n",
    "    if rating_metrics is None:\n",
    "        rating_metrics = {\n",
    "            \"RMSE\": np.nan,\n",
    "            \"MAE\": np.nan,\n",
    "            \"R2\": np.nan,\n",
    "            \"Explained Variance\": np.nan,\n",
    "        }\n",
    "    if ranking_metrics is None:\n",
    "        ranking_metrics = {\n",
    "            \"MAP\": np.nan,\n",
    "            \"nDCG@k\": np.nan,\n",
    "            \"Precision@k\": np.nan,\n",
    "            \"Recall@k\": np.nan,\n",
    "        }\n",
    "    summary.update(rating_metrics)\n",
    "    summary.update(ranking_metrics)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_recommenders():\n",
    "    cols = [\"Data\", \"Algo\", \"K\", \"RMSE\", \"MAE\", \"R2\", \"Explained Variance\", \"MAP\", \"nDCG@k\", \"Precision@k\", \"Recall@k\"]\n",
    "    df_results = pd.DataFrame(columns=cols)\n",
    "    sizes = [\"100k\"]\n",
    "    # algos=[\"svd\", \"lgcn\"]\n",
    "    algos=[\"lgcn\"]\n",
    "    models={\"svd\":svd_model_train_and_evaluate, \"lgcn\":lgcn_model_train_and_evaluate}\n",
    "    for size in sizes:\n",
    "        for algo in algos:\n",
    "            ratings, rankings = models[algo](size)\n",
    "            summary = generate_summary(size, algo, 10, ratings, rankings)\n",
    "            df_results.loc[df_results.shape[0] + 1] = summary\n",
    "    return df_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user  item  prediction\n",
      "0     1     1    4.018125\n",
      "3     1     4    4.204126\n",
      "5     1     6    3.188719\n",
      "7     1     8    4.805475\n",
      "9     1    10    4.200003\n",
      "   user  item  prediction  rank\n",
      "0   196   258    0.770406     1\n",
      "1   196    50    0.766580     2\n",
      "2   196   288    0.756307     3\n",
      "3   196   294    0.756078     4\n",
      "4   196   300    0.743543     5\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'rank'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/rec_env/lib/python3.9/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'rank'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_results \u001b[38;5;241m=\u001b[39m \u001b[43mbenchmark_recommenders\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m df_results\n",
      "Cell \u001b[0;32mIn[13], line 9\u001b[0m, in \u001b[0;36mbenchmark_recommenders\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m size \u001b[38;5;129;01min\u001b[39;00m sizes:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m algo \u001b[38;5;129;01min\u001b[39;00m algos:\n\u001b[0;32m----> 9\u001b[0m         ratings, rankings \u001b[38;5;241m=\u001b[39m \u001b[43mmodels\u001b[49m\u001b[43m[\u001b[49m\u001b[43malgo\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m         summary \u001b[38;5;241m=\u001b[39m generate_summary(size, algo, \u001b[38;5;241m10\u001b[39m, ratings, rankings)\n\u001b[1;32m     11\u001b[0m         df_results\u001b[38;5;241m.\u001b[39mloc[df_results\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m summary\n",
      "File \u001b[0;32m~/Downloads/cs5284_graphML/project/dinghui101/train_and_evaluate.py:36\u001b[0m, in \u001b[0;36mlgcn_model_train_and_evaluate\u001b[0;34m(size)\u001b[0m\n\u001b[1;32m     33\u001b[0m ratings \u001b[38;5;241m=\u001b[39m calculate_rating_metrics(lgcn_model\u001b[38;5;241m.\u001b[39mtest_pre, predictions)\n\u001b[1;32m     35\u001b[0m top_k_scores \u001b[38;5;241m=\u001b[39m lgcn_model\u001b[38;5;241m.\u001b[39mrecommend_k()\n\u001b[0;32m---> 36\u001b[0m rankings \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_ranking_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlgcn_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_pre\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ratings, rankings\n",
      "File \u001b[0;32m~/Downloads/cs5284_graphML/project/dinghui101/utils/metrics.py:425\u001b[0m, in \u001b[0;36mcalculate_ranking_metrics\u001b[0;34m(train, predictions, k)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_ranking_metrics\u001b[39m(train, predictions, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m    423\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    424\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMAP\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mmap\u001b[39m(train, predictions, k\u001b[38;5;241m=\u001b[39mk),\n\u001b[0;32m--> 425\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnDCG@k\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mndcg_at_k\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    426\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrecision@k\u001b[39m\u001b[38;5;124m\"\u001b[39m: precision_at_k(train, predictions, k\u001b[38;5;241m=\u001b[39mk),\n\u001b[1;32m    427\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecall@k\u001b[39m\u001b[38;5;124m\"\u001b[39m: recall_at_k(train, predictions, k\u001b[38;5;241m=\u001b[39mk),\n\u001b[1;32m    428\u001b[0m     }\n",
      "File \u001b[0;32m~/Downloads/cs5284_graphML/project/dinghui101/utils/metrics.py:326\u001b[0m, in \u001b[0;36mndcg_at_k\u001b[0;34m(rating_true, rating_pred, col_user, col_item, col_rating, col_prediction, relevancy_method, k, threshold, score_type, discfun_type, **_)\u001b[0m\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiscfun_type must be one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloge\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlog2\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;66;03m# Calculate the actual discounted gain for each record\u001b[39;00m\n\u001b[0;32m--> 326\u001b[0m df_dcg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdcg\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df_dcg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrel\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m/\u001b[39m discfun(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[43mdf_dcg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrank\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m    328\u001b[0m \u001b[38;5;66;03m# Calculate the ideal discounted gain for each record\u001b[39;00m\n\u001b[1;32m    329\u001b[0m df_idcg \u001b[38;5;241m=\u001b[39m df_dcg\u001b[38;5;241m.\u001b[39msort_values([col_user, col_rating], ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/rec_env/lib/python3.9/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/miniconda3/envs/rec_env/lib/python3.9/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'rank'"
     ]
    }
   ],
   "source": [
    "df_results = benchmark_recommenders()\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rec_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
