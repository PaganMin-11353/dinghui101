{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "CNRdzyWXmwAM",
        "6CVClbifhMt6"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Helper Class\n",
        "\n",
        "In order to form the GNN model for embedding generation, we use multiple mechanisms to ensure a fair embedding. Which includes adaptive neighbor sampler, meta aggregator and meta learner."
      ],
      "metadata": {
        "id": "XoM4MFmwmWUn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Meta aggregator\n",
        "\n",
        "Achieved by multi-head attention, we build something that could refine the relevance of high-order neighbor embedding to form a good embedding for target item/user."
      ],
      "metadata": {
        "id": "CNRdzyWXmwAM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, embedding_size, num_blocks, num_heads, d_ff, dropout_rate):\n",
        "        \"\"\"\n",
        "        Encoder module for multi-head attention and feedforward transformations.\n",
        "\n",
        "        Args:\n",
        "            embedding_size (int): Size of embeddings.\n",
        "            num_blocks (int): Number of attention blocks.\n",
        "            num_heads (int): Number of attention heads.\n",
        "            d_ff (int): Hidden layer size in feedforward network.\n",
        "            dropout_rate (float): Dropout rate.\n",
        "        \"\"\"\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding_size = embedding_size\n",
        "        self.num_blocks = num_blocks\n",
        "        self.num_heads = num_heads\n",
        "        self.d_ff = d_ff\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        # Define multi-head attention and feedforward layers for each block\n",
        "        self.attention_blocks = nn.ModuleList([\n",
        "            nn.MultiheadAttention(embed_dim=embedding_size, num_heads=num_heads, dropout=dropout_rate)\n",
        "            for _ in range(num_blocks)\n",
        "        ])\n",
        "        self.feedforward_blocks = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Linear(embedding_size, d_ff),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(d_ff, embedding_size),\n",
        "                nn.Dropout(dropout_rate)\n",
        "            )\n",
        "            for _ in range(num_blocks)\n",
        "        ])\n",
        "        self.layer_norms_attention = nn.ModuleList([nn.LayerNorm(embedding_size) for _ in range(num_blocks)])\n",
        "        self.layer_norms_ff = nn.ModuleList([nn.LayerNorm(embedding_size) for _ in range(num_blocks)])\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        Forward pass for the encoder.\n",
        "\n",
        "        Args:\n",
        "            input (Tensor): Input tensor of shape [batch_size, num_neighbors, embedding_size].\n",
        "\n",
        "        Returns:\n",
        "            enc (Tensor): Output tensor of shape [batch_size, num_neighbors, embedding_size].\n",
        "        \"\"\"\n",
        "        # Scale the input embeddings\n",
        "        enc = input * (self.embedding_size ** 0.5)  # [b, n, e]\n",
        "\n",
        "        # Process through each block\n",
        "        for i in range(self.num_blocks):\n",
        "            # Multi-head self-attention\n",
        "            enc_transposed = enc.permute(1, 0, 2)  # Convert to [n, b, e] for PyTorch MultiheadAttention\n",
        "            attn_output, _ = self.attention_blocks[i](enc_transposed, enc_transposed, enc_transposed)\n",
        "            attn_output = attn_output.permute(1, 0, 2)  # Convert back to [b, n, e]\n",
        "            # Add and normalize\n",
        "            enc = self.layer_norms_attention[i](enc + attn_output)\n",
        "\n",
        "            # Feedforward\n",
        "            ff_output = self.feedforward_blocks[i](enc)\n",
        "            # Add and normalize\n",
        "            enc = self.layer_norms_ff[i](enc + ff_output)\n",
        "\n",
        "        return enc  # [b, n, e]\n"
      ],
      "metadata": {
        "id": "Ohj1rX3PnJOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Major Class of GNN Embedding generation\n",
        "\n",
        "GNN models included are GAT, GraphSage and lightGCN."
      ],
      "metadata": {
        "id": "U9J5ErV9lqiG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_jBeG1QYfpK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "class GeneralGNN(nn.Module):\n",
        "    def __init__(self, name, settings):\n",
        "        super(GeneralGNN, self).__init__()\n",
        "        self.name = name\n",
        "\n",
        "        # Hyperparameters and settings\n",
        "        self.embedding_size = settings.embedding_size\n",
        "        self.learning_rate = settings.learning_rate\n",
        "        self.learning_rate_downstream = settings.learning_rate_downstream\n",
        "        self.num_items = settings.num_items\n",
        "        self.num_users = settings.num_users\n",
        "        self.k = settings.k\n",
        "        self.dropout = settings.dropout\n",
        "        self.batch_size = settings.batch_size\n",
        "        self.decay = settings.decay\n",
        "\n",
        "        # Transformer encoder structure\n",
        "        self.dropout_rate = settings.dropout_rate\n",
        "        self.num_heads = settings.num_heads\n",
        "        self.d_ff = settings.d_ff\n",
        "        self.num_blocks = settings.num_blocks\n",
        "\n",
        "        # Embedding matrices\n",
        "        self.user_embeddings = nn.Embedding(self.num_users + 1, self.embedding_size)\n",
        "        self.item_embeddings = nn.Embedding(self.num_items + 1, self.embedding_size)\n",
        "\n",
        "        # Initialize padding embedding (last row for padding)\n",
        "        with torch.no_grad():\n",
        "            self.user_embeddings.weight[-1].fill_(0)\n",
        "            self.item_embeddings.weight[-1].fill_(0)\n",
        "\n",
        "        # Encoder for multi-head attention\n",
        "        self.encoder = Encoder(\n",
        "            embedding_size=self.embedding_size,\n",
        "            num_blocks=self.num_blocks,\n",
        "            num_heads=self.num_heads,\n",
        "            d_ff=self.d_ff,\n",
        "            dropout_rate=self.dropout_rate\n",
        "        )\n",
        "\n",
        "        # Trainable weight matrices for second and third-order aggregation\n",
        "        self.second_order_weight = nn.Parameter(\n",
        "            torch.randn(self.embedding_size, self.embedding_size) * np.sqrt(2.0 / self.embedding_size)\n",
        "        )\n",
        "        self.third_order_weight = nn.Parameter(\n",
        "            torch.randn(self.embedding_size, self.embedding_size) * np.sqrt(2.0 / self.embedding_size)\n",
        "        )\n",
        "\n",
        "        # Parameters for agent networks\n",
        "        self.second_order_agent = self.create_agent_network(self.embedding_size)\n",
        "        self.third_order_agent = self.create_agent_network(self.embedding_size)\n",
        "\n",
        "        # Optimizer (you may need separate optimizers for different components if required)\n",
        "        self.optimizer = torch.optim.Adagrad(self.parameters(), lr=self.learning_rate)\n",
        "\n",
        "        # Pretrained embeddings (optional)\n",
        "        self.original_user_ebd = torch.tensor(np.load(settings.pre_train_user_ebd_path), dtype=torch.float32)\n",
        "        self.original_item_ebd = torch.tensor(np.load(settings.pre_train_item_ebd_path), dtype=torch.float32)\n",
        "        padding_ebd = torch.zeros((1, self.embedding_size), dtype=torch.float32)\n",
        "        self.original_user_ebd = torch.cat([self.original_user_ebd, padding_ebd], dim=0)\n",
        "        self.original_item_ebd = torch.cat([self.original_item_ebd, padding_ebd], dim=0)\n",
        "\n",
        "\n",
        "    def create_agent_network(self, state_size):\n",
        "        \"\"\"\n",
        "        Creates an agent network for second or third-order tasks.\n",
        "        \"\"\"\n",
        "        return nn.Sequential(\n",
        "            nn.Linear(state_size, self.embedding_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(self.embedding_size, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, target_ids, support_1st, support_2nd=None, support_3rd=None, task=\"user\", aggregation=\"GAT\"):\n",
        "        \"\"\"\n",
        "        Forward pass for the GeneralGNN model.\n",
        "\n",
        "        Args:\n",
        "            target_ids (Tensor): IDs of target users or items. Shape: [batch_size].\n",
        "            support_1st (Tensor): First-order neighbors (items/users). Shape: [batch_size, num_neighbors_1st].\n",
        "            support_2nd (Tensor, optional): Second-order neighbors (users/items). Shape: [batch_size, num_neighbors_2nd].\n",
        "            support_3rd (Tensor, optional): Third-order neighbors (items/users). Shape: [batch_size, num_neighbors_3rd].\n",
        "            task (str): Task type (\"user\" or \"item\").\n",
        "            aggregation (str): Aggregation method (\"GAT\", \"GraphSAGE\", \"FBNE\", etc.).\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Predicted embeddings for the target users/items. Shape: [batch_size, embedding_size].\n",
        "        \"\"\"\n",
        "        # Get embeddings for first-order neighbors\n",
        "        if task == \"user\":\n",
        "            # First-order neighbors are items for user tasks\n",
        "            first_order_embeddings = self.item_embeddings(support_1st)  # Shape: [batch_size, num_neighbors_1st, embedding_size]\n",
        "        elif task == \"item\":\n",
        "            # First-order neighbors are users for item tasks\n",
        "            first_order_embeddings = self.user_embeddings(support_1st)  # Shape: [batch_size, num_neighbors_1st, embedding_size]\n",
        "        else:\n",
        "            raise ValueError(\"Task must be 'user' or 'item'\")\n",
        "\n",
        "        # Aggregate first-order embeddings\n",
        "        if aggregation == \"GAT\":\n",
        "            first_order_agg = self.aggregate_gat(first_order_embeddings)  # Shape: [batch_size, embedding_size]\n",
        "        elif aggregation == \"GraphSAGE\":\n",
        "            first_order_agg = self.aggregate_graphsage(first_order_embeddings)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported aggregation method\")\n",
        "\n",
        "        # Handle second-order neighbors\n",
        "        if support_2nd is not None:\n",
        "            if task == \"user\":\n",
        "                # Second-order neighbors are users for user tasks\n",
        "                second_order_embeddings = self.user_embeddings(support_2nd)  # Shape: [batch_size, num_neighbors_2nd, embedding_size]\n",
        "            elif task == \"item\":\n",
        "                # Second-order neighbors are items for item tasks\n",
        "                second_order_embeddings = self.item_embeddings(support_2nd)  # Shape: [batch_size, num_neighbors_2nd, embedding_size]\n",
        "\n",
        "            second_order_agg = self.aggregate_gat(second_order_embeddings) if aggregation == \"GAT\" else self.aggregate_graphsage(second_order_embeddings)\n",
        "            # Combine first and second-order aggregations\n",
        "            combined_1st_2nd = torch.cat([first_order_agg, second_order_agg], dim=1)  # Shape: [batch_size, 2 * embedding_size]\n",
        "        else:\n",
        "            combined_1st_2nd = first_order_agg\n",
        "\n",
        "        # Handle third-order neighbors\n",
        "        if support_3rd is not None:\n",
        "            if task == \"user\":\n",
        "                # Third-order neighbors are items for user tasks\n",
        "                third_order_embeddings = self.item_embeddings(support_3rd)  # Shape: [batch_size, num_neighbors_3rd, embedding_size]\n",
        "            elif task == \"item\":\n",
        "                # Third-order neighbors are users for item tasks\n",
        "                third_order_embeddings = self.user_embeddings(support_3rd)  # Shape: [batch_size, num_neighbors_3rd, embedding_size]\n",
        "\n",
        "            third_order_agg = self.aggregate_gat(third_order_embeddings) if aggregation == \"GAT\" else self.aggregate_graphsage(third_order_embeddings)\n",
        "            # Combine first, second, and third-order aggregations\n",
        "            combined_1st_2nd_3rd = torch.cat([combined_1st_2nd, third_order_agg], dim=1)  # Shape: [batch_size, 3 * embedding_size]\n",
        "        else:\n",
        "            combined_1st_2nd_3rd = combined_1st_2nd\n",
        "\n",
        "        # Final embedding transformation\n",
        "        if task == \"user\":\n",
        "            target_embeddings = self.user_embeddings(target_ids)  # Shape: [batch_size, embedding_size]\n",
        "        elif task == \"item\":\n",
        "            target_embeddings = self.item_embeddings(target_ids)  # Shape: [batch_size, embedding_size]\n",
        "\n",
        "        # Apply a transformation layer (e.g., MLP) to refine embeddings\n",
        "        refined_embedding = self.refine_embedding(combined_1st_2nd_3rd, target_embeddings)\n",
        "\n",
        "        return refined_embedding\n",
        "\n",
        "\n",
        "\n",
        "    def _1st_user_task(self, support_item, target_user, training_phase):\n",
        "        \"\"\"\n",
        "        First-order user task in PyTorch.\n",
        "        Computes the user embedding by aggregating information from first-order neighbors (items).\n",
        "\n",
        "        Args:\n",
        "            support_item: Tensor of shape [batch_size, num_neighbors], indices of items.\n",
        "            target_user: Tensor of shape [batch_size, embedding_size], target user embeddings.\n",
        "            training_phase: Boolean indicating whether the model is in training mode.\n",
        "\n",
        "        Returns:\n",
        "            final_support_encode_user_task: Tensor of shape [batch_size, embedding_size], aggregated user embeddings.\n",
        "            cosine_similarity: Tensor of shape [batch_size], cosine similarity between predicted and target embeddings.\n",
        "            loss_user_task: Scalar, the loss value.\n",
        "        \"\"\"\n",
        "        # Create aggregated user embedding from neighbors (items)\n",
        "        final_support_encode_user_task = self._create_aggregator_network_user_task(support_item, training_phase)  # [batch_size, embedding_size]\n",
        "\n",
        "        # Cosine similarity between predicted user embeddings and target user embeddings\n",
        "        cosine_similarity = F.cosine_similarity(final_support_encode_user_task, target_user, dim=1)  # [batch_size]\n",
        "\n",
        "        # Define loss (negative mean cosine similarity)\n",
        "        loss_user_task = -torch.mean(cosine_similarity)\n",
        "\n",
        "        return final_support_encode_user_task, cosine_similarity, loss_user_task\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def _1st_item_task(self):\n",
        "        \"\"\"\n",
        "        First-order item task in PyTorch.\n",
        "        Computes the item embedding by aggregating information from first-order neighbors (users).\n",
        "        \"\"\"\n",
        "        # Load pretrained user embeddings\n",
        "        self.pre_train_user_ebd = torch.tensor(np.load(self.settings.pre_train_user_ebd_path), dtype=torch.float32)\n",
        "\n",
        "        # Create aggregated item embedding from neighbors (users)\n",
        "        final_support_encode_item_task = self._create_aggregator_network_item_task('active')  # [batch_size, embedding_size]\n",
        "\n",
        "        # Cosine similarity between predicted item embeddings and target item embeddings\n",
        "        cosine_similarity = F.cosine_similarity(final_support_encode_item_task, self.target_item, dim=1)  # [batch_size]\n",
        "\n",
        "        # Define loss (negative mean cosine similarity)\n",
        "        loss_item_task = -torch.mean(cosine_similarity)\n",
        "\n",
        "        return final_support_encode_item_task, cosine_similarity, loss_item_task\n",
        "\n",
        "\n",
        "    def _2nd_user_task(self, name, support_user_2nd, support_item_1st):\n",
        "        \"\"\"\n",
        "        Second-order user task in PyTorch.\n",
        "        Aggregates second-order neighbors (users and items) to compute user embeddings.\n",
        "        \"\"\"\n",
        "        if name == 'GAT':\n",
        "            # Initialize weights for transformations\n",
        "            w_0u = nn.Parameter(self.glorot([self.embedding_size, self.embedding_size]))\n",
        "            w_1u = nn.Parameter(self.glorot([3 * self.embedding_size, self.embedding_size]))\n",
        "\n",
        "            # Lookup embeddings for second-order neighbors (users)\n",
        "            support_ori_ebd_2nd = self.user_embeddings(support_user_2nd)  # [batch_size, n2, embedding_size]\n",
        "            support_encode_2nd = torch.mean(self.encoder(support_ori_ebd_2nd), dim=1)  # [batch_size, embedding_size]\n",
        "\n",
        "            # Lookup embeddings for first-order neighbors (items)\n",
        "            support_ori_ebd_1st = self.item_embeddings(support_item_1st)  # [batch_size, n1, embedding_size]\n",
        "            ori_1st_ebd = torch.mean(support_ori_ebd_1st, dim=1)  # [batch_size, embedding_size]\n",
        "\n",
        "            # Concatenate embeddings and apply transformations\n",
        "            aggregate_2nd = torch.cat([support_encode_2nd, support_encode_2nd, ori_1st_ebd], dim=1)  # [batch_size, 3 * embedding_size]\n",
        "            refined_first_neigh_ebd = torch.matmul(aggregate_2nd, w_1u)  # [batch_size, embedding_size]\n",
        "            refined_target_ebd = torch.matmul(refined_first_neigh_ebd, w_0u)  # [batch_size, embedding_size]\n",
        "\n",
        "            # Final prediction for second-order user task\n",
        "            predict_u_2nd = refined_target_ebd\n",
        "            cosine_similarity = F.cosine_similarity(predict_u_2nd, self.target_user, dim=1)\n",
        "            loss_2nd_user = -torch.mean(cosine_similarity)\n",
        "\n",
        "            return predict_u_2nd, cosine_similarity, loss_2nd_user\n",
        "        elif name == 'GraphSAGE':\n",
        "            # Initialize weights for transformations\n",
        "            w_0u = nn.Parameter(self.glorot([self.embedding_size, self.embedding_size]))\n",
        "            w_1u = nn.Parameter(self.glorot([3 * self.embedding_size, self.embedding_size]))\n",
        "\n",
        "            # Lookup embeddings for second-order neighbors (users)\n",
        "            support_ori_ebd_2nd = self.user_embeddings(support_user_2nd)  # [batch_size, n2, embedding_size]\n",
        "            support_encode_2nd = torch.mean(self.encoder(support_ori_ebd_2nd), dim=1)  # [batch_size, embedding_size]\n",
        "            ori_2nd_ebd = torch.mean(support_ori_ebd_2nd, dim=1)  # [batch_size, embedding_size]\n",
        "\n",
        "            # Lookup embeddings for first-order neighbors (items)\n",
        "            support_ori_ebd_1st = self.item_embeddings(support_item_1st)  # [batch_size, n1, embedding_size]\n",
        "            ori_1st_ebd = torch.mean(support_ori_ebd_1st, dim=1)  # [batch_size, embedding_size]\n",
        "\n",
        "            # Concatenate embeddings and apply transformations\n",
        "            aggregate_2nd = torch.cat([support_encode_2nd, ori_2nd_ebd, ori_1st_ebd], dim=1)  # [batch_size, 3 * embedding_size]\n",
        "            refined_first_neigh_ebd = torch.matmul(aggregate_2nd, w_1u)  # [batch_size, embedding_size]\n",
        "            refined_target_ebd = torch.matmul(refined_first_neigh_ebd, w_0u)  # [batch_size, embedding_size]\n",
        "\n",
        "            # Final prediction for second-order user task\n",
        "            predict_u_2nd = refined_target_ebd\n",
        "            cosine_similarity = F.cosine_similarity(predict_u_2nd, self.target_user, dim=1)\n",
        "            loss_2nd_user = -torch.mean(cosine_similarity)\n",
        "\n",
        "            return predict_u_2nd, cosine_similarity, loss_2nd_user\n",
        "\n",
        "\n",
        "    def _2nd_item_task(self, name, support_item_2nd, support_user_1st):\n",
        "        \"\"\"\n",
        "        Second-order item task in PyTorch.\n",
        "        Aggregates second-order neighbors (items and users) to compute item embeddings.\n",
        "        \"\"\"\n",
        "        if name == 'GAT':\n",
        "            # Initialize weights for transformations\n",
        "            w_0i = nn.Parameter(self.glorot([self.embedding_size, self.embedding_size]))\n",
        "            w_1i = nn.Parameter(self.glorot([3 * self.embedding_size, self.embedding_size]))\n",
        "\n",
        "            # Lookup embeddings for second-order neighbors (items)\n",
        "            support_ori_ebd_2nd = self.item_embeddings(support_item_2nd)  # [batch_size, n2, embedding_size]\n",
        "            support_encode_2nd = torch.mean(self.encoder(support_ori_ebd_2nd), dim=1)  # [batch_size, embedding_size]\n",
        "\n",
        "            # Lookup embeddings for first-order neighbors (users)\n",
        "            support_ori_ebd_1st = self.user_embeddings(support_user_1st)  # [batch_size, n1, embedding_size]\n",
        "            ori_1st_ebd = torch.mean(support_ori_ebd_1st, dim=1)  # [batch_size, embedding_size]\n",
        "\n",
        "            # Concatenate embeddings and apply transformations\n",
        "            aggregate_2nd = torch.cat([support_encode_2nd, support_encode_2nd, ori_1st_ebd], dim=1)  # [batch_size, 3 * embedding_size]\n",
        "            refined_first_neigh_ebd = torch.matmul(aggregate_2nd, w_1i)  # [batch_size, embedding_size]\n",
        "            refined_target_ebd = torch.matmul(refined_first_neigh_ebd, w_0i)  # [batch_size, embedding_size]\n",
        "\n",
        "            # Final prediction for second-order item task\n",
        "            predict_i_2nd = refined_target_ebd\n",
        "            cosine_similarity = F.cosine_similarity(predict_i_2nd, self.target_item, dim=1)\n",
        "            loss_2nd_item = -torch.mean(cosine_similarity)\n",
        "\n",
        "            return predict_i_2nd, cosine_similarity, loss_2nd_item\n",
        "        elif name == 'GraphSAGE':\n",
        "            # Initialize weights for transformations\n",
        "            w_0i = nn.Parameter(self.glorot([self.embedding_size, self.embedding_size]))\n",
        "            w_1i = nn.Parameter(self.glorot([3 * self.embedding_size, self.embedding_size]))\n",
        "\n",
        "            # Lookup embeddings for second-order neighbors (items)\n",
        "            support_ori_ebd_2nd = self.item_embeddings(support_item_2nd)  # [batch_size, n2, embedding_size]\n",
        "            support_encode_2nd = torch.mean(self.encoder(support_ori_ebd_2nd), dim=1)  # [batch_size, embedding_size]\n",
        "            ori_2nd_ebd = torch.mean(support_ori_ebd_2nd, dim=1)  # [batch_size, embedding_size]\n",
        "\n",
        "            # Lookup embeddings for first-order neighbors (users)\n",
        "            support_ori_ebd_1st = self.user_embeddings(support_user_1st)  # [batch_size, n1, embedding_size]\n",
        "            ori_1st_ebd = torch.mean(support_ori_ebd_1st, dim=1)  # [batch_size, embedding_size]\n",
        "\n",
        "            # Concatenate embeddings and apply transformations\n",
        "            aggregate_2nd = torch.cat([support_encode_2nd, ori_2nd_ebd, ori_1st_ebd], dim=1)  # [batch_size, 3 * embedding_size]\n",
        "            refined_first_neigh_ebd = torch.matmul(aggregate_2nd, w_1i)  # [batch_size, embedding_size]\n",
        "            refined_target_ebd = torch.matmul(refined_first_neigh_ebd, w_0i)  # [batch_size, embedding_size]\n",
        "\n",
        "            # Final prediction for second-order item task\n",
        "            predict_i_2nd = refined_target_ebd\n",
        "            cosine_similarity = F.cosine_similarity(predict_i_2nd, self.target_item, dim=1)\n",
        "            loss_2nd_item = -torch.mean(cosine_similarity)\n",
        "\n",
        "            return predict_i_2nd, cosine_similarity, loss_2nd_item\n",
        "\n",
        "    def _3rd_user_task(self, name, support_item_3rd, support_user_2nd_, support_item_1st_):\n",
        "        \"\"\"\n",
        "        Third-order user task in PyTorch.\n",
        "        Aggregates third-order neighbors (items and users) to compute user embeddings.\n",
        "        \"\"\"\n",
        "        if name == 'GAT':\n",
        "            # Initialize weights for transformations\n",
        "            w_0u = nn.Parameter(self.glorot([self.embedding_size, self.embedding_size]))\n",
        "            w_1u = nn.Parameter(self.glorot([3 * self.embedding_size, self.embedding_size]))\n",
        "            w_2u = nn.Parameter(self.glorot([3 * self.embedding_size, self.embedding_size]))\n",
        "\n",
        "            # Third-order embeddings\n",
        "            support_ori_ebd_3rd = self.item_embeddings(support_item_3rd)\n",
        "            support_encode_3rd = torch.mean(self.encoder(support_ori_ebd_3rd), dim=1)  # [batch_size, embedding_size]\n",
        "\n",
        "            # Second-order embeddings\n",
        "            support_ori_ebd_2nd = self.user_embeddings(support_user_2nd_)\n",
        "            support_encode_2nd = torch.mean(self.encoder(support_ori_ebd_2nd), dim=1)  # [batch_size, embedding_size]\n",
        "            ori_2nd_ebd = torch.mean(support_ori_ebd_2nd, dim=1)  # [batch_size, embedding_size]\n",
        "\n",
        "            # First-order embeddings\n",
        "            support_ori_ebd_1st = self.item_embeddings(support_item_1st_)\n",
        "            ori_1st_ebd = torch.mean(support_ori_ebd_1st, dim=1)  # [batch_size, embedding_size]\n",
        "\n",
        "            # Aggregate third-order to second-order\n",
        "            aggregate_3rd = torch.cat([support_encode_3rd, support_encode_3rd, ori_2nd_ebd], dim=1)  # [batch_size, 3 * embedding_size]\n",
        "            refined_second_neigh_ebd = torch.matmul(aggregate_3rd, w_2u)  # [batch_size, embedding_size]\n",
        "\n",
        "            # Aggregate second-order to first-order\n",
        "            aggregate_2nd = torch.cat([refined_second_neigh_ebd, support_encode_2nd, ori_1st_ebd], dim=1)  # [batch_size, 3 * embedding_size]\n",
        "            refined_first_neigh_ebd = torch.matmul(aggregate_2nd, w_1u)  # [batch_size, embedding_size]\n",
        "            refined_target_ebd = torch.matmul(refined_first_neigh_ebd, w_0u)  # [batch_size, embedding_size]\n",
        "\n",
        "            # Final prediction for third-order user task\n",
        "            predict_u_3rd = refined_target_ebd\n",
        "            cosine_similarity = F.cosine_similarity(predict_u_3rd, self.target_user, dim=1)\n",
        "            loss_3rd_user = -torch.mean(cosine_similarity)\n",
        "\n",
        "            return predict_u_3rd, cosine_similarity, loss_3rd_user\n",
        "        elif name == 'GraphSAGE':\n",
        "            # Initialize weights for transformations\n",
        "            w_0u = nn.Parameter(self.glorot([self.embedding_size, self.embedding_size]))\n",
        "            w_1u = nn.Parameter(self.glorot([3 * self.embedding_size, self.embedding_size]))\n",
        "            w_2u = nn.Parameter(self.glorot([3 * self.embedding_size, self.embedding_size]))\n",
        "\n",
        "            # Third-order embeddings\n",
        "            support_ori_ebd_3rd = self.item_embeddings(support_item_3rd)\n",
        "            support_encode_3rd = torch.mean(self.encoder(support_ori_ebd_3rd), dim=1)  # [batch_size, embedding_size]\n",
        "            ori_3rd_ebd  = torch.mean(support_ori_ebd_3rd, dim=1) # [batch_size, embedding_size\n",
        "\n",
        "            # Second-order embeddings\n",
        "            support_ori_ebd_2nd = self.user_embeddings(support_user_2nd_)\n",
        "            support_encode_2nd = torch.mean(self.encoder(support_ori_ebd_2nd), dim=1)  # [batch_size, embedding_size]\n",
        "            ori_2nd_ebd = torch.mean(support_ori_ebd_2nd, dim=1)  # [batch_size, embedding_size]\n",
        "\n",
        "            # First-order embeddings\n",
        "            support_ori_ebd_1st = self.item_embeddings(support_item_1st_)\n",
        "            ori_1st_ebd = torch.mean(support_ori_ebd_1st, dim=1)  # [batch_size, embedding_size]\n",
        "\n",
        "            # Aggregate third-order to second-order\n",
        "            aggregate_3rd = torch.cat([support_encode_3rd, ori_3rd_ebd, ori_2nd_ebd], dim=1)  # [batch_size, 3 * embedding_size]\n",
        "            refined_second_neigh_ebd = torch.matmul(aggregate_3rd, w_2u)  # [batch_size, embedding_size]\n",
        "\n",
        "            # Aggregate second-order to first-order\n",
        "            aggregate_2nd = torch.cat([refined_second_neigh_ebd, support_encode_2nd, ori_1st_ebd], dim=1)  # [batch_size, 3 * embedding_size]\n",
        "            refined_first_neigh_ebd = torch.matmul(aggregate_2nd, w_1u)  # [batch_size, embedding_size]\n",
        "            refined_target_ebd = torch.matmul(refined_first_neigh_ebd, w_0u)  # [batch_size, embedding_size]\n",
        "\n",
        "            # Final prediction for third-order user task\n",
        "            predict_u_3rd = refined_target_ebd\n",
        "            cosine_similarity = F.cosine_similarity(predict_u_3rd, self.target_user, dim=1)\n",
        "            loss_3rd_user = -torch.mean(cosine_similarity)\n",
        "\n",
        "\n",
        "    def _3rd_item_task(self, name, support_user_3rd, support_item_2nd_, support_user_1st_):\n",
        "        \"\"\"\n",
        "        Third-order item task in PyTorch.\n",
        "        Aggregates third-order neighbors (users and items) to compute item embeddings.\n",
        "        \"\"\"\n",
        "        if name == 'GAT':\n",
        "            # Initialize weights for transformations\n",
        "            w_0i = nn.Parameter(self.glorot([self.embedding_size, self.embedding_size]))\n",
        "            w_1i = nn.Parameter(self.glorot([3 * self.embedding_size, self.embedding_size]))\n",
        "            w_2i = nn.Parameter(self.glorot([3 * self.embedding_size, self.embedding_size]))\n",
        "\n",
        "            # Third-order embeddings\n",
        "            support_ori_ebd_3rd = self.user_embeddings(support_user_3rd)\n",
        "            support_encode_3rd = torch.mean(self.encoder(support_ori_ebd_3rd), dim=1)  # [batch_size, embedding_size]\n",
        "\n",
        "            # Second-order embeddings\n",
        "            support_ori_ebd_2nd = self.item_embeddings(support_item_2nd_)\n",
        "            support_encode_2nd = torch.mean(self.encoder(support_ori_ebd_2nd), dim=1)  # [batch_size, embedding_size]\n",
        "            ori_2nd_ebd = torch.mean(support_ori_ebd_2nd, dim=1)  # [batch_size, embedding_size]\n",
        "\n",
        "            # First-order embeddings\n",
        "            support_ori_ebd_1st = self.user_embeddings(support_user_1st_)\n",
        "            ori_1st_ebd = torch.mean(support_ori_ebd_1st, dim=1)  # [batch_size, embedding_size]\n",
        "\n",
        "            # Aggregate third-order to second-order\n",
        "            aggregate_3rd = torch.cat([support_encode_3rd, support_encode_3rd, ori_2nd_ebd], dim=1)  # [batch_size, 3 * embedding_size]\n",
        "            refined_second_neigh_ebd = torch.matmul(aggregate_3rd, w_2i)  # [batch_size, embedding_size]\n",
        "\n",
        "            # Aggregate second-order to first-order\n",
        "            aggregate_2nd = torch.cat([refined_second_neigh_ebd, support_encode_2nd, ori_1st_ebd], dim=1)  # [batch_size, 3 * embedding_size]\n",
        "            refined_first_neigh_ebd = torch.matmul(aggregate_2nd, w_1i)  # [batch_size, embedding_size]\n",
        "            refined_target_ebd = torch.matmul(refined_first_neigh_ebd, w_0i)  # [batch_size, embedding_size]\n",
        "\n",
        "            # Final prediction for third-order item task\n",
        "            predict_i_3rd = refined_target_ebd\n",
        "            cosine_similarity = F.cosine_similarity(predict_i_3rd, self.target_item, dim=1)\n",
        "            loss_3rd_item = -torch.mean(cosine_similarity)\n",
        "\n",
        "            return predict_i_3rd, cosine_similarity, loss_3rd_item\n",
        "        elif name == 'GraphSAGE':\n",
        "            # Lookup embeddings for third-order neighbors (users)\n",
        "            support_ori_ebd_3rd = self.user_embeddings(support_user_3rd)  # [batch_size, n3, embedding_size]\n",
        "            ori_3rd_ebd = torch.mean(support_ori_ebd_3rd, dim=1)  # [batch_size, embedding_size]\n",
        "            support_encode_3rd = self.encoder(support_ori_ebd_3rd)  # Apply encoder [batch_size, n3, embedding_size] -> [batch_size, embedding_size]\n",
        "\n",
        "            # Lookup embeddings for second-order neighbors (items)\n",
        "            support_ori_ebd_2nd = self.item_embeddings(support_item_2nd_)  # [batch_size, n2, embedding_size]\n",
        "            ori_2nd_ebd = torch.mean(support_ori_ebd_2nd, dim=1)  # [batch_size, embedding_size]\n",
        "            support_encode_2nd = self.encoder(support_ori_ebd_2nd)  # Apply encoder [batch_size, n2, embedding_size] -> [batch_size, embedding_size]\n",
        "\n",
        "            # Lookup embeddings for first-order neighbors (users)\n",
        "            support_ori_ebd_1st = self.user_embeddings(support_user_1st_)  # [batch_size, n1, embedding_size]\n",
        "            ori_1st_ebd = torch.mean(support_ori_ebd_1st, dim=1)  # [batch_size, embedding_size]\n",
        "            support_encode_1st = self.encoder(support_ori_ebd_1st)  # Apply encoder [batch_size, n1, embedding_size] -> [batch_size, embedding_size]\n",
        "\n",
        "            # Combine embeddings for third-order aggregation\n",
        "            aggregate_3rd = torch.cat([support_encode_3rd, ori_3rd_ebd, ori_2nd_ebd], dim=1)  # [batch_size, 3 * embedding_size]\n",
        "            refined_second_neigh_ebd = self.linear_sage_3rd(aggregate_3rd)  # Transform to [batch_size, embedding_size]\n",
        "\n",
        "            # Combine embeddings for second-order aggregation\n",
        "            aggregate_2nd = torch.cat([refined_second_neigh_ebd, support_encode_2nd, ori_1st_ebd], dim=1)  # [batch_size, 3 * embedding_size]\n",
        "            refined_first_neigh_ebd = self.linear_sage_2nd(aggregate_2nd)  # Transform to [batch_size, embedding_size]\n",
        "\n",
        "            # Final refined target embedding\n",
        "            refined_target_ebd = refined_first_neigh_ebd  # [batch_size, embedding_size]\n",
        "\n",
        "            # Final prediction for third-order item task\n",
        "            predict_i_3rd = refined_target_ebd\n",
        "            cosine_similarity = F.cosine_similarity(predict_i_3rd, self.target_item, dim=1)\n",
        "            loss_3rd_item = -torch.mean(cosine_similarity)\n",
        "\n",
        "            return predict_i_3rd, cosine_similarity, loss_3rd_item\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "################ BELOW ARE HELPER FUNCTIONS ####################\n",
        "    def aggregate_gat(self, neighbor_embeddings):\n",
        "        \"\"\"\n",
        "        Perform attention-weighted aggregation for GAT.\n",
        "        Args:\n",
        "            neighbor_embeddings (Tensor): Neighbor embeddings. Shape: [batch_size, num_neighbors, embedding_size].\n",
        "        Returns:\n",
        "            Tensor: Aggregated embedding. Shape: [batch_size, embedding_size].\n",
        "        \"\"\"\n",
        "        attention_weights = torch.nn.functional.softmax(neighbor_embeddings.mean(dim=-1), dim=1)  # [batch_size, num_neighbors]\n",
        "        aggregated = torch.sum(attention_weights.unsqueeze(-1) * neighbor_embeddings, dim=1)  # [batch_size, embedding_size]\n",
        "        return aggregated\n",
        "\n",
        "    def aggregate_graphsage(self, neighbor_embeddings):\n",
        "        \"\"\"\n",
        "        Perform mean pooling aggregation for GraphSAGE.\n",
        "        Args:\n",
        "            neighbor_embeddings (Tensor): Neighbor embeddings. Shape: [batch_size, num_neighbors, embedding_size].\n",
        "        Returns:\n",
        "            Tensor: Aggregated embedding. Shape: [batch_size, embedding_size].\n",
        "        \"\"\"\n",
        "        return torch.mean(neighbor_embeddings, dim=1)  # [batch_size, embedding_size]\n",
        "\n",
        "\n",
        "\n",
        "    def refine_embedding(self, combined_embeddings, target_embeddings):\n",
        "        \"\"\"\n",
        "        Refines the embeddings by applying a transformation layer.\n",
        "        Args:\n",
        "            combined_embeddings (Tensor): Combined neighbor embeddings. Shape: [batch_size, N * embedding_size].\n",
        "            target_embeddings (Tensor): Target embeddings. Shape: [batch_size, embedding_size].\n",
        "        Returns:\n",
        "            Tensor: Refined embeddings. Shape: [batch_size, embedding_size].\n",
        "        \"\"\"\n",
        "        # Apply transformation to the combined embedding\n",
        "        refined = torch.mm(combined_embeddings, self.second_order_weight)  # [batch_size, embedding_size]\n",
        "        refined += target_embeddings  # Residual connection\n",
        "        return F.relu(refined)\n",
        "\n",
        "\n",
        "    def _create_aggregator_network_user_task(self, support_item, training_phase):\n",
        "        \"\"\"\n",
        "        Creates the aggregator network for the first-order user task.\n",
        "        Aggregates embeddings of items (first-order neighbors) to compute user embeddings.\n",
        "\n",
        "        Args:\n",
        "            support_item: Tensor of shape [batch_size, num_neighbors], indices of items.\n",
        "            training_phase: Boolean indicating whether the model is in training mode.\n",
        "\n",
        "        Returns:\n",
        "            final_support_encode_user_task: Tensor of shape [batch_size, embedding_size], aggregated user embeddings.\n",
        "        \"\"\"\n",
        "        # Lookup embeddings for support items (first-order neighbors)\n",
        "        support_ebd = self.item_embeddings(support_item)  # [batch_size, num_neighbors, embedding_size]\n",
        "\n",
        "        # Apply encoding using the Encoder (multi-head attention mechanism)\n",
        "        support_encoded = self.encoder(support_ebd, training_phase)  # [batch_size, num_neighbors, embedding_size]\n",
        "\n",
        "        # Aggregate embeddings by averaging across neighbors\n",
        "        final_support_encode_user_task = torch.mean(support_encoded, dim=1)  # [batch_size, embedding_size]\n",
        "\n",
        "        return final_support_encode_user_task\n",
        "\n",
        "\n",
        "    def _create_aggregator_network_item_task(self, support_user, training_phase):\n",
        "        \"\"\"\n",
        "        Creates the aggregator network for the first-order item task.\n",
        "        Aggregates embeddings of users (first-order neighbors) to compute item embeddings.\n",
        "\n",
        "        Args:\n",
        "            support_user: Tensor of shape [batch_size, num_neighbors], indices of users.\n",
        "            training_phase: Boolean indicating whether the model is in training mode.\n",
        "\n",
        "        Returns:\n",
        "            final_support_encode_item_task: Tensor of shape [batch_size, embedding_size], aggregated item embeddings.\n",
        "        \"\"\"\n",
        "        # Lookup embeddings for support users (first-order neighbors)\n",
        "        support_ebd = self.user_embeddings(support_user)  # [batch_size, num_neighbors, embedding_size]\n",
        "\n",
        "        # Apply encoding using the Encoder (multi-head attention mechanism)\n",
        "        support_encoded = self.encoder(support_ebd, training_phase)  # [batch_size, num_neighbors, embedding_size]\n",
        "\n",
        "        # Aggregate embeddings by averaging across neighbors\n",
        "        final_support_encode_item_task = torch.mean(support_encoded, dim=1)  # [batch_size, embedding_size]\n",
        "\n",
        "        return final_support_encode_item_task\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Helper(training_helper.py)\n"
      ],
      "metadata": {
        "id": "6CVClbifhMt6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def train_first_order_task(model, train_loader, valid_loader, epochs, device, task=\"user\"):\n",
        "    \"\"\"\n",
        "    Train GeneralGNN for first-order tasks (user or item).\n",
        "    \"\"\"\n",
        "    optimizer = optim.Adagrad(model.parameters(), lr=model.learning_rate)\n",
        "    loss_fn = nn.CosineEmbeddingLoss()\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "\n",
        "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs} - Training First-Order {task} Task\"):\n",
        "            target_ids, support_1st, _, _, oracle_embeddings = batch\n",
        "            target_ids = target_ids.to(device)\n",
        "            support_1st = support_1st.to(device)\n",
        "            oracle_embeddings = oracle_embeddings.to(device)\n",
        "\n",
        "            predicted_embeddings = model(\n",
        "                target_ids, support_1st, None, None, task=task\n",
        "            )\n",
        "\n",
        "            target = torch.ones(predicted_embeddings.size(0), device=device)\n",
        "            loss = loss_fn(predicted_embeddings, oracle_embeddings, target)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        print(f\"Epoch {epoch + 1}/{epochs} - First-Order {task} Task: Train Loss = {avg_train_loss:.4f}\")\n",
        "\n",
        "        validate_task(model, valid_loader, loss_fn, device, task, \"First-Order\")\n",
        "\n",
        "\n",
        "def train_second_order_task(model, train_loader, valid_loader, epochs, device, task=\"user\"):\n",
        "    \"\"\"\n",
        "    Train GeneralGNN for second-order tasks (user or item).\n",
        "    \"\"\"\n",
        "    optimizer = optim.Adagrad(model.parameters(), lr=model.learning_rate)\n",
        "    loss_fn = nn.CosineEmbeddingLoss()\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "\n",
        "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs} - Training Second-Order {task} Task\"):\n",
        "            target_ids, support_1st, support_2nd, _, oracle_embeddings = batch\n",
        "            target_ids = target_ids.to(device)\n",
        "            support_1st = support_1st.to(device)\n",
        "            support_2nd = support_2nd.to(device)\n",
        "            oracle_embeddings = oracle_embeddings.to(device)\n",
        "\n",
        "            predicted_embeddings = model(\n",
        "                target_ids, support_1st, support_2nd, None, task=task\n",
        "            )\n",
        "\n",
        "            target = torch.ones(predicted_embeddings.size(0), device=device)\n",
        "            loss = loss_fn(predicted_embeddings, oracle_embeddings, target)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        print(f\"Epoch {epoch + 1}/{epochs} - Second-Order {task} Task: Train Loss = {avg_train_loss:.4f}\")\n",
        "\n",
        "        validate_task(model, valid_loader, loss_fn, device, task, \"Second-Order\")\n",
        "\n",
        "\n",
        "def train_third_order_task(model, train_loader, valid_loader, epochs, device, task=\"user\"):\n",
        "    \"\"\"\n",
        "    Train GeneralGNN for third-order tasks (user or item).\n",
        "    \"\"\"\n",
        "    optimizer = optim.Adagrad(model.parameters(), lr=model.learning_rate)\n",
        "    loss_fn = nn.CosineEmbeddingLoss()\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "\n",
        "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs} - Training Third-Order {task} Task\"):\n",
        "            target_ids, support_1st, support_2nd, support_3rd, oracle_embeddings = batch\n",
        "            target_ids = target_ids.to(device)\n",
        "            support_1st = support_1st.to(device)\n",
        "            support_2nd = support_2nd.to(device)\n",
        "            support_3rd = support_3rd.to(device)\n",
        "            oracle_embeddings = oracle_embeddings.to(device)\n",
        "\n",
        "            predicted_embeddings = model(\n",
        "                target_ids, support_1st, support_2nd, support_3rd, task=task\n",
        "            )\n",
        "\n",
        "            target = torch.ones(predicted_embeddings.size(0), device=device)\n",
        "            loss = loss_fn(predicted_embeddings, oracle_embeddings, target)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        print(f\"Epoch {epoch + 1}/{epochs} - Third-Order {task} Task: Train Loss = {avg_train_loss:.4f}\")\n",
        "\n",
        "        validate_task(model, valid_loader, loss_fn, device, task, \"Third-Order\")\n",
        "\n",
        "\n",
        "def validate_task(model, valid_loader, loss_fn, device, task, order):\n",
        "    \"\"\"\n",
        "    Validation logic for tasks.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    valid_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch in valid_loader:\n",
        "            target_ids, support_1st, support_2nd, support_3rd, oracle_embeddings = batch\n",
        "            target_ids = target_ids.to(device)\n",
        "            support_1st = support_1st.to(device)\n",
        "            support_2nd = support_2nd.to(device) if support_2nd is not None else None\n",
        "            support_3rd = support_3rd.to(device) if support_3rd is not None else None\n",
        "            oracle_embeddings = oracle_embeddings.to(device)\n",
        "\n",
        "            predicted_embeddings = model(\n",
        "                target_ids, support_1st, support_2nd, support_3rd, task=task\n",
        "            )\n",
        "\n",
        "            target = torch.ones(predicted_embeddings.size(0), device=device)\n",
        "            loss = loss_fn(predicted_embeddings, oracle_embeddings, target)\n",
        "            valid_loss += loss.item()\n",
        "\n",
        "    avg_valid_loss = valid_loss / len(valid_loader)\n",
        "    print(f\"Validation {order} {task} Task: Loss = {avg_valid_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "RNSnrIZEhSiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training(Train.py)\n",
        "Below is the tranning file for our model. Should be put under another file.\n"
      ],
      "metadata": {
        "id": "O89gyPPiclDH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from GeneralGNN import GeneralGNN\n",
        "from training_helper import (\n",
        "    train_first_order_task,\n",
        "    train_second_order_task,\n",
        "    train_third_order_task,\n",
        ")\n",
        "from dataset import UserDataset, ItemDataset  # Define your dataset classes\n",
        "from settings import Settings  # A settings file or object for configurations\n",
        "\n",
        "def main():\n",
        "    # Load settings\n",
        "    settings = Settings()\n",
        "\n",
        "    # Set device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Load datasets\n",
        "    train_user_dataset = UserDataset(settings.oracle_training_file_user_task)\n",
        "    valid_user_dataset = UserDataset(settings.oracle_valid_file_user_task)\n",
        "\n",
        "    train_item_dataset = ItemDataset(settings.oracle_training_file_item_task)\n",
        "    valid_item_dataset = ItemDataset(settings.oracle_valid_file_item_task)\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_user_loader = DataLoader(train_user_dataset, batch_size=settings.batch_size, shuffle=True)\n",
        "    valid_user_loader = DataLoader(valid_user_dataset, batch_size=settings.batch_size, shuffle=False)\n",
        "\n",
        "    train_item_loader = DataLoader(train_item_dataset, batch_size=settings.batch_size, shuffle=True)\n",
        "    valid_item_loader = DataLoader(valid_item_dataset, batch_size=settings.batch_size, shuffle=False)\n",
        "\n",
        "    # Instantiate the model\n",
        "    model = GeneralGNN(name=\"GraphSAGE\", settings=settings)\n",
        "\n",
        "    # Define training parameters\n",
        "    num_epochs = settings.epochs\n",
        "\n",
        "    # Train for user tasks\n",
        "    print(\"Training user tasks...\")\n",
        "    train_first_order_task(\n",
        "        model=model,\n",
        "        train_loader=train_user_loader,\n",
        "        valid_loader=valid_user_loader,\n",
        "        epochs=num_epochs,\n",
        "        device=device,\n",
        "        task=\"user\",\n",
        "    )\n",
        "\n",
        "    train_second_order_task(\n",
        "        model=model,\n",
        "        train_loader=train_user_loader,\n",
        "        valid_loader=valid_user_loader,\n",
        "        epochs=num_epochs,\n",
        "        device=device,\n",
        "        task=\"user\",\n",
        "    )\n",
        "\n",
        "    train_third_order_task(\n",
        "        model=model,\n",
        "        train_loader=train_user_loader,\n",
        "        valid_loader=valid_user_loader,\n",
        "        epochs=num_epochs,\n",
        "        device=device,\n",
        "        task=\"user\",\n",
        "    )\n",
        "\n",
        "    # Train for item tasks\n",
        "    print(\"Training item tasks...\")\n",
        "    train_first_order_task(\n",
        "        model=model,\n",
        "        train_loader=train_item_loader,\n",
        "        valid_loader=valid_item_loader,\n",
        "        epochs=num_epochs,\n",
        "        device=device,\n",
        "        task=\"item\",\n",
        "    )\n",
        "\n",
        "    train_second_order_task(\n",
        "        model=model,\n",
        "        train_loader=train_item_loader,\n",
        "        valid_loader=valid_item_loader,\n",
        "        epochs=num_epochs,\n",
        "        device=device,\n",
        "        task=\"item\",\n",
        "    )\n",
        "\n",
        "    train_third_order_task(\n",
        "        model=model,\n",
        "        train_loader=train_item_loader,\n",
        "        valid_loader=valid_item_loader,\n",
        "        epochs=num_epochs,\n",
        "        device=device,\n",
        "        task=\"item\",\n",
        "    )\n",
        "\n",
        "    print(\"Training completed.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "LXsh6mvFcjm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Next Step\n",
        "1. Prepare Data:\n",
        "\n",
        "  Write a data preparation pipeline that aligns with this batch format.\n",
        "Use torch.utils.data.DataLoader for batching.\n",
        "2. Test the Loop:\n",
        "\n",
        "  Run the training loop with dummy data to ensure everything works.\n",
        "3. Monitor Performance:\n",
        "\n",
        "  Add metrics like accuracy, precision, or recall for recommendation tasks."
      ],
      "metadata": {
        "id": "bwTNRRn3dBc3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JBLrlsjddRcn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}